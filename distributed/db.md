# 分布式db
ref:
- [一致性协议在分布式数据库系统中的应用](https://xblk.ecnu.edu.cn/CN/html/20180508.htm)

分布式数据库的核心——数据分片、数据同步.

分布式系统是分布式数据库的核心，它与存储引擎相互配合共同实现了完整的分布式数据库的功能.

NewSQL 是基于 NoSQL 模式构建的分布式数据库，它通常采用**现有的 SQL 类关系型数据库为底层存储或自研引擎**，并**在此之上加入分布式系统**，从而对终端用户屏蔽了分布式管理的细节. Citus 和 Vitess 就是此种类型的两个著名案例.

NoSQL 和 NewSQL 是建立在一个假设上，即构建一个完备功能的分布式数据库代价是高昂的，需要进行某种妥协。而商用 Distributed SQL 数据库的目标恰恰是要以合理的成本构建这样一种数据库，可以看到它们的理念是针锋相对的.

业界更加广泛接受的 NewSQL 定义包括：
- 创新的架构：分布式系统与存储引擎都需要创新
- 透明的 Sharding：自动的控制，解放用户，贴合云原生
- 分布式 SQL：打造商业可用分布式数据库的关键
- 高性能事务：NewSQL 创新点, 是区别不同种类 NewSQL 的关键

> DistributedSQL 是一类特殊的 NewSQL，它们可以进行全球部署.

> 开源的 NewSQL 选择的并不是 LSM 树，而是 RocksDB, 看中的是 RocksDB 的性能与功能.

## 分片
数据分片的方式一般有两种:
- 水平分片：在不同的数据库节点中存储同一表的不同行
- 垂直分片：在不同的数据库节点中存储表不同的表列

分片理念来源于经济学的边际收益理论：如果投资持续增加，但收益的增幅开始下降时，被称为边际收益递减状态, 而刚好要开始下降的那个点被称为边际平衡点.

该理论应用在数据库计算能力上往往被表述为：如果数据库处理能力遇到瓶颈，最简单的方式是持续提高系统性能，如更换更强劲的 CPU、更大内存等，这种模式被称为垂直扩展. 当持续增加资源以提升数据库能力时，垂直扩展有其自身的限制，最终达到边际平衡，收益开始递减.

分片算法一般指代水平分片所需要的算法:
- 哈希分片

    获取分片键，然后根据特定的哈希算法计算它的哈希值，最后使用哈希值确定数据应被放置在哪个分片中。数据库一般对所有数据使用统一的哈希算法（例如 ketama），以促成哈希函数在服务器之间均匀地分配数据，从而降低了数据不均衡所带来的热点风险。通过这种方法，数据不太可能放在同一分片上，从而使数据被随机分散开.

    这种算法非常适合随机读写的场景，能够很好地分散系统负载，但弊端是不利于范围扫描查询操作.

- 范围分片

    范围分片根据数据值或键空间的范围对数据进行划分，相邻的分片键更有可能落入相同的分片上。每行数据不像哈希分片那样需要进行转换，实际上它们只是简单地被分类到不同的分片上.

    范围分片需要选择合适的分片键，这些分片键需要尽量不包含重复数值，也就是其候选数值尽可能地离散。同时数据不要单调递增或递减，否则，数据不能很好地在集群中离散，从而造成热点。

    范围分片非常适合进行范围查找，但是其随机读写性能偏弱.

- 融合算法

    灵活地组合哈希分片和范围分片.

    比如可以建立一个多级分片策略，该策略在最上层使用哈希算法，而在每个基于哈希的分片单元中，数据将按顺序存储

- 地理位置算法

    该算法一般用于 NewSQL 数据库，提供全球范围内分布数据的能力

    在基于地理位置的分片算法中，数据被映射到特定的分片，而这些分片又被映射到特定区域以及这些区域中的节点, 然后在给定区域内，使用哈希或范围分片对数据进行分片.

自动分片是分布式数据库的主流功能，所有主要的分布式数据库，甚至数据库中间件都在尝试自动分片.

### 手动分片 vs 自动分片
手动分片是设置静态规则来将数据根据分片算法分散到数据库节点. 这一般是由于用户使用的数据库不支持自动的分片, 而在应用层面上做数据分片来解决, 也可以使用简单的数据库中间件或 Proxy 来设置静态的分片规则来解决.

手动分片的缺点是数据分布不均匀. 数据分布不均可能导致数据库负载极其不平衡.

使用自动分片意味着计算节点与分片算法可以相互配合，从而使数据库进行弹性伸缩.

使用基于范围的分片很容易实现自动分片：只需拆分或合并每个分片.

基于范围的分片可能会带来读取和写入热点，我们可以通过拆分和移动分片消除这些热点.

而使用基于哈希的分片的系统实现自动分片代价很高昂. 因为在应用哈希函数后，数据是随机分布的，并且调整散列算法肯定会更改大多数数据的分布情况.

## 数据复制
数据库的复制技术需要考虑两个因素：数据一致 RPO 和业务连续性 RTO.

常见的复制模式:
- 同步复制：如果由于从库已崩溃，存在网络故障或其他原因而没有响应，则主库也无法写入该数据
- 半同步复制：其中部分从库进行同步复制，而其他从库进行异步复制。也就是，如果其中一个从库同步确认，主库可以写入该数据
- 异步复制：不管从库的复制情况如何，主库可以写入该数据。而此时，如果主库失效，那么还未同步到从库的数据就会丢失

不同的同步模式是在性能和一致性上做平衡，三种模式对应不同场景，并没有好坏差异. 用户需要根据自己的业务场景来设置不同的同步模式.


常见的复制方式:
1. 基于语句的复制

    主库记录它所执行的每个写请求（一般以 SQL 语句形式保存），每个从库解析并执行该语句，就像从客户端收到该语句一样。但这种复制会有一些潜在问题，如语句使用了获取当前时间的函数，复制后会在不同数据节点上产生不同的值.

    另外如自增列、触发器、存储过程和函数都可能在复制后产生意想不到的问题, 但可以通过预处理规避这些问题。使用该复制方式的分布式数据库有 VoltDB、Calvin.

1. 日志（WAL）同步

    WAL 是一组字节序列，其中包含对数据库的所有写操作. 它的内容是一组低级操作，如向磁盘的某个页面的某个数据块写入一段二进制数据，主库通过网络将这样的数据发送给从库.

    这种方法避免了上面提到的语句中部分操作复制后产生的一些副作用，但要求主从的数据库引擎完全一致，最好版本也要一致. 如果要升级从库版本，那么就需要计划外停机. PostgreSQL 和 Oracle 中使用了此方法.

3. 行复制

    它由一系列记录组成，这些记录描述了以行的粒度对数据库表进行的写操作. 它与特定存储引擎解耦，并且第三方应用可以很容易解析其数据格式.

4. ETL 工具

    该功能一般是最灵活的方式. 用户可以根据自己的业务来设计复制的范围和机制，同时在复制过程中还可以进行如过滤、转换和压缩等操作, 但性能一般较低.


**以一致性算法为核心的强一致性复制技术是未来的发展方式**.

### 单主复制
即主从复制. 写入主节点的数据都需要复制到从节点，即存储数据库副本的节点. 当客户要写入数据库时，就必须将请求发送给主节点，而后主节点将这些数据转换为复制日志或修改数据流发送给其所有从节点. 从使用者的角度来看，从节点都是只读的.

### 多主复制
即主主复制. 数据库集群内存在多个对等的主节点，它们可以同时接受写入, 每个主节点同时充当主节点的从节点.

多主节点的架构模式最早来源于 DistributedSQL 这一类多数据中心，跨地域的分布式数据库。在这样的物理空间相距甚远，有多个数据中心参与的集群中，每个数据中心内都有一个主节点. 而在每个数据中心的内部，却是采用常规的单主复制模式.

此方法的最大缺点是，存在一种可能性，即两个不同的主节点同时修改相同的数据.

典型的多主复制产品有 MySQL 的 Tungsten Replicator、PostgreSQL 的 BDR 和 Oracle 的 GoldenGate.

### Mysql复制
Mysql复制经历了四代的发展: 第一代为传统复制，使用 MHA（Master High Available）架构；第二代是基于 GTID 的复制，即 GTID+Binlog server 的模式；第三代为增强半同步复制，GTID+增强半同步复制；第四代为 MySQL 原生高可用，即 MySQL InnoDB Cluster.

## 事务
### 隔离级别
快照隔离是现代分布式数据库存储引擎最常使用的隔离级别.

在快照隔离下, 可以使用 Percolator 模式描述的方案去实现新的原子提交，在冲突较低的场景下，该方案具有很好的性能.

### 分布式事务
TiDB 实现了 Percolator 乐观事务, 采用的是提交阶段的后向检测. 它也支持通过其他方式实现兼容MySQL的悲观模式的分布式事务.

TiDB 的悲观锁实现的原理是，在一个事务执行 DML（UPDATE/DELETE）的过程中，TiDB 不仅会将需要修改的行在本地缓存，同时还会对这些行直接上悲观锁，这里的悲观锁的格式和乐观事务中的锁几乎一致，但是锁的内容是空的，只是一个占位符，等到 Commit 的时候，直接将这些悲观锁改写成标准的 Percolator 模型的锁，后续流程和原来保持一致即可。

这个方案在很大程度上兼容了原有的事务实现，其扩展性、高可用和灵活性都有保证。同时该方案尽最大可能复用了原有 Percolator 的乐观事务方案，减少了事务模型整体的复杂度.

> MySQL 使用的分布式事务是悲观模式, 是在 SQL 执行阶段就能检测冲突，也就是前向模式.

## 存储引擎
### Bitcask
Bitcask是分布式键值数据库 Riak 的一种存储引擎，是一种典型的无顺序存储结构. 它没有内存表结构，也就是它根本不进行缓存而是直接将数据写到数据文件之中.

它在内存中有一个叫作 Keydir 的结构保存了指向数据最新版本的引用，旧数据依然在数据文件中，但是没有被 Keydir 引用，最终就会被垃圾收集器删除掉。Keydir 实际上是一个哈希表，在数据库启动时，从数据文件中构建出来.

这种查询很明显改善了 LSM 树的读放大问题，因为每条数据只有一个磁盘文件引用，且没有缓存数据，故只需要查询一个位置就可以将数据查询出来。但其缺陷同样明显：不支持范围查找，且启动时，如果数据量很大，启动时间会比较长。

此种结构优化了写入、空间以及对单条数据的查找，但牺牲了范围查找的功能.

### WiscKey
WiscKey将 Key 和 Value 分别放在两个文件中. Key 还是按照 LSM 树的形式，这样就保证了 Key 是有顺序的，可以进行范围扫描. 同时使用 LSM 树，即不需要将所有的 Key 放到内存里，这样也解决了 Bitcask 加载慢的问题.

而 Value 部分称为 vLogs（value Logs），其中的数据是没有顺序的。这种结构适合更新和删除比较少的场景，因为范围扫描会使用随机读，如果更新删除很多，那么其冲突合并的效率很低。同时在合并操作的时候，需要扫描 Key 而后确定合并方案，这个在普通的 LSM 树中也是不存在的。

WiscKey 非常适合在 SSD 进行运行，因为读取 Value 需要进行随机读取。目前 dgraph.io 的 Badger 是该模式比较成熟的实现.

## example
采用去中心化事务的方案一般需要结合：MVCC、2PC 和 PaxosGroup/MultiRaft.

### Spanner
Spanner的核心主要是两个部分: TrueTime 和 Paxos Group.

TrueTime 是一种逻辑与物理时间的融合，是由原子钟结合 IDC 本地时间生成的。区别于传统的单一时间点，TrueTime 的返回值是一个时间范围，数据操作可能发生在这个范围之内，故范围内的数据状态是不确定的（uncertainty）。系统必须等待一段时间，从而获得确定的系统状态。这段时间通常是比较短暂的，且多个操作可以并行执行，通常不会影响整体的吞吐量.

Spanner 提供了三种事务模式:
- 读写事务：该事务是通过分布式锁实现的，并发性是最差的。且数据写入每个分片 Paxos Group 的主节点。
- 只读事务：该事务是无锁的，可以在任意副本集上进行读取。但是，如果想读到最新的数据，需要从主节点上进行读取。主节点可以从 Paxos Group 中获取最新提交的时间节点。
- 快照读：顾名思义，Spanner 实现了 MVCC 和快照隔离，故读取操作在整个事务内部是一致的。同时这也暗示了，Spanner 可以保存同一份数据的多个版本。

在隔离方面，Spanner 实现了 SSI，也就是序列化的快照隔离。其方法就是lock table.

### Calvin
Calvin 事务模型是一种集中化的事务处理模式，但却在高竞争环境下具有非常明显的吞吐量的优势. 其关键就是通过重新调度事务的执行，消除了竞争，从而提高了吞吐量. 采用该模式的除了 Calvin 外，还有 VoltDB.

### Proxy 模式架构
这类数据库一般以 MySQL 或 PostgreSQL 为基础进行开发。MySQL 类的解决方案有 TDSQL、Vitess 和具有 JDTX 的 ShardingShpere。PGXC（PostgreSQL-XC）的本意是指一种以 PostgreSQL 为内核的开源分布式数据库。因为 PostgreSQL 的开放软件版权协议，很多厂商在 PGXC 上二次开发，推出自己的产品. 不过，这些改动都没有变更主体架构风格，所以把这类产品统称为 PGXC 风格，其中包括 TBase、GuassDB 和 AntDB 等.