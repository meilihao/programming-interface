# driver
参考:
- [Linux Platform驱动模型(二) _驱动方法](https://www.cnblogs.com/xiaojiang1025/p/6367910.html)

驱动本质上只做了两件事：向上提供接口，向下控制硬件:

开发一个驱动的流程：
1. 确定驱动架构：根据硬件连接方式结合分层/分离思想设计驱动的基本结构
1. 确定驱动对象：内核中的一个驱动/设备就是一个对象，1.定义，2.初始化，3.注册，4.注销
1. 向上提供接口：根据业务需要确定提供cdev/proc/sysfs哪种接口
1. 向下控制硬件：1.查看原理图确定引脚和控制逻辑，2.查看芯片手册确定寄存器配置方式，3.进行内存映射，4.实现控制逻辑

## 地址
物理地址: CPU地址总线使用的地址, 由硬件电路控制其具体含义. 物理地址中很大一部分是留给内存的, 但也常被映射到其他存储器上（如显存、BIOS等）. 在程序指令中的虚拟地址经过段映射和页面映射后, 就生成了物理地址.

总线地址: 总线使用的地址

外设使用的是总线地址, CPU使用的是物理地址.

物理地址与总线地址之间的关系由系统的设计决定的. 在x86平台上物理地址就是总线地址, 这是因为它们共享相同的地址空间.

虚拟地址: 现代操作系统普遍采用虚拟内存管理（Virtual Memory Management）机制, 这需要MMU（Memory Management Unit）的支持. MMU通常是CPU的一部分, 如果处理器没有MMU或者有MMU但没有启用, CPU执行单元发出的内存地址将直接传到芯片引脚上, 被内存芯片（物理内存）接收，这称为物理地址（Physical Address）. 如果处理器启用了MMU, CPU执行单元发出的内存地址将被MMU截获, 从CPU到MMU的地址称为虚拟地址（Virtual Address）, 而MMU将这个地址翻译成另一个地址发到CPU芯片的外部地址引脚上, 也就是将虚拟地址映射成物理地址.

## 和cpu交互的数据传输方式
1. 无条件传送方式

    用此方式的数据源设备一定是随时准备好了数据, CPU 随时取随时拿都没问题, 如寄存器、内存就是类似这样的设备, CPU 取数据时不用提前打招呼
2. 查询传送方式
    
    也称为程序 I/O或PIO (Programming Input/Output Model ）, 是指传输之前, 由程序先去检测设备的状态. 数据源设备在一定的条件下才能传送数据, 这类设备通常是低速设备, 比CPU 慢很多. CPU 需要数据时, 先检查该设备的状态, 如果状态为“准备好了可以发送飞, CPU 再去获取数据. 硬盘有 status 寄存器, 里面保存了工作状态, 所以对硬盘可以用此方式来获取数据.
3. 中断传送方式

    也称为中断驱动I/O. “查询传送方式”有这样的缺陷, 由于 CPU需要不断查询设备状态, 所以意味着只有最后一刻的查询才是有意义的, 之前的查询都是发生在数据尚未准备好的时间段里，所以说效率不高，仅对于不要求速度的系统可以采用. 可以改进的地方是如果数据源设备将数据准备好后再通知 CPU 来取，这样效率就高了. 通知 CPU 可以采用中断的方式，当数据源设备准备好数据后，它通过发中断来通知 CPU 来拿数据，这样避免了 CPU花在查询上的时间，效率较高.

    bios设置的中断号从0x08开始, 但由于中断号 0x00--0x1F 属于 Intel 公司专门保留给 CPU 使用的, 为解决这个冲突Linux 操作系统不会直接使用这些 BIOS 默认设置好的中断号。在上电启动时， Linux 操作系统会在内核初始化期间又重新对 8259A 进行了设置，把所有系统硬件中断请求号全部都映射到了 0X20 及以上中断号上.
4. 直接存储器存取方式(DMA)

    在中断传送方式中，虽然极大地提高了 CPU 的利用率，但通过中断方式来通知 CPU, CPU 就要通过压找来保护现场，还要执行传输指令，最后还要恢复现场. 其实还可一点都不要浪费 CPU 资源，不让 CPU参与传输，完全由数据源设备和内存直接传输. CPU 直接到内存中拿数据就好了. 这就是此方式中“直接”的意思. 不过 DMA 是由硬件实现的, 所以需要 DMA 控制器才行.

    DMA的发起者可以是处理器, 也可以是I/O设备. 以处理器发起DMA为例, 设备驱动首先在内存中分配一块DMA缓冲区, 随后发起DMA请求, 设备收到请求后通过DMA机制将数据传输至DMA缓冲区. DMA操作完成后, 设备触发中断通知处理器对DMA缓冲区中的数据进行处理.

    在一些总线中, DMA的发起还需要DMA控制器的参与, 此时DMA控制器相当于收发双方（处理器和设备）的第三方, 因此这种机制被称为第三方DMA（third-party DMA）, 也被称为标准DMA.

    在标准DMA场景下由处理器发起DMA的具体步骤:
	1. 处理器向DMA控制器发送DMA缓冲区的位置和长度, 以及数据传输的方向, 随后放弃对总线的控制
	2. DMA控制器获得总线控制权, 可以直接与内存和设备进行通信
	3. DMA控制器根据从处理器获得的指令, 将设备的数据拷贝至内存, 在这期间处理器可以执行其他任务
	4. DMA控制器完成DMA后向处理器发送中断, 通知处理器DMA已经完成. 此时, 处理器会重新得到总线的控制权

	另一种情况是由设备发起DMA, 设备首先通知DMA控制器, 并由DMA控制器向总线控制器申请占用总线, 此后的流程与处理器发起DMA的情况相似.

	不少总线允许设备直接获取总线控制权并进行DMA操作, 而无须借助DMA控制器, 这种机制被称为第一方DMA（first-party DMA）, 也称为总线控制（bus mastering）. 此时可以理解为将DMA控制器的角色和设备的角色合二为一, 其执行过程和上述2种情况类似. 此时, 如果多个设备希望同时进行DMA, 总线控制器需要进行仲裁, 决定优先次序, 同一时间只允许一个设备进行DMA.

5. I/O 处理机传送方式

    DMA中数据输入之后或输出之前还是有一部分工作要由 CPU 来完成的.

    I/O处理机是专门用于处理 IO, 并且它其实是一种处理器, 只不过用的是另一套擅长 IO 的指令系统，随时可以处理数据, 彻底解放CPU, 让CPU 甚至可以不知道有传输这回事.

## cpu访问外设的方法
ref:
- [由于与IO地址空间相关的一些限制和不良影响, IO地址空间很快就失去了软件和硬件供应商的青睐](https://blog.csdn.net/u013253075/article/details/119491100)
- [x86 I/O端口分配表](https://bochs.sourceforge.io/techspec/PORTS.LST)

CPU通常以读写设备寄存器的方式与设备进行通信. 一个设备通常有多个寄存器, 可以在内存地址空间或I/O地址空间中被访问. 设备寄存器可以分为如下几种类型:
1. 控制寄存器:用于接收来自驱动程序的命令
1. 状态寄存器:用于反馈当前设备的工作状态
1. 输入/输出寄存器(即数据寄存器):用于驱动和设备之间的数据交互

设备寄存器也称为`I/O端口`, 而IO端口有两种编址方式：独立编址和统一编址. 而具体采用哪一种则取决于CPU的体系结构.

- 内存映射(memory-mapped, 统一编址, mmio): 此类cpu通常只实现一个物理地址空间, 会将外设 I/O端口的物理地址映射到CPU的单一物理地址空间中, 成为存储空间的一部分，通过直接读写内存地址的方式来访问外设.

	RISC指令系统的CPU（如ARM、PowerPC等）通常支持内存映射模式.

	优点: 访问速度较快, 编程比较简单
	缺点: 需要占用CPU的内存地址空间
	适用: 需要频繁访问外设的应用

	IO内存：当寄存器或内存位于内存空间时的称呼, 可通过`cat /proc/iomem`查看.
- io映射(i/o-mapped, 独立编址): 为外设专门实现了一个单独地地址空间, 称为`I/O地址空间或I/O端口空间`(32位X86有64K的IO空间, `cat /proc/ioport`). IO地址与内存地址分开独立编址, I/O端口地址不占用存储空间的地址范围. 此时在系统中就存在了另一种与存储地址无关的IO地址. 此时, 需要使用专用的CPU指令来访问某种特定外设, 比如x86的in/out等指令.

	接口的作用是连接处理器和外部设备, 处理速度不匹配, 格式转换(模拟转数字, cpu只能处理数字信号)等情况.

	> IA32 体系系统中，因为用于存储端口号的寄存器是 16 位的，所以最大有 65536 个端口，即 0～ 65535.

	x86体系的CPU均支持io映射模式.

	优点: 不占用CPU的内存地址
	缺点: 编程比较复杂, 也就是CPU访问外设时相对复杂
	适用: 需要同时访问多个外设的应用

	IO端口：当寄存器或内存位于IO空间时的称呼, 可通过`cat /proc/ioports`查看.

	**不过x86平台也支持内存映射（MMIO）, 该技术是PCI规范的一部分, 如果硬件支持MMIO, 就可将其IO设备端口映射到内存空间. 为了兼容一些之前开发的软件, PCIe仍然支持IO地址空间, 只是建议在新开发的软件中采用MMIO**.

> 控制显卡用的是内存映射+io映射.

对于Linux内核而言, 它适用于不同的CPU, 所以它必须都要考虑这两种方式, 于是它采用一种新的方法, 将基于I/O映射方式(对应于独立编址)的或内存映射方式(对应于统一编址)的I/O端口通称为`I/O区域(I/O region, include/linux/ioport.h)`, 不论采用哪种方式, 都要先申请IO区域: request_resource(), 结束时释放它: release_resource().

## 中断
单核cpu使用pic模型, 只有一张IDT; 多核使用APIC.

APIC由2部分组成:
- LAPIC

	每个core一个LAPIC, 且有一个唯一id, 该id用作区分多核cpu不同核心的标志

	每个LAPIC有自己的定时器, 能处理自己的内部中断.

	x86 LAPIC的属性在一些列的寄存器中, 它们通过mmio映射, 因此设置LAPIC定时器的中断时间就可通过mmio实现.

	LVT(Local Vector Table, 本地向量表)是联系中断信号和IDT的纽带, 是程序调度实现的关键硬件基础.

	LVT的Vector保存IDT的中断号

	获取LAPIC路径: RSDP->XSDT->MADT->LAPIC
- IO APIC

	只有一个io apic, 也有表, 可设置哪个中断由哪个cpu处理.

	处理硬盘, 网卡等外部设备产生的中断

ps: bsp(随机指定by IPI) cpu core通过IPI(Inter-Processor Interrupt, 是硬件中断)中断来唤醒其他cpu core.

## IOMMU
在大多数情况下, 处理器上执行的操作系统代码在进行内存访问时使用的是虚拟地址, 并通过MMU翻译成物理地址. 设备进行DMA时访问的内存地址是总线地址（bus address）, 它不同于虚拟地址和物理地址. 当操作系统闷DMA控制器注册DMA内存缓冲区时, 需要填写的是总线地址. 设备和内存之间的输入输出内存管理单元（Input-Output Memory Management Unit,  IOMMU）会负责将总线地址翻译成物理地址.


IOMMU作用:
1. 由于总线或者一些设备本身的限制, 设备可以访问到的内存范围远远小于物理内存的总大小. 对于这种设备, os在分配DMA内存缓冲区时, 内存地址受限. 它可以有效地解决这—问题

	某些设备只能使用24位长度的地址, 因此只能访问到16MB范围内的物理内存
1. 由于DMA在进行过程中会向连续的地址写入数据. 因此操作系统需要分配连续的物理内存作为DMA缓冲区. IOMMU的存在免除了DMA缓冲区各内存页必须物理连续的限制, 同时IOMMU还限定了设备可以访问到的物理内存范围
1. 为os提供内存访问保护机制, 用于防止恶意设备以及驱动对物理内存的非法访问, 尤其是虚拟化环境下.

## 设备识别
计算机系统需要提供某种的机制, 帮助os识别识别计算机已经连接的设备. 常见的设备识别机制有设备树和ACPI.

ACPI和设备树均使用树状结构对设备信息和层次关系进行描述. 类似于设备树的compatible属性, ACPI则通过命名空间（namespace）和特定的ACPI/PNPID来完成设备和驱动代码的匹配. ACPI的命名空间用于对平台上设备的拓扑关系进行建模, 设备以命名空间内的对象（object）的形式存在, 类似于设备树的节点（node）.

设备树描述的仅仅是一种of的关系, 即当前计算机系统拥有哪些设备. 而ACPI除了描述当前计算机的设备信息之外, 还能通过AML代码直接控制设备的行为, 这是设备树所不具备的.

### 设备树
ARM设备种类数不胜数, 配置信息各不相同, 把硬件配置硬编码到系统代码中的方式非常不灵活, 会极大增加os的维护成本, 所以ARM架构普遍使用设备树来编码硬件信息.

设备树（device tree）是描述计算机硬件信息的数据结构, 或理解为os可理解的硬件描述语言.

设备树中包含了CPU的名称、内存、总线、设备等硬件信息.

设备树并不对所有设备信息都进行描述, 通常只有不能被**动态探测**的设备才会出现在设备树中.

设备树源码（DeviceTreeSource, DTS）文件以文本形式描述了计算机系统设备的一个树状结构. 每种设备都存在一个节点描述, 并且挂载在根节点`/`之下, 这种树状格式被称为设备树块（Device Tree Blob, DTB）或者展开的设备树（Flattened Device Tree, FDT）.

设备树源码文件的扩展名为.dts. 在Linux内核项目源码里, 可以在平台相关代码目录中找到大量设备树源码文件. `.dts`源码文件可以编译生成二进制文件, 其扩展名为`.dtb`. 在启动过程中, os内核读取dtb二进制文件以获取设备信息.

Linux内核根据设备树注册各节点所表示的设备, 根据compatible属性后接的字符串内容识别设备并匹配对应的驱动代码, 进而实现对设备的管理.

### ACPI
这是x86架构计算机上的标准设备识别方案. ACPI可以理解为设备和操作系统之间的一层抽象, 该层抽象统一地向os汇报硬件设备的情况, 同时提供管理设备的接口和方法.

ACPI分为两部分:
1. ACPI表: ACPI提供了大量的表结构信息, 这些信息描述了当前计算机系统的各种状态, 包括多处理器信息、NUMA的配置信息等
1. ACPI运行时: ACPI提供了ACPI机器语言（ACPI Machine Language, 简称AML）. os可以解释执行AML代码和底层的固件进行交互,进而完成对设备的识别和相应配置功能

ACPI还负责计算机整体系统、各个处理器以及不同设备的电源和温度的管理.

考虑到ACPI在服务器领域的广泛使用, ARMv8服务器也支持了ACPI.

> windows不支持设备树. 基于兼容性的考量. Linux系统同时支持设备树和ACPI两种设备识别机制.

## 中断控制器
ARM架构在CPU与设备之间引入了中断控制器（interrupt controller）来管理中断.

ARMv8的中断控制器称为通用中断控制器（Generic Interrupt Controller, GIC）, 负责对设备的中断信号进行处理, 并将其发送至CPU.

GIC的中断类型可以分为三类:
1. 软件生成中断（Software Generated Interrupt, SGI）: 由软件通过写GICD_SGIR系统寄存器触发, 常用于发送核间中断
1. 私有设备中断（Private Peripheral Interrupt, PPI）:由每个处理器核上私有的设备触发, 如通用定时器（Generic Time）
1. 共享设备中断（Shared Peripheral Interrupt, SPI）:由所有CPU核心共同连接的设备触发, 可以发送给任意核心

根据中断事件的重要程度和紧迫程度, GIC允许os为中断配置不同的中断优先级. 通常优先级的数字越小表示优先级越高. 在中断仲裁时, 高优先级的中断会先于低优先级的中断被发送给CPU处理. 当CPU在响应低优先级中断时, 如果此时有高优先级中断到来, 那么高优先级中断会抢占低优先级中断, 被CPU优先响应.

GIC中断类型以及对应的INTID(Interrupt ID)之间的关系:
- SGI, 0～15 : 由核间通信等产生的中断, 需交由指定核进行处理, 每个核各自维护中断队列
- PPI, 16～31, 1056～1119: 由设备产生的中断, 需交由指定核进行处理, 每个核各自维护中断队列
- SPI, 32～1019, 4096 ~ 5119 : 由设备产生的中断, 可交由指定核/非指定的任意一个或多个核进行处理, 所有核维护一个共享的中断队列

GIC中的每个中断都存在如下四种状态:
1. Inactive:中断处于无效状态, 此时没有中断到来
1. Pending:中断处于有效状态, 此时中断已经发生, 但CPU没有响应中断
1. Active:CPU处于响应并处理中断的过程中
1. Active & Pending: 在CPU响应并处理中断的过程中, 同时又有相同中断号的中断发生

GIC的中断响应过程通常包括如下四个步骡:
1. Generate:某个中断源产生中断, 传递给GIC, 中断从Inactive变为Pending
2. Deliver: GIC将中断转发给CPU,中断仍处于Pending状态
3. Active: CPU调用中断处理函数响应并处理该中断, 中断处于Active状态
4. Deactivate: CPU处理完中断, 通知GIC中断处理完毕, GIC将中断状态更新为Inactive

其中的第四步又称为中断完成（End Of Interrupt, EOI）. EOI在中断处理中是非常重要的步骤, 只有在CPU确认了EOI后, 对应的中断（此时的状态为Inactive）才能重新被响应.

为了提高中断响应的实时性, GIC将中断完成分解为两个阶段:
1. 优先级重置（priority drop）:将当前中断屏蔽的优先级进行重置, 从而能够响应较低???优先级的中断
1. 中断无效（interrupt deactivation）:将对应中断的状态设置为Inactive, 从而可以重新响应该优先级的中断

为了及时响应中断, Linux操作系统将中断处理过程分为两个阶段:
1. 上半部（top half）:完成一些必要但轻量级的操作, 比如向中断控制器确认中断
1. 下半部（bottom half）:完成剩余的、复杂且时延要求相对较低的操作

当出现中断时, 会首先执行上半部, 在执行上半部期间关闭中断. 上半部执行完成后, 立即向中断控制器声明该中断事件已处理完毕, 从而允许CPU继续响应该中断. 而下半部的执行时间将由系统调度来确定. Linux提供了多种下半部的实现方式, 下半部属于具有较高优先级的内核任务.

设备驱动代码通过调用request_irq()来注册相应设备的硬中断处理函数. 硬中断处理函数又称为中断服务例程（Interrup Service Routine, ISR）. 硬中断处理函数实质上是Linux中断处理的上半部. 上半部在执行过程中, 会向系统注册新的处理任务, 将其作为下半部的执行函数. linux有多种注册下半部任务的方式, 常用的三种下半部机制是软中断, tasklet和工作队列.

软中断（softirq）是最基本的下半部处理方式, 可以看作一种用普通内核任务模拟硬中断处理函数的方法. 软中断和硬中断都用于处理来自I/O设备的中断请求, 只不过软中断不使用硬中断上下文, 而是拥有独立的上下文环境. 软中断不会中断当前CPU的任务, 而是等待调度器的主动调度.

软中断与硬中断的另一点区别是, 软中断的处理函数必须是可重人的. 硬中断之所以无须可重入是因为在硬中断响应过程中, 处理器已经关闭了中断, 从而屏蔽了新到来的硬中断. 而软中断的执行允许被硬中断抢占, 同时Linux还允许软中断在多个CPU核上并行执行以提升软中断的处理效率.

Linux内核会选择恰当的时机来处理软中断. 常见的处理时机是在硬中断得到处理后, 此时内核会随即处理软中断. linux限制了软中断连续处理的最长时间和最大次数, 分别对应于MAXSOFT工RQ_T工ME和MAX_SOFTIRQ_RESTART两个常量, 一旦超过这个限制就将软中断任务追加到名为ksoftirqd的内核线程. 在Linux内核中, 每个CPU上都运行着`ksoftirqd/<n>`内核线程（n是线程所在的处理器编号）. ksoftirqd线程的优先级被设置为最低, 因此不会和系统上的其他重要任务争夺资源, 从而避免发生用户任务饿死的情况.

Linux的软中断只能在代码编译时静态分配, 而无法根据需要在运行时进行动态创建. 为了解决这个问题, Linux基于软中断实现了tasklet机制, 其也运行在软中断上下文中.

Linux内核使用ne×t指针将需要处理的tasklet以链表的形式串联起来. 每个tasklet都有两种状态, 其中TASKLET_STATE_SCHED表示当前tasklet巳经在内核中被注册, 并处于可调度的状态, 而TASKLET_STATE_RUN则表示当前tasklet正在运行. count变量指明当前tasklet是否被禁用,大于0表示被禁用, 等于0表示启用. func指针指向将被执行的具体函数, date是func回调函数的参数.

Linux内核用tasklet_schedule()或tasklet_hi_shedule()来调度tasklet, 它们分别对应TASKLET_SOFTIRQ和HI_SOFTIRQ两个不同的优先级.

除了允许在运行时动态创建tasklet外, Linux不允许在多个CPU上并发执行tasklet(使用state), 因此避免了同步. Linux还保证了tasklet的执行的原子性, 使其不能被其他下半部机制所抢占, 因此无须考虑可重入问题, 开发起来也更直接简单.

通过对比软中断和tasklet, 可以发现这两种下半部机制在设计上有不同的侧重: 软中断侧重于中断的处理效率, 如在多核情况下软中断可并行执行; tasklet则更侧重于编程开发的友好性, tasklet使用者无须考虑同步加锁和代码可重入等问题.

因为tasklet本身依托于中断上下文, 执行期间不能睡眠, 外加设计上的不可抢占性, 导致tasklet可能引起难以预测的系统延迟, 严重的话甚至可能影响系统的整体实时性.

工作队列, 它允许睡眠, 降低了下半部的开发难度, 同时也关注效率.

工作队列（workqueue）把需要推迟执行的函数, 即中断下半部, 交由内核线程来执行. 工作队列的特点是借助线程上下文来执行下半部操作, 从而允许下半部的睡眠和重新调度, 解决了因为软中断和tasklet单个实例执行时间过长且无法睡眠而导致的系统实时性下降的问题.

和ksoftirqd类似, 在每个CPU上都运行着一个`kworker/<n>`工作线程. 内核工作线程会串行地执行加人队列中的所有work. 如果某个队列中没有work要做, 那么该内核工作线程就会变成idle态.

工作队列并非没有缺点:
1. 本身原因:
	1. 因为工作队列本身是串行执行的, 所以一旦有某个work被阻塞, 将导致该队列上其他work无法执行
1. 驱动开发原因: 任意创建工作队列, 会导致大量工作队列的存在
	1. 创建工作队列会消耗PID全局资源（PID的默认最大值为32768）
	1. 内核线程导致的资源抢占也给调度器带来压力, 造成频繁调度的开销, 反而使系统整体性能下降
	1. Linux工作队列中的工作任务是不能进行跨CPU迁移的, 如果同一队列中的工作任务之间存在相互依赖会进一步导致死锁

因为以上原因, Linux内核引入了并发可管理工作队列（Concurrency Managed Work Queue, CMWQ）机制. CMWQ本质上是将工作队列的管理从驱动开发者手里交还给内核, 由内核来决定工作线程的创建时机. CMWQ不同于传统工作队列的主要特点是同一个工作队列里的两个work不再遵循严格的串行性执行原则, 而是可以在两个不同CPU上并发执行, 不必等待其中一个完成后才调度另一个.

和传统的Linux工作队列相比, CMWQ在设计上具有如下优点:
1. 兼容性:CMWQ在接口上保持对传统工作队列接口的兼容, 方便已有设备驱动的移植
1. 灵活性: CMWQ提出了线程池（worker-pool）的概念, 不同工作队列之间可以共享线程池, 从而有效地减少了资源浪费和频繁调度
1. 并发性:CMWQ会根据情况动态创建新的工作线程来处理被阻塞的任务, 同时自动调节线程池大小与并发级别. 使用CMWQ的驱动开发者不必关心并发的具体细节, 这也正是CMWQ得名的原因

linux不同中断处理方式的特点比较
|属 性|硬中断|软中断|tasklet|工作队列|CMWQ|
|是否屏蔽相同中断|是 是 否 否 否|
|是否可以睡眠（拥有线程上下文）|否 否 否 是 是|
|是否可以被高优先级的任务所抢占|否 否 否 是 是|
|是否能在多个处理器核心上同时运行相同实例| 是 是 否 是 是|
|分配后是否能在不同处理器核心上迁移|否 否 否 否 是|

## 设备驱动与设备驱动模型
设备驱动（device driver）是操作系统中负责控制设备的定制化(根据设备的具体型号和相应参数进行特定配置)程序.

设备驱动模型是为了解决设备多样性的问题.

Linux系统里的三种基本设备抽象: 字符设备, 块设备, 网络设备.

字符设备（char device）的主要特点是将设备上的信息抽象为连续的字节流, 应用程序通常以顺序方式对字符设备进行字节粒度的读写. 字符设备包含了标准的文件操作接口,包括open, read, write, close等. 在Linux中, 可以通过查看`/proc/devices`来获取字符设备列表. 终端（tty）、显存（Ib）、声卡（alsa）都属于Linux的字符设备.

对于每个字符设备, Linux系统都会分配一个主设备号（major number）, 用于识别操作该设备的驱动实体. 一个驱动可能同时操作同类型的多个设备, 因此还会再分配一个次设备号（minor number）.

块设备（BlockDevjce）是以块的粒度对设备上的信息进行随机读写访问. 块是指设备寻址的最小单元, 通常为512字节、1KB或4KB. 设备一般用于存储设备的抽象. 由于块设备要求能访问存储设备的任意位置, 提供随机读写能力, 为了降低块设备操作的复杂度, Linux内核专门设计了BlockI/O（简称BIO）子系统, 向上服务于存储栈的文件系统, 向下沟通于存储设备的驱动程序. Linux的块设备包括虚拟磁盘（ramdisk）, 存储卡（SD卡和MMC卡）, 磁盘（HDD和SSD）等.

通常, 应用程序对字符设备的读写会直接触发驱动对设备的I/O操作; 而块设备的访问则通过添加一层页缓存来降低系统和设备频繁I/O所导致的性能开销. 

网络设备（net device）处理的数据单位是网络包（packet）. 网络设备使用独立的接抽象--套接字（socket）. 应用通过套接字接口与网络设备进行通信, 通常使用send, receive等网络独有的接口完成对网络包数据的收发请求. linux在内核空间维护着复杂的协议栈, 负责对网络包进行封装, 解析, 寻址等处理.

Ljnux的设备驱动模型包括一套统一的数据结构和一套用户空间接口. 设备驱动模型的数据结构对象对应于系统拓扑结构的各个部分, 各对象的关系也体现了不同设备之间的依赖关系, 从而可以帮助内核记录并追踪系统连接的设备.

Linux设备驱动模型定义了四种基本的数据结构:
1. 设备（device）:用于抽象系统中所有的硬件, 包括CPU和内存
1. 驱动（driver）:用于抽象系统中用于控制设备的驱动程序. Linux内核驱动的开发基本围绕该抽象进行, 即实现规定好的接口函数
1. 总线（bus）:用于抽象I／O设备和CPU之间的通信. Linux规定, 所有的设备都要至少连接一条总线（USB或PCI）
1. 类（class）:具有相似功能或属性的设备集合. 其思想来自面向对象程序设计中的类（class）的概念, 旨在抽象出一套可以在多个设备间共享的数据结构和接口. 属于相同类的驱动程序, 就可以直接继承父类定义好的公共资源.

在Linux设备驱动模型里,总线和类可以看成在两个不同的层面上对设备的组织和管理. 总线是在拓扑结构上对设备进行组织, 而类则是右逻辑结构上对设备进行组织, 这两种组织彼此正交.

Linux设备驱动模型的本质是围绕没备, 驱动, 总线和类这四个核心抽象, 将不问的外部设备和驱动方法以树状的形式, 交给os统一管理.

为了实现统一管理, Linux内核设计了专门的内核抽象kobject, 是所有设备驱动模型抽象的统一基类. kernel将所有kobject以层次结构进行管理同时引入引用计数机制对其释放进行管理, 还通过sysfs, 将每个kobject的特性、当前状态和参数暴露给用户空间.

kset表示为一组kobject的集合, 通常被理解为kobject的容器抽象或sysfs里的子系统. `/sys/devices`目录就是一个kset对象, 它包含系统中的设备所对应的目录. ktype则表示为kobjec属性及其操作的集合, 其数扰结构包含sysfs_ops, 用于通过sysfs来和驱动进行交互的应用程序读写函数.

Linux将设备驱动抽象为device_driver结构体, 驱动开发人员只需要关注结构体内要求的搜口实现. device_driver结构体中包含多个函数指针, 要求开发者柱册相应的回调函数, 以响应设备驱动需要处理的系统事件, 这些系统事覆盖了驱动的整个生命周期, 比如:
- probe: 需要实现驱动与设备的匹配检测的功能, 并将驱动对象与设备对象相关联
- remove: 函数用于在移除驱动时释放设备申请的系统资源
- shutdown、suspend和resume函数分别对应于os的关机、休眠和唤醒事件, 需要重新配置设备的工作状态.

Linux设备驱动模型提供了设备资源管理（device resource management）框架, 驱动开发者只需要申请资源, 而资源的回收和释放则交给设备资源管理框架自动完成. 使用设备资源管理框架提供的以`devm_`开
头的内核接口, 可以在驱动初始化失败时由内核自动回滚对资源的申请.

设备资源管理框架将设备所用到的系统资源以链表的形式进行管理, 在驱动模块被`remove()`移除的时候自动释放资源.

设备驱动环境（Device Driver Environment, DDE）的主要用意是复用其他系统上的设备驱动环境, 以便其他操作系统（如Linux和FreeBSD）的现成设备驱动程序可以在L4上重用.

由于Ljnux的快速迭代, 以及设备类型和数量的持续增多, 基于DDE模拟Linux设备驱动模型环境的维护成本愈来愈高. 后来L4采取了借助虚拟化复用二进制驱动程序的路线, 将自己化身为虚拟机监控器, 用L4微内核的接口将Linux运行在用户态上, L4Linux作为L4在用户态的服务器, 为L4系统上的其他应用提供驱动服务.

## Linux的用户态驱动框架—用户空间I/O和虚拟空间I/O
UIO有一个明显的缺点, 它缺乏在用户态空间动态创建DMA区域的能力, 这也限制了一些驱动在UIO上的移植.

VFIO借助IOMMU限制了用户空间驱动代码的内存访问能力, 从而允许非特权用户驱动进程进行DMA操作.

和UIO类似, VFIO也在devfs下暴露了`/dev/vfio/<Group>`接口, 用于和用户态驱动进行交互. 这里的group是VFIO使用的基本隔离粒度, 表示一组和系统内其他设备相隔离的设备. group是确保安全访问的基本隔离粒度, 但不一定是VFIO控制设备的基本粒度. 出于性能考虑, VFIO允许在不同group之间共享同一虚拟地址空间. 为此, VFIO又引人了container类型, 容纳多个彼此间共享页表的group, 可以将container视为VFIO的基本操作单位.

打开`/dev/vfio/<vfio>字符设备`就代表创建了一个container, 但是container本身提供的接口很少, 用户需要将和设备相关的group加入container中才能进一步操作设备.