# 存储栈

## bio
bio描述了磁盘里要真实操作的位置与page cache中页的映射关系.

每个bio对应磁盘里一块连续的位置(由磁盘物理特性决定), 但一个bio请求要读/写的数据在内存中可以不连续, 每个不连续的数据段由bio_vec来描述. bio_vec是一个bio的数据容器, 也是组成bio数据的最小单位.

![bio、bio_vec与page之间的关系](/misc/img/io/Image00037.jpg)

bio_idx指向当前的bio_vec，通过它可以跟踪I/O操作的完成进度. 但bio_idx更重要的作用在于对bio结构体进行分割，像磁盘阵列这样的驱动器可以把单独的bio结构体（原本是为单个设备使用准备的）分割到磁盘阵列中的各个硬盘上，磁盘阵列设备驱动只需复制这个bio结构体，再把bio_idx域设置为每个独立硬盘操作时需要的位置就可以了.

bio在块层会被转化为request，多个连续的bio可以合并到一个request中，生成的request会继续进行合并、排序，并最终调用块设备驱动的接口将request从块层的request队列（request_queue）中移到驱动层进行处理，以完成I/O请求在通用块层的整个处理流程.

![bio关系.jpg](/misc/img/io/bio关系.jpg)

request用来描述单次I/O请求，request_queue用来描述与设备相关的请求队列，每个块设备在块层都有一个request_queue与之对应，所有对该块设备的I/O请求最后都会流经request_queue。块层正是借助bio、bio_vec、request、request_queue这几个结构将I/O请求在内核I/O子系统各个层次的处理过程联系起来的.

面对新的、快速的存储设备，一个块设备一个队列的策略，开始变得不合时宜, kernel的块层已切换到多队列(multi-queue)模型.

## io合并
io合并指将符合条件的多个i/o请求合并成一个请求一并处理, 从而提升io请求的处理效率.

![](/misc/img/io/io_merge.webp)

> plug, 将块层的io请求聚集起来, 使得零散的请求有机会进行合并和排序, 最终达到高效利用存储设备的目的.

进程产生的IO路径主要有三种：
1. buffered IO, 对应图中的路径①，系统中绝大部分IO走的这种形式，充分利用filesystem 层的page cache所带来的优势， 应用程序产生的IO经系统调用落入page cache之后便可以直接返回，page cache中的缓存数据由内核回写线程在适当时机负责同步到底层的存储介质之上，当然应用程序也可以主动发起回写过程（如fsync系统调用）来确保数据尽快同步到存储介质上，从而避免系统崩溃或者掉电带来的数据不一致性. 

    缓存IO可以带来很多好处:
    1. 首先应用程序将IO丢给page cache之后就直接返回了，避免了每次IO都将整个IO协议栈走一遍，从而减少了IO的延迟
    1. 其次，page cache中的缓存最后以页或块为单位进行回写，并非应用程序向page cache中提交了几次IO, 回写的时候就需要往通用块层提交几次IO, 这样在提交时间上不连续但在空间上连续的小块IO请求就可以合并到同一个缓存页中一并处理
    1. 再次，如果应用程序之前产生的IO已经在page cache中，后续又产生了相同的IO，那么只需要将后到的IO覆盖page cache中的旧IO，这样一来如果应用程序频繁的操作文件的同一个位置，我们只需要向底层存储设备提交最后一次IO就可以了
    1. 最后，应用程序写入到page cache中的缓存数据可以为后续的读操作服务，读取数据的时候先搜索page cache，如果命中了则直接返回，如果没命中则从底层读取并保存到page cache中，下次再读的时候便可以从page cache中命中.

1. direct IO（with plug），对应图中的路径②，这种IO绕过文件系统层的cache. 用户在打开要读写的文件的时候需要加上“O_DIRECT”标志，意为直接IO，不让文件系统的page cache介入. 从用户角度而言，应用程序能直接控制的IO形式除了上面提到的“缓存IO”，剩下的IO都走的这种形式，就算文件打开时加上了 ”O_SYNC” 标志，最终产生的IO也会进入蓄流链表（Plug List）. 如果应用程序在用户空间自己做了缓存，那么就可以使用这种IO方式，常见的如数据库应用.

1. direct IO（without plug），对应图中的路径③，内核通用块层的蓄流机制只给内核空间提供了接口来控制IO请求是否蓄流，用户空间进程没有办法控制提交的IO请求进入通用块层的时候是否蓄流. 严格的说**用户空间直接产生的IO都会走蓄流路径，哪怕是IO的时候附上了“O_DIRECT” 和 ”O_SYNC”标志**，用户间接产生的IO，如文件系统日志数据、元数据，有的不会走蓄流路径而是直接进入调度队列尽快得到调度.

**注意一点，通用块层的蓄流只提供机制和接口而不提供策略，至于需不需要蓄流、何时蓄流完全由内核中的IO派发者决定**.

应用程序不管使用图中哪条IO路径，内核都会想方设法对IO进行合并. 内核为促进这种合并，在IO协议栈上设置了三个最佳狙击点：
1. Page Cache （页高速缓存）
1. Plug List （蓄流链表）

    每个进程都有一个私有的蓄流链表(plug队列)，进程在往通用块层派发IO之前如果开启了蓄流功能，那么IO请求在被发送给IO调度器之前都保存在蓄流链表中，直到泄流(unplug)的时候才批量交给调度器. 蓄流的主要目的就是为了增加请求合并的机会，bio在进入蓄流链表之前会尝试与蓄流链表中保存的request进行合并,使用的接口为[blk_attempt_plug_merge()](https://elixir.bootlin.com/linux/v5.8-rc4/source/block/blk-core.c#L747)

    在蓄流的过程中，还要完成一项重要的工作就是造请求（make request）。make request会尝试把bio合并到一个进程本地Plug队列里的request，如果无法合并，则创造一个新的request。request里面包含一个bio的队列，这个队列的bio对应的硬盘位置，最终在硬盘上是连续存放的.
1. Elevator Queue （调度队列）

    elevator queue更多的是承担进程间IO的合并，用来弥补plug list对进程间合并的不足.

从实际应用角度出发，IO合并更多的是发生在page cache和plug list中, 因为从设备的角度而言进程内产生的IO相关性更强.

该过程可用命令dd和blktrace追踪了解 by [Linux 通用块层之IO合并](https://mp.weixin.qq.com/s?__biz=MzAwMDUwNDgxOA==&mid=2652664651&idx=1&sn=3b908b92b4209fd49bac561814369e5a&chksm=810f35d6b678bcc0353c80e744244ab61ca81b93ae4adf18dfdff28ab35b6f97c0e7635f5f46&scene=21)

## lvm2
lvm存储模型:disk/partition -> pv -> vg -> lv -> fs.

device mapper在内核中通过模块化的Target driver插件来实现，对I/O请求进行过滤或重定向等工作. 当前已经实现的插件包括磁盘阵列(Linear)、加密、多路径(multipath)、镜像(mirror)、快照(Snapshot)等.

这体现了在Linux内核设计中策略(表现)和机制(实现)分离的原则，即将所有与策略相关的工作放到用户空间完成，如逻辑设备和哪些物理设备建立映射，怎么建立这些映射关系，等等. 而内核主要用来提供完成这些策略所需要的机制，主要有I/O请求的过滤和重定向. 因此整个device mapper机制由两部分组成：内核空间的device mapper驱动，用户空间的device mapper库.

device mapper在内核中被注册为一个**块设备驱动**，它包含3个重要的对象概念：Mapped Device、Target device、Mapping table。Mapped device是一个逻辑抽象，可以理解为内核向外提供的逻辑设备，它通过Mapping table描述的映射关系和Target device建立映射. Target device表示的是Mapped device所映射的物理空间段，对Mapped device所表示的逻辑设备来说，就是该逻辑设备映射到的一个物理设备。Mapping table里有mapped device逻辑上的起始地址、范围和表示在Target device上的地址偏移量及Target类型等信息（这些地址和偏移量都是以磁盘的扇区为单位的，即512字节，所以，当看到128的时候，其实表示的是128×512=64KB）.

device mapper中的逻辑设备Mapped device不但可以映射一个或多个物理设备Target device，还可以映射另一个Mapped device，于是就出现了迭代或递归的情况，理论上可以无限嵌套下去.

![](/misc/img/io/Image00043.jpg)

device mapper所要完成的工作就是根据映射关系和Target driver描述的I/O处理规则，将I/O请求从逻辑设备Mapped device转发到相应的Target device上.

device mapper提供了一个从逻辑设备到物理设备的映射架构，只要用户在用户空间制定好映射策略，并按照自己的需求编写处理具体I/O请求的Target driver，就可以很方便地实现一个类似LVM的逻辑卷管理器.

## bcache
bcache是Linux内核的块层缓存，它使用**固态硬盘作为硬盘驱动器的缓存**，既解决了固态硬盘容量太小的问题，又解决了硬盘驱动器运行速度太慢的问题.

bcache从3.10版本开始被集成进内核，支持3种缓存策略，分别是写回（writeback）、写透（writethrough）、writearoud，默认使用writethrough，缓存策略可被动态修改.

bcache可以将固态硬盘资源池化，一块固态硬盘形成一个缓存池对应多块硬盘驱动器，并且支持从缓存池中划出瘦分配的纯Flash卷（thin-flash LUN）单独使用.

> thin: 初始较小, 随着使用会再分配空间.

Linux开源社区有多个通用的块级缓存解决方案，其中包括bcache、dm-cache、flashcache、enhanceIO等. 其中，dm-cache与bcache分别于3.9、3.10版本合并进内核，flashcache虽然没有引入内核，但在某些生产环境中也有使用. enhanceIO衍生于flashcache项目.

flashcache, dm-cache均是基于device mapper实现的.

## DRBD
分布式块设备复制（Distributed Relicated Block Device，DRBD）用于通过网络在服务器之间对块设备（硬盘、分区、逻辑卷等）进行镜像，以解决磁盘单点故障的问题. 可以将DRBD视作一种网络磁盘阵列，允许用户在远程服务器上建立一个本地块设备的实时镜像.

DRBD的工作位于Buffer Cache与I/O调度器之间，数据经过Buffer Cache后，DRBD模块通过TCP/IP协议栈经过网卡和备用节点建立数据同步.

DRBD的工作模式有以下两种:
1. 主从模型（Master/Slave）：在某一时刻只允许有一个主节点. 主节点可以挂载使用，也可以写入数据等；从节点只是一个备份，用来作为主节点的镜像，不能读/写、不能挂载.

1. 双主模型（Dula Primary）：两个节点都可以当作主节点来挂载使用. 它需使用集群文件系统，通过分布式的文件锁管理器，解决双写问题. 该方法的原理是一个节点对文件加锁之后会通过某种机制来通知其他节点有关锁的信息，从而实现文件锁共享.

# 存储加速
一般从应用的角度来讲，一个存储任务或需求的完成，可以理解为用户从软件（客户端）发出一个存储需求（包括读和写），然后从存储设备返回用户的软件的过程.

存储服务到存储设备的整个I/O栈包括软件自身的I/O逻辑、网络的I/O逻辑、存储驱动的I/O逻辑，在其中可以找到很多可以优化的点. 例如，在内存中，可以利用CPU的特殊指令对一些存储保护算法进行优化；在所经过的网络上，可以把数据传输的任务从CPU中卸载，交由具备RDMA功能的网卡或智能网卡来进行远程DMA；在操作系统到实际存储设备落盘的过程中，可以通过用户态的I/O栈来旁路（Bypass）操作系统内部的大部分I/O栈进行加速.

存储的加速方案分为以下两类:
1. 基于CPU处理器的加速和优化方案

    1. 超线程技术
    1. 使用合适的指令

        - SIMD
        - 在存储领域具有特定功能的扩展指令，如AES-NI指令集、CRC32扩展指令集、SHA-NI指令集
1. 基于协处理器或其他硬件的加速方案

    1. FPGA加速

        目前已有的FPGA加速方案按照用途的不同可以分为3类：
        1. 数据密集计算的加速

            目前市场上已经存在的FPGA加速卡能够实现加速的存储算法有以下几种:
            - 纠删码（Erasure Code，EC）
            - 众多无损数据压缩算法，如gzip/zlib、LZO、LZ4
            - 高性能的对称加密与解密算法及安全哈希算法

            应用FPGA计算加速卡后，相关算法的计算速度往往有数倍甚至数十倍的性能提升. 将FPGA加速库集成到存储产品中，能够释放有限的处理器内存，同时减轻处理器的计算负载. 在此基础上，FPGA能够降低功耗，为数据中心带来最高性能.
        1. 存储协议转换的加速

            FPGA解决方案可以使用基于 RoCE、iWarp等RDMA方式的NVMe over Fabric, 从而在远程存储系统中实现近乎原生的固态硬盘延迟、吞吐量和IOps.

            ![](/misc/img/io/Image00048.jpg)

            在NVMe over Fabric的实施过程中，能够将服务器操作网络的职责卸载至FPGA，从而减轻CPU工作负载、释放内存资源. 借助RDMA，服务器到服务器的数据存储传输不需要CPU，从而使整个过程能够以超低网络延迟完成.
        1. 特殊存储接口的加速

            阿里巴巴集团数据库事业部不久前公布了FPGA KV存储引擎X-Engine，它提高了50%以上的吞吐性能；微软研究院公布的一个基于FPGA加速实现的KV存储服务，其核心思想是依赖可编程网卡将KV存储的核心逻辑在网卡的FPGA中实现，然后FPGA通过RDMA机制访问宿主机的物理内存，从而达到GET请求12.2亿QPS和PUT请求6.1亿QPS的超高性能.
    1. 智能网卡加速

        与传统网卡的区别在于，智能网卡考虑的是如何将主机CPU上的工作放到网卡上来完成.

        智能网卡的处理能力依赖FPGA，因为FPGA组件具备了在本地编程运行软件代码的能力, 因为网卡带上了"智能".

        智能网卡看上去更像是一个小系统，完成相应的处理任务后，根据需要和主机CPU进行通信.

        目前业界已经提供了基于FPGA的成熟方案，这些智能网卡集成了来自英特尔或Xilinx的FPGA，其中一些还采用了最新的FPGA技术，基本的I/O控制器的功能完全由FPGA编程来实现.
    1. Intel QAT

        Intel QAT（Quick Assist Technology）就是一款专注数据安全和压缩的硬件加速器，用于助力数据中心性能提升.

        Intel QAT与btrfs的压缩功能合用可实现不增加CPU开销的情况下对数据进行透明压缩和解压缩，从而达到节省磁盘空间和磁盘读/写带宽、增加磁盘读/写吞吐量（IOps）的目的.

        采用英特尔提供的QATzip或zlib软件库补丁，用户能快速地利用QAT的硬件优化功能，来加快RocksDB进行读/写操作时带来的压缩和解压缩工作的速度，也能优化数据库文件合并时的解压、排序、合并、压缩的流程.
    1. NVDIMM（Non-Volatile Dual In-Line Memory Module）为存储加速
1. 基于软件的加速

    1. 智能存储加速库（Intelligent Storage Acceleration Library，ISA-L）

        ISA-L是一套在IA架构上加速算法执行的开源函数库，目的在于解决特定存储市场的计算需求.

        ISA-L底层函数都是使用汇编语言代码编写的，通过使用高效的SIMD指令和专用指令，最大化地利用CPU的微架构来加速存储算法的计算过程. 通过源码包中的C示例函数，ISA-L可以非常容易地理解并整合到客户的软件系统中.

        ISA-L中的算法函数覆盖了数据保护、数据安全、数据完整性、数据压缩及数据加密，例如，纠删码用于磁盘阵列的同位检查，防止数据传输错误的CRC算法；从MD5、SHA1到SHA512等多种安全哈希算法.
    1. 存储性能软件的加速库（SPDK）

        > [Linux 5.1合入了一个新的异步IO框架和实现：io_uring](https://kernel.taobao.org/2019/06/io_uring-a-new-linux-asynchronous-io-API/). 在polling模式下，io_uring能与spdk接近.

        用于加速NVMe SSD作为后端存储使用的应用软件加速库. 这个软件库的核心是**用户态**、异步、轮询方式的NVMe驱动. 相比内核的NVMe驱动，SPDK可以大幅降低NVMe command的延迟，提高单CPU核的IOps，形成一套高性价比的解决方案，如SPDK的vhost解决方案可以被应用到HCI中加速虚拟机的NVMe I/O.

        从目前来讲，SPDK并不是一个通用的适配解决方案. 把内核驱动放到用户态，导致需要在用户态实施一套基于用户态软件驱动的完整I/O栈.

        > 目前SPDK提供了非常简单的文件系统blobfs/blostore，但是并不支持POSIX(可移植操作系统接口).

        目前SPDK使用比较好的场景有以下几种:
        1. 提供块设备接口的后端存储应用，如iSCSI Target、NVMe-oF Target
        1. 对虚拟机中I/O的加速，主要是指在Linux系统下QEMU/KVM作为Hypervisor管理虚拟机的场景，使用vhost交互协议，实现基于共享内存通道的高效vhost用户态Target. 如vhost SCSI/blk/NVMe Target，从而加速虚拟机中virtio SCSI/blk及Kernel Native NVMe协议的I/O驱动. 其主要原理是减少了VM中断等事件的数目（如interrupt、VM_EXIT），并且缩短了host OS中的I/O栈.
        1. SPDK加速数据库存储引擎，通过实现RocksDB中的抽象文件类，SPDK的blobfs/blobstore目前可以和RocksDB集成，用于加速在NVMe SSD上使用RocksDB引擎，其实质是bypass kernel文件系统，完全使用基于SPDK的用户态I/O栈. 此外，参照SPDK对RocksDB的支持，亦可以用SPDK的blobfs/blobstore整合其他的数据库存储引擎.

        ![Linux内核NVMe驱动和SPDK NVMe驱动实现的区别](/misc/img/io/Image00063.jpg)

        SPDK应用:
        - spdk vhost target
        - spdk  iscsi target
        - spdk nvme-of target
        - spdk rpc

## 软件定义存储
OpenSDS就是适用于多云环境下的存储资源统一编排调度的存储控制器，它可以提供如下的功能:
- 标准：OpenSDS目的是建立一套关于软件定义存储的开放标准
- 服务发现（Discovery）：把每一个存储的后端作为一个服务，来进行后端的一些资源池及一些能力的上报
- 资源池：提供一套统一的资源池，供上面的云平台进行调度
- 服务发放（Provisioning）：针对存储相关的业务提供一个服务发放的功能
- 管理：有一个统一的控制器，对下面的存储资源进行统一的管理
- 自动化：OpenSDS的目标是提供一套用于云化存储的自动化的一个解决方案
- 自服务（Self-service）：可以提供自服务的功能，比如说会有一些内部的系统监控，以保证系统的高可用性
- 异构：定位是解决现在存储异构的一些统一的管理问题
- 编排：会提供一套基于策略的编排调度的框架

OpenSDS包括两个核心的子项目：Sushi与Hotpot: Sushi是OpenSDS的北向插件项目，与容器、OpenStack等云管理平台对接; Hotpot是OpenSDS的Controller项目.

## 驱动
### 用户态驱动
参考:
- [<<linux开源存储全栈详解#第4章　存储加速 - 4.4.1　SPDK NVMe驱动 - 用户态驱动>>]

用户态驱动出现的目的就是减少软件本身的开销，包括比如上下文切换、系统调用等. 在用户态，目前可以通过UIO（Userspace I/O）或VFIO（Virtual Function I/O）两种方式对硬件固态硬盘设备进行访问.

要在用户态实现设备驱动，主要需要解决以下两个问题:
1. 如何访问设备的内存：Linux通过映射物理设备的内存到用户态来提供访问，但是这种方法会引入安全性和可靠性的问题.

1. 如何处理设备产生的中断：中断本身需要在内核处理，因此针对这个限制，还需要一个小的内核模块通过最基本的中断服务程序来处理. 这个中断服务程序可以只是向操作系统确认中断，或者关闭中断等最基础的操作，剩下的具体操作可以在用户态处理.

#### UIO
UIO框架最早于Linux 2.6.32版本引入，其提供了在用户态实现设备驱动的可能性.

UIO通过限制不相关的物理设备的映射改善映射物理设备的内存到用户态带来的安全性和可靠性问题. 由此基于UIO开发的用户态驱动不需要关心与内存映射相关的安全性和可靠性的问题.

用户态驱动和UIO内核模块通过/dev/uioX设备来实现基本交互，同时通过sysfs来得到相关的设备、内存映射、内核驱动等信息.

#### VFIO
相对于UIO，VFIO不仅提供了UIO所能提供的两个最基础的功能，更多的是从安全角度考虑，把设备I/O、中断、DMA暴露到用户空间，从而可以在用户空间完成设备驱动的框架.

VFIO里的一个难点是如何将DMA以安全可控的方式暴露到用户空间，防止设备通过写内存的任意页来发动DMA攻击.

IOMMU（I/O Memory Management Unit）的引入对设备进行了限制，设备I/O地址需要经过IOMMU重映射为内存物理地址. 那么恶意的或存在错误的设备就不能读/写没有被明确映射过的内存. 操作系统以互斥的方式管理MMU和IOMMU，这样物理设备将不能绕过或污染可配置的内存管理表项.

推荐通过采用VFIO的方式来实现用户态驱动. SPDK用户态驱动同时支持UIO和VFIO两种方式.