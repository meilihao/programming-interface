# 存储
参考:
- [阿里云分布式存储系统的研究与分享](https://blog.7street.top/2018/04/13/pangu/)
- [浅谈分布式存储系统Pangu2.0——它让双11运维变得智能起来](https://segmentfault.com/a/1190000012526855)
- [阿里云携ESSD高性能云盘亮相2019全球闪存峰会，领跑微秒存储时代](https://www.csdn.net/article/a/2019-08-23/15979950)
- [NAS、OSS和EBS的区别](https://help.aliyun.com/document_detail/140812.html?spm=5176.cnnas.0.0.78d06689n3cMGJ)

## 概念
- 带内(In Band) : 控制信令和数据走同一条路线.
- 带外(Out Band) : 控制信令和数据分开走.
- 静默损毁(Silent Data Corruption) : 经过若干路径(cpu, 内存, 总线, io芯片等)持久化后的数据与实际数据不一致.
- DIF(Data Integrity Field, 数据一致性保护) : 保障存储侧的数据一致性.
- Erasure-Code（简称EC，也称擦除码或纠删码）: 1组数据冗余和恢复算法的统称.
- 条带化(striping)

  一种自动的将I/O 的负载均衡到多个物理磁盘上的技术, 避免多进程同时访问磁盘导致冲突, 而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I/O 并行能力，从而获得非常好的性能

  条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法
- [LUN(Logical Unit Number)](https://support.huawei.com/enterprise/zh/doc/EDOC1100096895)

  逻辑单元号，是SCSI中的概念. 它扩充了Target ID(因为SCSI总线上可挂接的设备数量是有限的). 每个Target下都可以有多个LUN Device.

  LUN的神秘之处在于，它很多时候不是什么可见的实体，而是一些虚拟的对象: 磁盘空间等. 比如一个存储磁盘阵列在SCSI总线看来是一个Target，占用一个SCSI的Target地址，但存储阵列的存储空间太大，我们需要将其分成不同的部分，以供不同的应用，达到集中存储，集中管理的目的, 所以在分割出来的每个存储部分（或区域）就用lun来区别.

  操作系统识别的最小存储对象级别就是LUN Device, os会认为它就是物理磁盘. 我们通常简称LUN Device为LUN. 主机的RAID卡/HBA卡识别存储资源就是靠 target + lun. 许多厂商的存储设备只支持一个RAID组作为一个LUN. 

  LUN是对存储设备而言的，volume是对主机而言的. 在LUN上可创建VOLUME. 此时VOLUME相对于主机是一个逻辑设备.
- NL-SAS
  
  NL-SAS硬盘就是SATA的盘体与SAS连接器的组合体, NL-SAS硬盘的转速只有7200转，因此性能比SAS硬盘差. 由于使用了SAS接口，所以在寻址与速度上有了提升.
  NL-SAS硬盘拥有了SATA的硬盘容量，SAS硬盘的可靠性，而价格却与SATA硬盘很接近.
- SAS/SATA

  SAS是Serial Attach SCSI（串行SCSI）, SATA是串行ATA.

## 硬盘
参考:
- [存储基础：ATA、SATA、SCSI、SAS、FC](https://blog.csdn.net/solaraceboy/article/details/79122090)

低级格式化: 划分磁道和扇区的过程, 通常由厂商完成.
高级格式化: 对磁盘上所存储的数据进行文件系统的标记.
柱面: 所有盘面上的同一磁道, 在竖直方向上构成一个柱面. 数据的读写按柱面进行, 从上往下在同一柱面的不同盘面进行操作, 只有该柱面的所有操作完成后磁头才会移动到下一个柱面.
扇区: 将每个环形磁道等距切割, 形成等长度的圆弧, 每个圆弧就是一个扇区. 每个扇区 = 扇区标识符 + 扇区数据段(4K). 扇区地址使用LBA编址方式(线性地址).

> 以前每个磁道的扇区数相同,且每个扇区的容量相同. 现在很多硬盘采用同密度盘片，意味着内外磁道上的扇区数量不同，外磁道的扇区数量有所增加.
> 磁盘每个时刻只允许一个磁头读写数据.

影响磁盘性能的因素:
1. 转速
1. 寻道速度
1. 接口速度
1. 单碟容量

硬盘类型:
- HDD
- SSD

  为优化磁道寻址而提出的"电梯算法"不适合ssd.
- 非易失性存储, 比如Intel Optane

磁盘接口技术:
- ATA (AT Attachment)接口标准是IDE(Integrated DriveElectronics)硬盘的特定接口标准, 读写速度在100MB/s左右, 已淘汰
- SATA(Serial ATA) : 当前最新是SATA 3.0, 理论带宽6Gbps, 读写速度不超过600MB/s.

  SATA的热拔插功能形同虚设: 虽然可以热插拔, 但不能象SCSI、FC和SAS那样，具有SAF-TE机制用指示灯显示具体的坏盘位置, 导致可能取下好盘.

  - mSATA, 性能与SATA一样, 但硬盘更小巧, 通常用于轻薄笔记本上.
- M.2 :  用于取代mSATA的接口, 其在传输带宽, 磁盘容量, 小巧轻薄上都更有优势.

  M.2接口的SSD又分为:
  1. AHCI标准, AHCI每条命令需要读取4次寄存器, 一共消耗8000次cpu循环, 从而造成2.5μs的延迟.
  1. NVMe标准, 使用PCIE通道直连CPU, 有效降低数据延迟, 并精简了调用方式. NVMe指令不用读取寄存器, PCIE 3.0*4理论带宽是32Gbps.
- SCSI

  参考:
  - [Linux SCSI 子系统剖析](https://www.ibm.com/developerworks/cn/linux/l-scsi-subsystem/)

  小型计算机系统接口（英语：Small Computer System Interface; 简写：SCSI），一种用于计算机和智能设备之间（硬盘、软驱、光驱、打印机、扫描仪等）系统级接口的独立处理器标准.
  它是一种智能的通用接口标准，它具备与多种类型的外设进行通信的功能. SCSI采用ASPI（高级SCSI编程接口）的标准软件接口使驱动器和计算机内部安装的SCSI适配器进行通信.
  SCSI并不是专门为硬盘设计的接口, 而是一种广泛应用于小型机上的高速数据传输技术. SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低(因为有独立的数据处理芯片)，以及热插拔等优点. 但较高的价格使得它很难如SATA硬盘般普及，因此SCSI硬盘主要应用于中、高端服务器和高档工作站中.

  SAS(serial attached SCSI)是串行连接的SCSI,是新一代的SCSI技术，和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等.
  此接口的设计是为了改善存储系统的效能、可用性和扩充性，提供与SATA硬盘的兼容性(可将SATA看做是SAS的子协议).

  SCSI 命令是在 Command Descriptor Block (CDB) 中定义的.

  在 ${kernel}/drivers/scsi 可以找到 SCSI 子系统（SCSI 较高层、中间层和各种驱动器）的源代码. SCSI 数据结构则位于 SCSI 源目录，在 ${kernel}/include/scsi 也可以找到.
- NVMe : 全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升

> NVMe标准定义了SSD的访问命令及操作方式，并且定义了逻辑设备接口标准；和SATA体系类比，NVMe标准替代了SATA体系中的AHCI逻辑接口以及ATA/SCSI命令规范.
> NVMe over Fabric，有两种类型的传输方式: 使用远程直接内存访问(RDMA)和使用光纤通道(FC-NVMe), 用以取代iSCSI和走光纤通道的SCSI.
- FC（Fiber Channel）

  FC即为光纤通道技术，最早应用于SAN (存储局域网络). FC接口是光纤对接的一种接口标准形式.

## SES(SCSI Enclosure Services)
SES是SCSI协议中用于查询设备状态(温度/风扇/电源/指示灯)的一项服务. 这里的设备可以是移动硬盘盒/磁盘阵列柜/硬盘托架等.SES可以让主机端透过SCSI命令去控制外接SCSI设备的电源/风扇以及其他与数据传输无关的东西.要使用这项技术,外置设备和主机上的SCSI/ATA控制芯片都需要支持SES技术才OK. 事实上,目前大多数外置移动硬盘和所有磁盘阵列柜都支持SES规范.

## 磁盘柜(disk enclosure)
磁盘柜一般分为磁盘阵列(disk array)和磁盘簇(jbod).

jbod(just a bundle of disks), 在linux中它相当于linear raid, 但这不是标准的raid级别, jbod在逻辑上将几个物理磁盘串联在一起, 从而提供一个大的逻辑磁盘, 存取性能完全等同于对单一磁盘的操作, 它也不提供数据安全保障.

磁盘阵列(disk array)和磁盘簇(jbod)通常包括以下主要部分:
- 两个电源(power supply)和风扇(fan)模块
- 两个链路控制卡(link control card, lcc)
- 一个背板(midplane)

除了背板, 其他组件都是冗余的, 因此存在"单点故障", 因此会设计成无源(不需要外部电源供电)背板以增加可靠性.

有些磁盘柜可通过GPIO管理外围器件, 比如磁盘故障时, 点亮对应的led.

## 存储适配器
按照协议可分为scsi/fc/sata/sas适配器等.

在适配器上电后其firmware独立于os运行, 而其bios驱动会被主板bios映射到特定的内存空间上执行, 根据需要可跳过bios驱动的执行. 如果bios驱动要保存信息, 可保存在其EEPROM中.

通常提供raid功能的存储适配器都需要bios驱动, 以实现在进入os前进行raid配置, 否则bios驱动不是必须的, 但要支持从连接到存储适配器上的磁盘进行引导进入os则必须bios驱动.

os与存储适配器通信是其驱动来实现的, 该驱动通常被编译入内核或在os运行过程中作为module载入, 该驱动通过特定的消息传递接口和该适配器的firmware进行交互, 完成拓扑发现和i/o处理, 同时通过ioctl等向管理软件提供接口支持, 实现状态查询和存储管理.

### iscsi适配器
1. 软件实现的启动器 + 标准以太网卡
1. 软件实现的启动器 + 带tcp offload engin(toe)的网卡, 以减轻cpu负担
1. 使用iscsi主机适配器, 即硬件方式的iscsi卡

## 备份
分为:
- 基于文件的备份
- 基于数据块的备份, 绕过文件系统, 更加快捷, 但颗粒度粗.

### 快照
它是一个数据对象产生完全可用的副本, 包含对该数据对象在某一时刻的映像. 它的实现可分为:
1. 写前复制(copy on write)
1. 写时转向, 典型例子: WAFL(write anywhere file layout)文件系统
1. 拆分镜像, 典型例子: MEC的TimeFinder.

快照一致性: 快照前必须将缓存flush到磁盘.

### CDP(continuous data protection), 连续数据保护技术
它是一种近实时捕获和保存数据变化的技术, 允许将数据恢复到之前的任意时间点.

基本原理: 写前复制+时间戳

## 多路径
使用一个以上的物理路径来访问网络存储设备, 并通过容错, i/o流量负载均衡甚至更细粒度的i/o调度测量等方式, 为网络存储设备提供更高的可用性和性能.

多路径软件的作用就是识别出系统中的多路径设备, 将它作为一个设备来管理, 同时管理这些路径, 以实现i/o流量的负载均衡和i/o路径故障时的切换和恢复.

实现方式:
- hba驱动
- 内核. 在linux中支持基于software raid的多路径技术和基于device mapper的多路径技术.
- 应用程序, 比如EMC PowerPath, IBM RDAC等

实现多路径的关键是磁盘标识, 方法有2种:
1. scsi inquiry.
1. world wide node name. 每个物理磁盘有两个WWPN地址, 但只有一个WWNN地址.

## 虚拟化
- 按层次: 块级和文件级虚拟化. linux的Multi-Disk和Device Mapper是块级虚拟化的典型.
- 按形式: 聚合虚拟化, 拆分虚拟化和仿真虚拟化(比如虚拟磁带库).
- 按实现方式: 带内虚拟化(在服务器和存储设备的数据通道中实现)和带外虚拟化(实现位于数据通道之外, 通过传送控制信息来完成物理设备和逻辑卷的映射)
- 按位置: 基于服务器的虚拟化, 基于网络的虚拟化和基于存储子系统的虚拟化.

## RAID
Redundant Array of Indepent Disks (独立磁盘冗余阵列) 是为了增加容量(capacity), 提升性能(performance), 增强容错(redundancy)和降低成本(cost).

一共有0~6一共7种，这其中RAID 0、RAID1、RAID 5和RAID6比较常用.

![总结](/misc/img/io/20160712135213243.png)

软raid是允许在操作系统之上, 因此系统盘无法设置成raid模式, 解决方法: raid卡.

raid卡写模式:
- WriteBack : 数据写入其缓存即通知系统已完成, 掉电时有丢数据风险, 可通过电池维持缓存来解决
- WriteThrough(写透) : 数据真实写入存储后通知系统已完成.

RAID 2.0（独立磁盘冗余数组2.0, Redundant Array of Independent Disks Version 2.0），为增强型RAID技术，有效解决了机械硬盘容量越来越大，重构一块机械硬盘所需时间越来越长，传统RAID组重构窗口越来越大而导致重构期间又故障一块硬盘而彻底丢失数据风险的问题.

> 华为有[raid 2.0+](http://www.learnfuture.com/article/1272)
> [华为存储虚拟化解决方案](https://kkutysllb.cn/2019/05/29/2019-05-29-%E5%8D%8E%E4%B8%BA%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/)
> [Linux原生的存储虚拟化软RAID和LVM](https://kkutysllb.cn/2019/06/09/2019-06-09-Linux%E5%8E%9F%E7%94%9F%E7%9A%84%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BD%AFRAID%E5%92%8CLVM/)

### RAID 0
![](/misc/img/io/20160712135137243.png)
RAID 0的特点：
- 最少需要2块磁盘
- 数据条带式分布
- 没有冗余，性能最佳(不存储镜像、校验信息)
- 安全性能低, 不支持数据冗余，无容错功能，不能应用于对数据安全性要求高的场合即不适用于关键任务环境
- 磁盘空间使用率：100%，成本最低

线性raid(jbod)和raid0区别: raid0有条带.

### RAID 1
![](/misc/img/io/20160712135148165.png)
RAID 1的特点：
- 最少需要2块磁盘
- 提供数据块冗余, 磁盘的利用率却只有50%，磁盘利用率最低，成本最高
- 数据冗余备份，容错，数据安全性高；当故障发生时，系统可自动切换到镜像磁盘，无需重组数据
- 读性能：只能在一个磁盘上读取，取决于磁盘中较快的那块盘
- 写性能：两块磁盘都要写入，虽然是并行写入，但因为要比对，故性能单块磁盘慢

### RAID 3
![]()
实际数据占用的有效空间为N个硬盘的空间总和，而第N+1个硬盘上存储的数据是校验容错信息

特点:
- n最少为3
- **校验码集中在单独的一块磁盘**

### RAID 5
![](/misc/img/io/20160712135201306.png)
RAID 5特点：
- 最少3块磁盘
- 数据条带形式分布
- 高可靠性, 奇偶校检与数据分别都存储不同盘. 损坏时，可以自动重建, 但只允许一块磁盘损坏
- 适合多读少写的情景，是性能与数据冗余最佳的折中方案
- (n-1)*单块磁盘的读性能，和RAID0相近似的数据读取速度
- 写入数据的速度比对单个磁盘进行写入操作稍慢
- 磁盘空间利用率：(N-1)/N，即只浪费一块磁盘用于奇偶校验, 存储成本相对较低

Raid6:
类似RAID5两组奇偶校验信息块，两个独立的奇偶系统使用不同的算法， 数据的可靠性非常高，即使两块磁盘同时失效也不会影响数据的使用.
相对于RAID 5有更大的`写损失`，因此`写性能`非常差.

4坏2的恢复过程(推测): 先参照raid5恢复一块盘, 再结合其他三盘恢复第四块盘.

RAID 7:
三奇偶校验

raid 5~7均是分布式校验码

**raid5~7没有上限(不考虑阵列卡或软raid性能的限制),但有下限**

### RAID 10
![](/misc/img/io/20160712135207894.png)
RAID 10(又叫RAID 1+0)特点：
- 最少需要4块磁盘
- 先按RAID 0分成两组，再分别对两组按RAID 1方式镜像, 安全性上RAID10要好于 RAID01
- 兼顾冗余(提供镜像存储)和性能(数据条带形分布)
- 在实际应用中较为常用
- 磁盘利用率50% 成本较高
- RAID01比起RAID10有着更快的读写速度

## DAS、SAN、NAS网络存储的异同
参考:
- [存储相关知识-DAS/SAN/NAS](https://yq.aliyun.com/articles/467199)

![](/misc/img/io/s8xI-fyfrfvv5160915.jpg)
NAS、SAN和DAS三者的目的相同：存储, 但是初衷、达成路径和结果也不相同.

> 从NAS与SAN系统的角度来看，NAS用于文件I/O，SAN用于块I/O. 比较NAS与SAN时需要记住的另一件事是，NAS最终将文件I/O请求转换为与其相连的存储设备的块访问.
> 供应商现在开始提供具有NAS或SAN体验的前端系统，而后端则基于对象存储
> 共享访问文件的环境，最好使用 NAS, 因为它自带文件系统.

DAS(Direct-Attached Storage), 是某台独立服务器中的一部分(**通过总线直接连接设备**)，将外置存储通设备通过SCSI或FC接口直接连接到应用服务器上来使用. 事实上，这种DAS存储模式，一般在中小型企业汇总应用十分广泛.
它更依赖主机的操作系统来实现数据的IO读写、数据管理、数据备份等工作, 但是这种存储模式也存在一定的缺点, 比如可管理性差、弹性扩展能力弱，难以跟上IT发展趋势等问题.

NAS是前端用网络文件系统提供服务, 后端用任何方式的存储空间, 即通过网络文件系统提供服务的存储设备.

NAS 和 SAN 存储系统的区别是 NAS 有自己的文件系统. 即**NAS提供文件系统, 而SAN提供的是块设备, 比如磁盘等.
NAS 是将目光集中在应用、用户和文件以及它们共享的数据上. SAN 是将目光集中在磁盘、磁带以及联
接它们的可靠的基础结构. 将来从桌面系统到数据集中管理到存储设备的全面解决方案将是`NAS + SAN`.

## SAN
SAN(Storage Area Network)存储区域网络，是一种高速的、专门用于存储操作的网络，通常独立于计算机局域网(LAN).
SAN将主机和存储设备连接在一起，能够为其上的任意一台主机和任意一台存储设备提供专用的通信通道.
SAN将存储设备从服务器中独立出来，实现了服务器层次上的存储资源共享.
SAN将通道技术和网络技术引入存储环境中，提供了一种新型的网络存储解决方案，能够同时满足吞吐率、可用性、可靠性、可扩展性和可管理性等方面的要求.

与ip-san比, fc-san败退原因:
1. 成本高
1. 可扩展性相对较弱
1. 易用性相对较弱
1. 兼容性相对较差
 
### FC-SAN
通常SAN由磁盘阵列(RAID)连接FC(Fibre Channel, 网状通道)组成(为了区别于IP SAN，通常SAN也称为FC-SAN). SAN和服务器和客户机的数据通信通过SCSI命令而非TCP/IP，数据处理是`块级(lock level)`.

> FC是一套网络协议, 与tcp/ip类似, 传输介质是光纤(fiber).
> 目前我查到的FC最快是16Gb/s.
> FC磁盘已经淘汰.
> FCoE现基本被抛弃了, NVMe/TCP开始流行.
> FC已被SAS和IP淘汰中.

### IP-SAN
简单来讲，IP-SAN（IP存储）的通信通道是使用IP通道，而不是光纤通道，把服务器与存储设备连接起来的技术，除了标准已获通过的iSCSI，还有FCIP、iFCP等正在制定的标准, 而iSCSI发展最快，已经成了IP存储一个有力的代表.
像光纤通道一样，IP存储是可交换的，但是与光纤通道不一样的是，IP网络是成熟通用的，不存在互操作性问题，而光纤通道SAN最令人头痛的就是这个问题.

IP存储的优势：
1. 利用无所不在的IP网络，一定程度上保护了现有投资.
2. IP存储超越了地理距离的限制, 十分适合于对现存关键数据的远程备份.
3. IP网络技术成熟, IP存储减少了配置、维护、管理的复杂度.

> aliyun已使用25Gb/s网络

### iSCSI
参考:
- [iSCSI存储技术全攻略](https://www.sansky.net/article/2007-12-03-iscsi-storage.html)

iSCSI即internet SCSI，是IETF制订的一项存储传输协议标准，目的是为了用IP协议将存储设备连接在一起.
它是将SCSI数据区块映像成以太网数据封包, 并通过IP网络来传输SCSI 数据区块的方法.
称作Initiator（发起方），被请求的目标设备则称作Target, 透过这种方式可在IP网络上以区块级模式存取大量数据.

组成:
- iSCSI initiator

    发起读写请求的来源机器设备，诸如服务器，连接在IP网络并对iSCSI target发起请求以及接收响应. 每一个iSCSI主机通过唯一的IQN来识别，类似于光纤通道的WWN. 要在IP网络上传递SCSI块命令，必须在iSCSI主机上安装iSCSI驱动.
- iSCSI Target

    iSCSI target是接收iSCSI命令的设备. 此设备可以是终端节点，如存储设备，或是中间设备，如IP和光纤设备之间的连接桥. 每一个iSCSI target通过唯一的IQN来标识，存储阵列控制器上（或桥接器上）的各端口通过一个或多个IP地址来标识.

iSCSI 会话:
iSCSI 会话建立于一个initiator与一个target之间，一个会话允许多个TCP连接，并且支持跨连接的错误恢复.
建立一个iSCSI会话，包括：
1. 命名阶段：确定需要访问的存储，以及initiator，与FC不同，命名与位置无关
1. 发现阶段：找到需要访问的存储
1. 登录阶段：建立于存储的连接，读写之前首先进行参数协商，按照TCP连接登录

> ISCSI技术的核心是在TCP/IP网络上传输SCSI协议，是指用TCP/IP报文、和ISCSI报文封装SCSI报文，使得SCSI命令和数据可以在普通以太网络上进行传输.

## 指标
Recovery Point Objective (RPO) : 指的是最多可能丢失的数据的时长.
Recovery Time Objective (RTO) : 指的是从灾难发生到整个系统恢复正常所需要的最大时长.

## 串行总线磁盘为什么比并行总线磁盘快
实际的传输速率是位宽和频率的乘积，如果频率相同，没错16bit的并口传输是串口的传输的十多倍，但是随着频率的大幅提高，并口多根信号线之间的串扰越来越严重.
当达到一定频率之后，并口传输的数据开始失真，就不能再提高传输频率了; 而串口通信一般是采用的差分信号，功耗低，串扰问题很小，因此可以大幅提高通信频率.

## LVM(Logical Volume Manager)
- pv(physical volume) : lvm通过os识别到的物理盘或逻辑盘
- vg(volume group) : 多个pv被逻辑地组成一个组
- pp(physical partition, 物理区块) : 在逻辑上将vg分割成连续的小块. lvm会记录pp的大小和pp序号的偏移.
- lp(logical partition, 逻辑区块) : 若干PP组成的逻辑块.
- lv(logical volume, 逻辑卷) : 若干lp组成的卷, 是lvm最终用来存储数据的单位.

使用步骤:
1. 创建pv, 将pv加入vg
2. 在vg中创建lv, 最后格式化lv即可
   
## 备份
备份分类:
1. 文件级备份
1. 快级备份
   
   直接通过磁盘控制器而无需考虑文件系统, 但会备份无用快


快照(**只读**):
- Copy on First Write, CoFW, 常见.

  元数据复制后, 新的数据写入时需先将原数据块复制出来让后再覆盖写入, 更新快照元数据相应条目指向复制的数据块.
  一读两写(block)), 不影响读性能.
  删除快照时, 可直接清理, 没有后遗症.
- Redirect on First Write, RoFW

  元数据复制后, 新的数据会重定向写入到某剩余空间, 之后更新原元数据对应条目的指向.
  容易产生碎片, 影响读性能; 因为一写导致更省io
  删除时, 数据块散乱, 不好清理, 此时影响读写性能.

> 做快照时, 新卷的容量是原卷的30%(经验值, 需根据业务调整).

卷clone: 源数据集在某时刻的若干份实实在在的复制实体.

## CDP(Continuous Data Protect, 持续数据保护)
一种在不影响数据运行的前提下, 可以实现持续捕捉或跟踪目标数据发生的任何改变, 并且能够恢复到此前任意时间点的方法.

> cdp起源于linux的cdp模块, 它持续地捕获所有I/O请求，并且在这些请求打上时间戳标志, 再将数据变化以及时间戳保存下来，以便恢复到过去的任意时刻.
> cdp的核心思想是时间戳.

cdp可提供的备份级别
- 快级
  
  捕捉底层卷的写io变化, 并将每次变化的快数据打入时间戳再保存下来.
- 文件级
  
  通过监控文件系统动作, 文件的每一次变化(含元数据), 比如日志型的文件系统.
- 应用级
  
  比如DB的WAL.

启用CDP时, CoW会三写, 此时使用RoW更优.

> CDP绝不适合大数据量改动的场合.

## 文件系统
精髓: metadata(链表,B树和位图等结构)和对这些metadata的管理方式.

fs一定是先写文件实体数据, 再将元数据从缓存中flush到磁盘.

## 配置项
- read_ahead_kb

  增大`/sys/block/${disk}/queue/read_ahead_kb`可提升连续读的性能. 不过大多数服务器工作负载主要是随机io, 则提升有限. 

  > ionice允许在进程级别设置优先级, 限制其对磁盘子系统的使用率.

- noatime : 禁止更新文件的access time, 减少不必要的io

  ```
  // /etc/fstab
  UUID=948d792c-2d5a-4f81-97b4-811b34749d8f /home/jr/git ext4 defaults,nodelalloc,noatime,nodiratime        0  0
  ```

## FAQ
### iscsi和nas区别
参考:
- [iSCSI vs NAS: Full Comparison](https://www.msp360.com/resources/blog/iscsi-nas-comparison/)


![](/misc/img/io/NAS-iSCSI-Scheme.png)

iSCSI和NAS系统之间的主要区别在于它们的存储访问体系结构. **iSCSI是SAN系统的流行实现**，而NAS是将存储设备连接到用户网络的常用方法.
换句话说，我们不应该直接比较NAS和iSCSI，因此我们选择一种NAS实施方案-SMB上的NAS存储.

SAN系统是部署为高性能**块级**存储模型, 面向单一用户(即系统)，可在最低数据级别上显示存储设备. NAS是一种**对象级**存储，面向多用户即共享, 可向客户端OS提供即用型文件和文件夹树.

SAN无法共享给多人, 否则会数据错乱. 而NAS由上层软件处理冲突, 可避免这个问题.