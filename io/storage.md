# 存储
参考:
- [阿里云分布式存储系统的研究与分享](https://blog.7street.top/2018/04/13/pangu/)
- [浅谈分布式存储系统Pangu2.0——它让双11运维变得智能起来](https://segmentfault.com/a/1190000012526855)
- [阿里云携ESSD高性能云盘亮相2019全球闪存峰会，领跑微秒存储时代](https://www.csdn.net/article/a/2019-08-23/15979950)
- [NAS、OSS和EBS的区别](https://help.aliyun.com/document_detail/140812.html?spm=5176.cnnas.0.0.78d06689n3cMGJ)

## 存储金字塔
![存储金字塔](/misc/img/io/v2-ddb5b180fe342d92f7e3be8b1e39a07e_720w.jpg)
![存储金字塔](/misc/img/io/storage_speed.jpg)
![存储器level](/misc/img/io/storage_level.png)

常见硬件性能参数:
|类别 | 耗时 |
|-|-|
|访问L1 Cache|0.5ns|
|分支预测失败|5ns|
|访问L2 Cache|7ns|
|Mutex 加锁/解锁|100ns|
|内存访问|100ns|
|千兆网络发送1MB数据|10ms|
|从内存顺序读取1MB数据|0.25ms|
|机房内网络来回|0.5ms|
|异地机房之间网络来回|30～100ms|
|SATA磁盘寻道|10ms|
|从SATA磁盘顺序读取1MB数据|20ms|
|固态盘SSD访问延迟|0.1～0.2ms|

![](/misc/img/io/Image00059.jpg)

存储器分类:
- 信息的可保存性

  1. 易失性存储: 
  
    - SRAM（Static Random-Access Memory，静态随机存取存储器）
    - DRAM（Dynamic Random Access Memory，动态随机存取存储器），比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少
  1. 非易失性存储

    1. 根据介质和工作原理

      - 机械硬盘(harddisk)
      - 固态硬盘(sdid state drive, ssd)

        - 根据颗粒和介质

          - nand
          - 3d xpoint
      - 磁带(tap)
    1. 根据协议

      - PCIe
      - SATA(serial advanced technology attachment)

        sata盘使用AHCI传输模式. AHCI需要主板,磁盘和os都支持才行.

        只有一个端口, 通过SATA转接卡上的端口选择芯片模拟出两个端口, 但实际上在某刻只有一个端口是活动的.
      - SCSI
      - SAS(serial attached scsi), 新一代scsi技术, 速度更快, cpu占用更低

        有两个端口
      - FC
      - AHCI(advanced host controller interface)

# linux存储服务
## 1. linux本地文件系统
ext4, xfs, btrfs

## 2. linux远程存储服务
1. 块设备服务

  目前基于SCSI协议的SAS和SATA只能是单个队列而且每个队列的深度也比较低，分别是254和32的队列深度。而NVMe协议设计之初就考虑了该问题，它的最大队列数量可以是64K（65535个命令队列和1个管理队列），而每个队列的深度可以高达64K.

  - iscsi
  - nvme over fabrics: nvme协议在fabrics上的延伸, 主要设计目的是让client能更高效地访问远端server上的nvme盘.
  
    它往往与RDMA(remote direct memory access)功能的以太网卡, 或者光纤通道, infiniband一起工作.
1. 文件存储服务

  - nfs(network file system)
  - cifs(common internet file system)
1. 对象存储

  文件存储和对象存储的本质区别是有无层次结构.

## 压缩
本质是用更少的数据表示更多的数据.

无损压缩可实现的基础是真实世界的信息存在大量冗余.

熵编码是指对出现的每个不同符号, 创建分配一个唯一的前缀码. 前缀码是一种可变长度码, 并且每个码字都具有前置性, 即每个码字都不会被其他码字作为前置部分.

常见压缩编码:
1. 霍夫曼编码

  原理: 为出现频率更高的字符分配更短的编码.
1. 算术编码

  算术编码与其他熵编码不同的是, 算术编码可以把整条信息编码成一个一定精度的小数q(0.0<=q<1.0), 基本原理是根据信源发射不同符号的概率, 对区间[0,1]进行划分, 区间宽度代表各符号出现的概率.

  算术编码是目前为止编码效率最高的一种统计熵编码方式, 但编码复杂.

## 重复数据删除
分文件和块级别, 常用在备份场景. zfs提供重删功能.

百度云的极速上传就是文件级别的重删体现.

按应用位置分:
- 源端重复数据删除
- 目的端重复数据删除

按应用时间点分:
- 离线重复数据删除
- 在线重复数据删除

## 概念
- 带内(In Band) : 控制信令和数据走同一条路线.
- 带外(Out Band) : 控制信令和数据分开走.
- 静默损毁(Silent Data Corruption) : 经过若干路径(cpu, 内存, 总线, io芯片等)持久化后的数据与实际数据不一致.
- DIF(Data Integrity Field, 数据一致性保护) : 保障存储侧的数据一致性.
- Erasure-Code（简称EC，也称擦除码或纠删码）: 1组数据冗余和恢复算法的统称.
- 条带化(striping)

  一种自动的将I/O 的负载均衡到多个物理磁盘上的技术, 避免多进程同时访问磁盘导致冲突, 而且在需要对这种数据进行顺序访问的时候可以获得最大程度上的 I/O 并行能力，从而获得非常好的性能

  条带（strip）是把连续的数据分割成相同大小的数据块，把每段数据分别写入到阵列中的不同磁盘上的方法
- [LUN(Logical Unit Number)](https://support.huawei.com/enterprise/zh/doc/EDOC1100096895)

  逻辑单元号，是SCSI中的概念. 它扩充了Target ID(因为SCSI总线上可挂接的设备数量是有限的). 每个Target下都可以有多个LUN Device.

  LUN的神秘之处在于，它很多时候不是什么可见的实体，而是一些虚拟的对象: 磁盘空间等. 比如一个存储磁盘阵列在SCSI总线看来是一个Target，占用一个SCSI的Target地址，但存储阵列的存储空间太大，我们需要将其分成不同的部分，以供不同的应用，达到集中存储，集中管理的目的, 所以在分割出来的每个存储部分（或区域）就用lun来区别.

  操作系统识别的最小存储对象级别就是LUN Device, os会认为它就是物理磁盘. 我们通常简称LUN Device为LUN. 主机的RAID卡/HBA卡识别存储资源就是靠 target + lun. 许多厂商的存储设备只支持一个RAID组作为一个LUN. 

  LUN是对存储设备而言的，volume是对主机而言的. 在LUN上可创建VOLUME. 此时VOLUME相对于主机是一个逻辑设备.

  zfs/lvm中的lun指来自于存储池Pool(Pool的空间来源于组成磁盘阵列的若干块硬盘)的逻辑设备, 分Thick/Thin LUN:
  - Thick LUN

    非精简LUN，是LUN类型的一种，支持虚拟资源分配，能以较为简便的方式进行创建、扩容和压缩操作.

    Thick LUN在创建完成后就会从存储池Pool中**分配满额**的存储空间，即LUN的大小完全等于分配的空间. 因此，它拥有较高的可预测性.

    Thick LUN由于在一开始就会拥有所分配的空间，所以Thick LUN在顺序读写的时候拥有较高的性能，但是会造成空间资源的浪费.

    适用场景:
    1. 对性能要求较高
    1. 对存储空间利用率不太敏感(存在空间浪费)
    1. 对成本要求不太高

  - Thin LUN

    精简LUN，也是LUN类型的一种，支持虚拟资源分配，能够以较简便的方式进行创建、扩容和压缩操作.

    Thin LUN在创建的时候，可以设置一个初始分配容量. 创建完成后，存储池Pool只会分配这个初始容量大小的空间剩余的空间仍然放在存储池中. 当Thin LUN已分配的存储空间的使用率达到阈值时，存储系统才会再从Pool中划分一定的配额给Thin LUN. 如此反复，直到达到Thin LUN最初设定的全部容量, 即使用按需分配(会导致分配的存储空间不连续). 因此，它拥有较高的存储空间利用率.

    Thin LUN由于是实时分配空间，每次扩容时，需要重新增加容量，后台重新格式化，这个时候性能会受到一定影响，而且每次分配空间可能会导致硬盘中存储空间不连续，这样硬盘读写数据时在寻找存放位置上花费的时间会较多，会在顺序读写时对性能有一定影响.

    适用场景:
    1. 对性能要求一般
    1. 对存储空间利用率敏感
    1. 对成本要求敏感

- NL-SAS
  
  NL-SAS硬盘就是SATA的盘体与SAS连接器的组合体, NL-SAS硬盘的转速只有7200转，因此性能比SAS硬盘差. 由于使用了SAS接口，所以在寻址与速度上有了提升.
  NL-SAS硬盘拥有了SATA的硬盘容量，SAS硬盘的可靠性，而价格却与SATA硬盘很接近.
- SAS/SATA

  SAS是Serial Attach SCSI（串行SCSI）, SATA是串行ATA.

## 硬盘
参考:
- [存储基础：ATA、SATA、SCSI、SAS、FC](https://blog.csdn.net/solaraceboy/article/details/79122090)

低级格式化: 划分磁道和扇区的过程, 通常由厂商完成.
高级格式化: 对磁盘上所存储的数据进行文件系统的标记.
柱面(cylinder): 所有盘面上的同一磁道, 在竖直方向上构成一个柱面. 数据的读写按柱面进行, 从上往下在同一柱面的不同盘面进行操作, 只有该柱面的所有操作完成后磁头才会移动到下一个柱面.
扇区: 将每个环形磁道等距切割, 形成等长度的圆弧, 每个圆弧就是一个扇区. 每个扇区 = 扇区标识符 + 扇区数据段(512B/4K)+错误校验码ecc(有前两者计算得到). 扇区地址使用LBA编址方式(线性地址).

> 以前每个磁道的扇区数相同,且每个扇区的容量相同. 现在很多硬盘采用同密度盘片，意味着内外磁道上的扇区数量不同，外磁道的扇区数量有所增加.
> 磁盘每个时刻只允许一个磁头读写数据.

影响磁盘性能的因素:
1. 转速

  旋转延迟(rotational latency)等待扇区旋转到磁头下面的时间.
1. 寻道速度

  寻道: 移动磁头臂到指定指定柱面.
1. 接口速度
1. 单碟容量

硬盘类型:
- HDD
- SSD

  为优化磁道寻址而提出的"电梯算法"不适合ssd.
- 非易失性存储, 比如Intel Optane

磁盘接口技术:
- ATA (AT Attachment)接口标准是IDE(Integrated DriveElectronics)硬盘的特定接口标准, 读写速度在100MB/s左右, 已淘汰
- SATA(Serial ATA) : 当前最新是SATA 3.0, 理论带宽6Gbps, 读写速度不超过600MB/s.

  SATA的热拔插功能形同虚设: 虽然可以热插拔, 但不能象SCSI、FC和SAS那样，具有SAF-TE机制用指示灯显示具体的坏盘位置, 导致可能取下好盘.

  - mSATA, 性能与SATA一样, 但硬盘更小巧, 通常用于轻薄笔记本上.

  > sata支持一个队列, 一次只能接收32条数据.
- M.2 :  用于取代mSATA的接口, 其在传输带宽, 磁盘容量, 小巧轻薄上都更有优势.

  M.2接口的SSD又分为:
  1. AHCI标准, AHCI每条命令需要读取4次寄存器, 一共消耗8000次cpu循环, 从而造成2.5μs的延迟.
  1. NVMe标准, 使用PCIE通道直连CPU, 有效降低数据延迟, 并精简了调用方式. NVMe指令不用读取寄存器, PCIE 3.0*4理论带宽是32Gbps. 即更大带宽和更低延迟.

  > NVMe支持最对64000个队列, 每个队列有64000个条目, 极利于并行, 极大地提高了吞吐量.

  > nvme-cli : 用于监控和配置管理NVMe设备
- SCSI

  参考:
  - [Linux SCSI 子系统剖析](https://www.ibm.com/developerworks/cn/linux/l-scsi-subsystem/)

  小型计算机系统接口（英语：Small Computer System Interface; 简写：SCSI），一种用于计算机和智能设备之间（硬盘、软驱、光驱、打印机、扫描仪等）系统级接口的独立处理器标准.
  它是一种智能的通用接口标准，它具备与多种类型的外设进行通信的功能. SCSI采用ASPI（高级SCSI编程接口）的标准软件接口使驱动器和计算机内部安装的SCSI适配器进行通信.
  SCSI并不是专门为硬盘设计的接口, 而是一种广泛应用于小型机上的高速数据传输技术. SCSI接口具有应用范围广、多任务、带宽大、CPU占用率低(因为有独立的数据处理芯片)，以及热插拔等优点. 但较高的价格使得它很难如SATA硬盘般普及，因此SCSI硬盘主要应用于中、高端服务器和高档工作站中.

  SAS(serial attached SCSI)是串行连接的SCSI,是新一代的SCSI技术，和现在流行的Serial ATA(SATA)硬盘相同，都是采用串行技术以获得更高的传输速度，并通过缩短连结线改善内部空间等.
  此接口的设计是为了改善存储系统的效能、可用性和扩充性，提供与SATA硬盘的兼容性(可将SATA看做是SAS的子协议).

  SCSI 命令是在 Command Descriptor Block (CDB) 中定义的.

  在 ${kernel}/drivers/scsi 可以找到 SCSI 子系统（SCSI 较高层、中间层和各种驱动器）的源代码. SCSI 数据结构则位于 SCSI 源目录，在 ${kernel}/include/scsi 也可以找到.
- NVMe : 全称Non-Volatile Memory Express，非易失性存储器标准，是使用PCI-E通道的SSD一种规范，相比与现在的AHCI标准，NVMe标准可以带来多方面的性能提升

> NVMe标准定义了SSD的访问命令及操作方式，并且定义了逻辑设备接口标准；和SATA体系类比，NVMe标准替代了SATA体系中的AHCI逻辑接口以及ATA/SCSI命令规范.
> NVMe over Fabric，有两种类型的传输方式: 使用远程直接内存访问(RDMA)和使用光纤通道(FC-NVMe), 用以取代iSCSI和走光纤通道的SCSI. nvme-of与nvme有90%的相同, 只是在nvme transport部分进行了扩展以支持infiniband, 光纤通道等.
- FC（Fiber Channel）

  FC即为光纤通道技术，最早应用于SAN (存储局域网络). FC接口是光纤对接的一种接口标准形式.

![存储接口协议](/misc/img/io/v2-df6c31c1b8a90b8d87b9f0f274eac217_720w.jpg)

AHCI设计之初是面向机械硬盘, 针对的是高延迟的机械盘的优化, 因此它不能发挥固态硬盘的优势, 而NVMe因此应运而生.

随着nvme ssd的普及, 本地的计算能力已不能完全发挥固态硬盘的全部性能, 就有了两种改进方法:
1. 减少软件开销, 即SPDK
1. 计算和存储分离, 这带来了带宽和延迟上的挑战, 从而催生了NVMe-of.

## SES(SCSI Enclosure Services)
SES是SCSI协议中用于查询设备状态(温度/风扇/电源/指示灯)的一项服务. 这里的设备可以是移动硬盘盒/磁盘阵列柜/硬盘托架等.SES可以让主机端透过SCSI命令去控制外接SCSI设备的电源/风扇以及其他与数据传输无关的东西.要使用这项技术,外置设备和主机上的SCSI/ATA控制芯片都需要支持SES技术才OK. 事实上,目前大多数外置移动硬盘和所有磁盘阵列柜都支持SES规范.

## 磁盘柜(disk enclosure)
磁盘柜一般分为磁盘阵列(disk array)和磁盘簇(jbod).

jbod(just a bundle of disks), 在linux中它相当于linear raid, 但这不是标准的raid级别, jbod在逻辑上将几个物理磁盘串联在一起, 从而提供一个大的逻辑磁盘, 存取性能完全等同于对单一磁盘的操作, 它也不提供数据安全保障.

磁盘阵列(disk array)和磁盘簇(jbod)通常包括以下主要部分:
- 两个电源(power supply)和风扇(fan)模块
- 两个链路控制卡(link control card, lcc)

  链路卡间通过心跳线(低速信号线, 比如I2C或串行线)连接到对方, 以便轮询对方的活动状态.  

  每个链路卡上有flash(保存firmware)和EEPROM(SAS地址等信息). 有些卡上还有独立的管理芯片, 用于通过GPIO等管理外围器件, 比如磁盘故障时, 点亮对应的led.
- 一个背板(midplane)

  磁盘是通过SAS或SATA转接卡连接到背板上的.

除了背板, 其他组件都是冗余的, 因此存在"单点故障", 因此会设计成无源(不需要外部电源供电)背板以增加可靠性.

## 存储适配器
按照协议可分为scsi/fc/sata/sas适配器等.

存储适配器硬件上有两个接口, 一个连接到主机的i/o总线, 通常是pcie, 另一个是连接到存储i/o, 此外还有flash(保存其biso驱动和firmware)和EEPROM等器件; 软件包括固件和驱动.

在适配器上电后其firmware独立于os运行, 供其驱动和bios驱动访问, 而其bios驱动会被主板bios映射到特定的内存空间上执行, 根据需要可跳过bios驱动的执行. 如果bios驱动要保存信息, 可保存在其EEPROM中.

通常提供raid功能的存储适配器都需要bios驱动, 以实现在进入os前进行raid配置, 否则bios驱动不是必须的, 但要支持从连接到存储适配器上的磁盘进行引导进入os则必须bios驱动.

os与存储适配器通信是其驱动来实现的, 该驱动通常被编译入内核或在os运行过程中作为module载入, 该驱动通过特定的消息传递接口和该适配器的firmware进行交互, 完成拓扑发现和i/o处理, 同时通过ioctl等向管理软件提供接口支持, 实现状态查询和存储管理.

### iscsi适配器
1. 软件实现的启动器 + 标准以太网卡
1. 软件实现的启动器 + 带tcp offload engin(toe)的网卡(卸载tcp+ip), 以减轻cpu负担
1. 使用iscsi主机适配器, 即硬件方式的iscsi卡(卸载iscsi+tcp+ip)

## 备份
分为:
- 基于文件的备份
- 基于数据块的备份, 绕过文件系统, 更加快捷, 但颗粒度粗.

### 快照
ref:
- [数据备份 快照技术 之第一次写时复制（COW）和写时重定向（ROW）](https://cloud.tencent.com/developer/article/1748016)
- [ROW/COW 快照技术原理解析](http://t.zoukankan.com/jmilkfan-fanguiju-p-10589828.html)

根据 SNIA(Storage Networking Industry Association, 全球网络存储工业协会SNIA), 快照分为完整快照和增量快照, 增量快照又分COW和ROW:
- 全量快照：镜像分离（Split Mirror）。
- 增量快照：
    
    1. 写时拷贝（Copy-On-Write）
    1. 写时重定向（Redirect-On-Write）

它是一个数据对象产生完全可用的副本, 包含对该数据对象在某一时刻的映像. 它的实现可分为:
1. 写前复制(copy on write, COW)

  在**第一次**对源卷进行写操作前, 将源卷对应数据块复制到快照卷中, 后续对源卷中相应的数据块位置直接写.

1. 写时重定向（Redirect-On-Write, ROW）, 典型例子: WAFL(write anywhere file layout)文件系统

  与cow类似, 对源卷的写操作被重定向到另一个位置.
1. 拆分镜像(Split Mirror), 典型例子: MEC的TimeFinder.

使用场景区别: COW在写入时需要拷贝一份数据到快照卷的动作，而ROW是直接重定向到快照卷写入，所以ROW适合写密集型; 相反, 由于ROW不断进行指针重定向，读性能会有较大影响, 而COW不会, 所以COW适合读密集型.

应用场景: 在传统存储设备上, ROW 快照在多次读写后, 源数据卷的数据被分散, 对于连续读写的性能不如COW. 所以 ROW 比较适合 Write-Intensive(写密集) 类型的存储系统. 但是, 在分布式存储设备上, ROW 的连续读写的性能会比 COW 更加好. 一般而言, 读写性能的瓶颈都在磁盘上. 而分布式存储的特性是数据越是分散到不同的存储设备中, 系统性能越高. 所以 ROW 的源数据卷重定向分散性反而带来了好处. 因此, ROW 逐渐成为了业界的主流.

快照一致性: 快照前必须将缓存flush到磁盘.

### CDP(continuous data protection), 连续数据保护技术
它是一种近实时捕获和保存数据变化的技术, 允许将数据恢复到之前的任意时间点.

基本原理: 写前复制+时间戳

## 多路径
使用一个以上的物理路径来访问网络存储设备, 并通过容错, i/o流量负载均衡甚至更细粒度的i/o调度测量等方式, 为网络存储设备提供更高的可用性和性能.

多路径软件的作用就是识别出系统中的多路径设备, 将它作为一个设备来管理, 同时管理这些路径, 以实现i/o流量的负载均衡和i/o路径故障时的切换和恢复.

实现方式:
- hba驱动
- 内核. 在linux中支持基于software raid的多路径技术和基于device mapper的多路径技术.
- 应用程序, 比如EMC PowerPath, IBM RDAC等

实现多路径的关键是磁盘标识, 方法有2种:
1. scsi inquiry.
1. world wide node name. 每个物理磁盘有两个WWPN地址, 但只有一个WWNN地址.

## 虚拟化
- 按层次: 块级和文件级虚拟化. linux的Multi-Disk和Device Mapper是块级虚拟化的典型.
- 按形式: 聚合虚拟化, 拆分虚拟化和仿真虚拟化(比如虚拟磁带库).
- 按实现方式: 带内虚拟化(在服务器和存储设备的数据通道中实现)和带外虚拟化(实现位于数据通道之外, 通过传送控制信息来完成物理设备和逻辑卷的映射)
- 按位置: 基于服务器的虚拟化, 基于网络的虚拟化和基于存储子系统的虚拟化.

## RAID
Redundant Array of Indepent Disks (独立磁盘冗余阵列) 是为了增加容量(capacity), 提升性能(performance), 增强容错(redundancy)和降低成本(cost).

一共有0~6一共7种，这其中RAID 0、RAID1、RAID 5和RAID6比较常用.

![](/misc/img/io/20200516194429.png)
![总结](/misc/img/io/20160712135213243.png)

软raid是允许在操作系统之上, 因此系统盘无法设置成raid模式, 解决方法: raid卡.

raid卡写模式:
- WriteBack : 数据写入其缓存即通知系统已完成, 掉电时有丢数据风险, 可通过电池维持缓存来解决
- WriteThrough(写透) : 数据真实写入存储后通知系统已完成.

RAID 2.0（独立磁盘冗余数组2.0, Redundant Array of Independent Disks Version 2.0），为增强型RAID技术，有效解决了机械硬盘容量越来越大，重构一块机械硬盘所需时间越来越长，传统RAID组重构窗口越来越大而导致重构期间又故障一块硬盘而彻底丢失数据风险的问题.

> 华为有[raid 2.0+](http://www.learnfuture.com/article/1272)
> [华为存储虚拟化解决方案](https://kkutysllb.cn/2019/05/29/2019-05-29-%E5%8D%8E%E4%B8%BA%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/)
> [Linux原生的存储虚拟化软RAID和LVM](https://kkutysllb.cn/2019/06/09/2019-06-09-Linux%E5%8E%9F%E7%94%9F%E7%9A%84%E5%AD%98%E5%82%A8%E8%99%9A%E6%8B%9F%E5%8C%96%E8%BD%AFRAID%E5%92%8CLVM/)
> [各RAID级别比较](https://support.huawei.com/enterprise/zh/doc/EDOC1000045885?section=j005)

### RAID 0
![](/misc/img/io/20160712135137243.png)
RAID 0的特点：
- 最少需要2块磁盘
- 数据条带式分布
- 没有冗余，性能最佳(不存储镜像、校验信息)
- 安全性能低, 不支持数据冗余，无容错功能，不能应用于对数据安全性要求高的场合即不适用于关键任务环境
- 磁盘空间使用率：100%，成本最低

线性raid(jbod)和raid0区别: raid0有条带.

### RAID 1
![](/misc/img/io/20160712135148165.png)
RAID 1的特点：
- 最少需要2块磁盘
- 提供数据块冗余, 磁盘的利用率却只有50%，磁盘利用率最低，成本最高
- 数据冗余备份，容错，数据安全性高；当故障发生时，系统可自动切换到镜像磁盘，无需重组数据
- 读性能：只能在一个磁盘上读取，取决于磁盘中较快的那块盘
- 写性能：两块磁盘都要写入，虽然是并行写入，但因为要比对，故性能单块磁盘慢

### RAID 3
![]()
实际数据占用的有效空间为N个硬盘的空间总和，而第N+1个硬盘上存储的数据是校验容错信息

特点:
- n最少为3
- **校验码集中在单独的一块磁盘**

### RAID 5
![](/misc/img/io/20160712135201306.png)
RAID 5特点：
- 最少3块磁盘
- 数据条带形式分布
- 高可靠性, 奇偶校检与数据分别都存储不同盘. 损坏时，可以自动重建, 但只允许一块磁盘损坏
- 适合多读少写的情景，是性能与数据冗余最佳的折中方案
- (n-1)*单块磁盘的读性能，和RAID0相近似的数据读取速度
- 写入数据的速度比对单个磁盘进行写入操作稍慢
- 磁盘空间利用率：(N-1)/N，即只浪费一块磁盘用于奇偶校验, 存储成本相对较低

RAID5又被称为XOR方式，它是通过对所有存储数据条带做一次XOR操作，来得到一份冗余条带作为校验数据的.

### Raid6
类似RAID5两组奇偶校验信息块，两个独立的奇偶系统使用不同的算法， 数据的可靠性非常高，即使两块磁盘同时失效也不会影响数据的使用.
相对于RAID 5有更大的`写损失`，因此`写性能`非常差.

4坏2的恢复过程(推测): 先参照raid5恢复一块盘, 再结合其他三盘恢复第四块盘.

RAID6又被称为P+Q方式，它在RAID5 XOR的校验数据（该校验数据通常被称为P）基础上进一步计算出第二份校验数据（通常被称为Q）. 通常Q的生成是各个存储数据条带乘以不同系数后求得的XOR结果.

### RAID 7
三奇偶校验

raid 5~7均是分布式校验码

**raid5~7没有上限(不考虑阵列卡或软raid性能的限制),但有下限**

### RAID 10
![](/misc/img/io/20160712135207894.png)
RAID 10(又叫RAID 1+0)特点：
- 最少需要4块磁盘
- 先按RAID 0分成两组，再分别对两组按RAID 1方式镜像, 安全性上RAID10要好于 RAID01
- 兼顾冗余(提供镜像存储)和性能(数据条带形分布)
- 在实际应用中较为常用
- 磁盘利用率50% 成本较高
- RAID01比起RAID10有着更快的读写速度

### 纠删码
目的: 为了达到更高的数据冗余度.

纠删码可以看作RAID5和RAID6的超集，k +m 纠删码的基本思想是将k 块原始的数据元素通过一定的计算，得到m 块冗余元素（校验块）. 对于这k +m 块的元素，当其中任意m 块元素出错（包括原始数据和冗余数据）时，均可以通过对应的重构算法恢复出原来的k 块数据. 生成校验的过程被称为编码，恢复丢失数据块的过程被称为解码.

在分布式存储系统中，为了保证数据的可靠性，通常采用多副本(通常是3)的形式, 其带来的问题是存储利用率大为降低. 如何平衡存储空间和数据可靠性，成了分布式存储需要考虑的重要问题. 纠删码可以平衡这两者的关系，在提高存储空间利用率的前提下，不会影响数据可靠性. 因此，Ceph、Hadoop、Sheepdog等分布式存储系统都有采用纠删码.

### 循环冗余校验码
目的: 数据完整性

CRC是一种错误检测码，被广泛用于数字网络和存储设备中，其作用是检测原始数据在传输过程中的意外变化. 数据块根据数据内容本身，通过一个多项式计算获得一个短的校验值. 在数据接收端，会对这个数据块重新计算校验值，如果不匹配，则会对损坏的数据采取补救措施.

CRC是通过使用二进制除法（无须进位，使用XOR而不是减法）对字节数据流做除法而获得的余数. 被除数是信息数据流的二进制数表示. 除数是长度为n + 1的预定义二进制数（即生成多项式，n 为CRC位数），通常由多项式系数表示. 不同的生成多项式对应不同场景下的不同协议.

## DAS、SAN、NAS网络存储的异同
参考:
- [存储相关知识-DAS/SAN/NAS](https://yq.aliyun.com/articles/467199)

![](/misc/img/io/s8xI-fyfrfvv5160915.jpg)
NAS、SAN和DAS三者的目的相同：存储, 但是初衷、达成路径和结果也不相同.

> 从NAS与SAN系统的角度来看，NAS用于文件I/O，SAN用于块I/O. 比较NAS与SAN时需要记住的另一件事是，NAS最终将文件I/O请求转换为与其相连的存储设备的块访问.
> 供应商现在开始提供具有NAS或SAN体验的前端系统，而后端则基于对象存储
> 共享访问文件的环境，最好使用 NAS, 因为它自带文件系统.

DAS(Direct-Attached Storage), 是某台独立服务器中的一部分(**通过总线直接连接设备**)，将外置存储通设备通过SCSI或FC接口直接连接到应用服务器上来使用. 事实上，这种DAS存储模式，一般在中小型企业汇总应用十分广泛.
它更依赖主机的操作系统来实现数据的IO读写、数据管理、数据备份等工作, 但是这种存储模式也存在一定的缺点, 比如可管理性差、弹性扩展能力弱，难以跟上IT发展趋势等问题.

NAS是前端用网络文件系统提供服务, 后端用任何方式的存储空间, 即通过网络文件系统提供服务的存储设备.

NAS 和 SAN 存储系统的区别是 NAS 有自己的文件系统. 即**NAS提供文件系统, 而SAN提供的是块设备, 比如磁盘等.
NAS 是将目光集中在应用、用户和文件以及它们共享的数据上. SAN 是将目光集中在磁盘、磁带以及联
接它们的可靠的基础结构. 将来从桌面系统到数据集中管理到存储设备的全面解决方案将是`NAS + SAN`.

## SAN
SAN(Storage Area Network)存储区域网络，是一种高速的、专门用于存储操作的网络，通常独立于计算机局域网(LAN).
SAN将主机和存储设备连接在一起，能够为其上的任意一台主机和任意一台存储设备提供专用的通信通道.
SAN将存储设备从服务器中独立出来，实现了服务器层次上的存储资源共享.
SAN将通道技术和网络技术引入存储环境中，提供了一种新型的网络存储解决方案，能够同时满足吞吐率、可用性、可靠性、可扩展性和可管理性等方面的要求.

与ip-san比, fc-san败退原因:
1. 成本高
1. 可扩展性相对较弱
1. 易用性相对较弱
1. 兼容性相对较差
 
### FC-SAN
通常SAN由磁盘阵列(RAID)连接FC(Fibre Channel, 网状通道)组成(为了区别于IP SAN，通常SAN也称为FC-SAN). SAN和服务器和客户机的数据通信通过SCSI命令而非TCP/IP，数据处理是`块级(lock level)`.

> FC是一套网络协议, 与tcp/ip类似, 传输介质是光纤(fiber).
> 目前我查到的FC最快是16Gb/s.
> FC磁盘已经淘汰.
> FCoE现基本被抛弃了, NVMe/TCP开始流行.
> FC已被SAS和IP淘汰中.

### IP-SAN
简单来讲，IP-SAN（IP存储）的通信通道是使用IP通道，而不是光纤通道，把服务器与存储设备连接起来的技术，除了标准已获通过的iSCSI，还有FCIP、iFCP等正在制定的标准, 而iSCSI发展最快，已经成了IP存储一个有力的代表.
像光纤通道一样，IP存储是可交换的，但是与光纤通道不一样的是，IP网络是成熟通用的，不存在互操作性问题，而光纤通道SAN最令人头痛的就是这个问题.

IP存储的优势：
1. 利用无所不在的IP网络，一定程度上保护了现有投资.
2. IP存储超越了地理距离的限制, 十分适合于对现存关键数据的远程备份.
3. IP网络技术成熟, IP存储减少了配置、维护、管理的复杂度.

> aliyun已使用25Gb/s网络

### iSCSI
参考:
- [iSCSI存储技术全攻略](https://www.sansky.net/article/2007-12-03-iscsi-storage.html)

iSCSI即internet SCSI，是IETF制订的一项存储传输协议标准，目的是为了用IP协议将存储设备连接在一起.
它是将SCSI数据区块映像成以太网数据封包, 并通过IP网络来传输SCSI 数据区块的方法.
称作Initiator（发起方），被请求的目标设备则称作Target, 透过这种方式可在IP网络上以区块级模式存取大量数据.

组成:
- iSCSI initiator

    发起读写请求的来源机器设备，诸如服务器，连接在IP网络并对iSCSI target发起请求以及接收响应. 每一个iSCSI主机通过唯一的IQN来识别，类似于光纤通道的WWN. 要在IP网络上传递SCSI块命令，必须在iSCSI主机上安装iSCSI驱动.
- iSCSI Target

    iSCSI target是接收iSCSI命令的设备. 此设备可以是终端节点，如存储设备，或是中间设备，如IP和光纤设备之间的连接桥. 每一个iSCSI target通过唯一的IQN来标识，存储阵列控制器上（或桥接器上）的各端口通过一个或多个IP地址来标识.

iSCSI 会话:
iSCSI 会话建立于一个initiator与一个target之间，一个会话允许多个TCP连接，并且支持跨连接的错误恢复.
建立一个iSCSI会话，包括：
1. 命名阶段：确定需要访问的存储，以及initiator，与FC不同，命名与位置无关
1. 发现阶段：找到需要访问的存储
1. 登录阶段：建立于存储的连接，读写之前首先进行参数协商，按照TCP连接登录

> ISCSI技术的核心是在TCP/IP网络上传输SCSI协议，是指用TCP/IP报文、和ISCSI报文封装SCSI报文，使得SCSI命令和数据可以在普通以太网络上进行传输.

## 指标
Recovery Point Objective (RPO) : 指的是最多可能丢失的数据的时长.
Recovery Time Objective (RTO) : 指的是从灾难发生到整个系统恢复正常所需要的最大时长.

## 串行总线磁盘为什么比并行总线磁盘快
实际的传输速率是位宽和频率的乘积，如果频率相同，没错16bit的并口传输是串口的传输的十多倍，但是随着频率的大幅提高，并口多根信号线之间的串扰越来越严重.
当达到一定频率之后，并口传输的数据开始失真，就不能再提高传输频率了; 而串口通信一般是采用的差分信号，功耗低，串扰问题很小，因此可以大幅提高通信频率.

## LVM(Logical Volume Manager)
- pv(physical volume) : lvm通过os识别到的物理盘或逻辑盘
- vg(volume group) : 多个pv被逻辑地组成一个组
- pp(physical partition, 物理区块) : 在逻辑上将vg分割成连续的小块. lvm会记录pp的大小和pp序号的偏移.
- lp(logical partition, 逻辑区块) : 若干PP组成的逻辑块.
- lv(logical volume, 逻辑卷) : 若干lp组成的卷, 是lvm最终用来存储数据的单位.

使用步骤:
1. 创建pv, 将pv加入vg
2. 在vg中创建lv, 最后格式化lv即可

> lvm2通过device mapper完成. device mapper在内核是由模块化的target driver插件来实现.
   
## 备份
备份分类:
1. 文件级备份
1. 快级备份
   
   直接通过磁盘控制器而无需考虑文件系统, 但会备份无用快


快照(**只读**):
- Copy on First Write, CoFW, 常见.

  元数据复制后, 新的数据写入时需先将原数据块复制出来让后再覆盖写入, 更新快照元数据相应条目指向复制的数据块.
  一读两写(block)), 不影响读性能.
  删除快照时, 可直接清理, 没有后遗症.
- Redirect on First Write, RoFW

  元数据复制后, 新的数据会重定向写入到某剩余空间, 之后更新原元数据对应条目的指向.
  容易产生碎片, 影响读性能; 因为一写导致更省io
  删除时, 数据块散乱, 不好清理, 此时影响读写性能.

> 做快照时, 新卷的容量是原卷的30%(经验值, 需根据业务调整).

卷clone: 源数据集在某时刻的若干份实实在在的复制实体.

## CDP(Continuous Data Protect, 持续数据保护)
一种在不影响数据运行的前提下, 可以实现持续捕捉或跟踪目标数据发生的任何改变, 并且能够恢复到此前任意时间点的方法.

> cdp起源于linux的cdp模块, 它持续地捕获所有I/O请求，并且在这些请求打上时间戳标志, 再将数据变化以及时间戳保存下来，以便恢复到过去的任意时刻.
> cdp的核心思想是时间戳.

cdp可提供的备份级别
- 快级
  
  捕捉底层卷的写io变化, 并将每次变化的快数据打入时间戳再保存下来.
- 文件级
  
  通过监控文件系统动作, 文件的每一次变化(含元数据), 比如日志型的文件系统.
- 应用级
  
  比如DB的WAL.

启用CDP时, CoW会三写, 此时使用RoW更优.

> CDP绝不适合大数据量改动的场合.

## 文件系统
精髓: metadata(链表,B树和位图等结构)和对这些metadata的管理方式.

fs一定是先写文件实体数据, 再将元数据从缓存中flush到磁盘.

## 配置项
- read_ahead_kb

  增大`/sys/block/${disk}/queue/read_ahead_kb`可提升连续读的性能. 不过大多数服务器工作负载主要是随机io, 则提升有限. 

  > ionice允许在进程级别设置优先级, 限制其对磁盘子系统的使用率.

- noatime : 禁止更新文件的access time, 减少不必要的io

  ```
  // /etc/fstab
  UUID=948d792c-2d5a-4f81-97b4-811b34749d8f /home/jr/git ext4 defaults,nodelalloc,noatime,nodiratime        0  0
  ```

## FAQ
### iscsi和nas区别
参考:
- [iSCSI vs NAS: Full Comparison](https://www.msp360.com/resources/blog/iscsi-nas-comparison/)


![](/misc/img/io/NAS-iSCSI-Scheme.png)

iSCSI和NAS系统之间的主要区别在于它们的存储访问体系结构. **iSCSI是SAN系统的流行实现**，而NAS是将存储设备连接到用户网络的常用方法.
换句话说，我们不应该直接比较NAS和iSCSI，因此我们选择一种NAS实施方案-SMB上的NAS存储.

SAN系统是部署为高性能**块级**存储模型, 面向单一用户(即系统)，可在最低数据级别上显示存储设备. NAS是一种**对象级**存储，面向多用户即共享, 可向客户端OS提供即用型文件和文件夹树.

SAN无法共享给多人, 否则会数据错乱. 而NAS由上层软件处理冲突, 可避免这个问题.

### 云盘
参考:
- [云盘三副本技术 - aliyun](https://help.aliyun.com/document_detail/35108.html)
- [云硬盘三副本技术 - 华为](https://support.huaweicloud.com/productdesc-evs/evs_01_0056.html)

在阿里云上对云盘的读写最终都会被映射为对阿里云数据存储平台上的文件的读写. 阿里云提供了一个扁平的线性存储空间，在内部会对线性地址进行切片，一个分片称为一个Chunk（即块）. 每一个Chunk，阿里云都会复制成三个副本，并将这些副本按照一定的策略存放在存储集群中的不同数据节点上，保证数据的可靠性.


阿里云三副本原理:

在阿里云数据存储平台中，有三类角色，分别为Master、Chunk Server和Client, 一个写操作最终由Client执行，执行过程简要说明如下：
- Client收到写操作请求，并计算出写操作对应的Chunk
- Client向Master查询该Chunk的三份副本存放的数据节点（即Chunk Server）
- Client根据Master返回的结果，向这三个Chunk Server发出写请求
- 如果三份都写成功，Client返回成功，反之则Client返回失败

为防止由于一个Chunk Server或一个机架的故障导致数据不可用，Master会保证三份副本分布在不同机架下的不同Chunk Server上. 因此，Master的分布策略中会综合考虑数据存储平台中所有Chunk Server的硬盘使用情况、交换机的分布情况、电源供电情况和节点负载情况等.


三副本技术确保数据一致性:

数据一致性表示当应用成功写入一份数据到存储系统时，存储系统中的3个数据副本必须一致, 当应用无论通过哪个副本再次读取这些数据时，该副本上的数据和之前写入的数据都是一致的.

云硬盘三副本技术主要通过以下机制确保数据一致性：
- 写入数据时，同时在3个副本执行写入操作

  当应用写入数据时，存储系统会同步对3个副本执行写入数据的操作，并且只有当多个副本的数据都写入完成时，才会向应用返回数据写入成功的响应。
  
- 读取数据失败时，自动修复损坏的副本

  当应用读数据失败时，存储系统会判断错误类型。如果是物理磁盘扇区读取错误，则存储系统会自动从其他节点保存的副本中读取数据，然后在物理磁盘扇区错误的节点上重新写入数据，从而保证数据副本总数不减少以及副本数据一致性。

###  块存储、文件存储、对象存储这三者的本质差别
- 对象存储：就是键值存储，接口比较简单，就是GET,PUT,DEL和其他扩展，如S3、OSS、NOS等
- 文件存储：支持POSIX接口，对应的传统的文件系统有Ext3、Ext4等，与传统文件系统的区别在于分布式存储提供了并行化的能力，如Ceph的CephFS，但是有时候又会把GFS，HDFS这种非POSIX接口的类文件存储接口归入此类
- 块存储：这种接口通常以QEMU Driver或者Kernel Module的方式存在，这种接口需要实现Linux的Block Device的接口或者QEMU提供的Block Driver接口，如Sheepdog，AWS的EBS，青云的云硬盘和阿里云的盘古系统，还有Ceph的RBD(RBD是Ceph面向块存储的接口)

按照这三种接口和其应用场景，很容易了解这三种类型的IO特点，括号里代表了它在非分布式情况下的对应：
- 对象存储（键值数据库）：接口简单，一个对象我们可以看成一个文件，只能全写全读，通常以大文件为主，要求足够的IO带宽。
- 块存储（硬盘）：它的IO特点与传统的硬盘是一致的，一个硬盘应该是能面向通用需求的，即能应付大文件读写，也能处理好小文件读写。但是硬盘的特点是容量大，热点明显。因此块存储主要可以应付热点问题。另外，块存储要求的延迟是最低的。
- 文件存储（文件系统）：支持文件存储的接口的系统设计跟传统本地文件系统如Ext4这种的特点和难点是一致的，它比块存储具有更丰富的接口，需要考虑目录、文件属性等支持，实现一个支持并行化的文件存储应该是最困难的。但像HDFS、GFS这种自己定义标准的系统，可以通过根据实现来定义接口，会容易一点。

### 如何选用NAS、OSS和EBS
- 文件存储NAS和对象存储OSS有什么不同？

文件存储NAS和对象存储OSS的主要区别：无需修改应用，即可直接像访问本地文件系统一样访问文件存储NAS. 它提供支持文件的随机读写和在线修改.

对象存储OSS是比较新的存储类型，相对于文件存储目录树的组织形式，对象存储OSS采用扁平的文件组织形式，采用RESTFul API接口访问，不支持文件随机读写，主要适用于互联网架构的海量数据的上传下载和分发.

- 文件存储NAS和块存储EBS有什么不同？

文件存储NAS相对于块存储EBS的主要区别：文件存储NAS可以同时支持多个客户端同时共享访问.

块存储EBS是裸磁盘，挂载到ECS后不能被操作系统应用直接访问，需要格式化成文件系统（ext3、ext4、NTFS等）后才能被访问。块存储EBS的优势是性能高、时延低，适合于OLTP数据库、NoSQL数据库等IO密集型的高性能、低时延应用工作负载。但是块存储EBS无法容量弹性扩展，单盘最大只能32TB，并且对共享访问的支持有限，需要配合类Oracle RAC、WSFC Windows故障转移集群等集群管理软件才能进行共享访问。因此，块存储EBS主要还是针对单ECS的高性能，低时延的存储产品。

### 双活和多路径
参考:
- [浅谈双活和多路径](https://blog.csdn.net/TV8MbnO2Y2RfU/article/details/78103847)

传统存储起码是双控冗余的，两台控制器通过后端 SAS 控制器连接到 JBOD 上即JBOD 上通过两片 SAS Expander 分别上连到两台服务器的后端 SAS 控制器，同样，每块磁盘用两条路径上连到每片 Expander （ SAS 接口有两套数据金手指）.

双控冗余有三种方式:
- HA

  HA 模式是指平时只有一个控制器处理 IO ，另一个控制器完全不干活，开着机在那待着转等干活的控制器挂掉，它接管过来，传统存储产品中目前没有人使用 HA 方式因为不划算. 目前多数产品为**互备模式**.

- 互备（非对称双活）

  互备模式，或者叫非对称双活，是指每个 Lun 都有自己的 Owner 控制器，比如 A 控处理 Lun1 的 IO ， B 控处理 Lun2 的 IO ，如果 A 控收到发向 Lun2 的 IO ，则通过控制器间交换网络转发给 B 控处理而不能自己私自处理，双控各干各的互不干扰，不会产生冲突， “ 非对称 ” 意思就是 “ 各干各的 ” ，你坏了我接管，我坏了你接管，但是两个控制器都在干活，所以 “ 双活 ” .

- 双活（全对称双活）

  对称双活，则是指两个控制器角色完全对等，不再分 Lun 的 Ownership ，任何控制器都可以处理任何 Lun 的 IO ，这给系统设计带来了复杂性，首先要求双控要配合起来，针对已经应答的目标地址有重叠的写 IO ，要保证时序一致性，双控必须做好沟通保证后应答的 IO 后写入；另外，同时还要解决数据防撕裂问题，有时候阵列内部会自行读或者写某些目标地址数据块，此时双控要用锁来保证每次读写的防撕裂，对某个块的操作可能会被分为多次子 IO ，这些子 IO 是一个一致性组或者说组成一次原子操作，中途不能被交织入其他 IO ，否则就会撕裂导致不一致。正因如此，对称式双活增加了开发难度。但是对称式双活能够以 IO 粒度来平衡系统的负载，不会出现 Lun1 太忙而 Lun2 很闲从而导致 A 控负载远高于 B 控而又无计可施的尴尬.

> 双控之间可通过内建高速NTB进行连接，提供实时数据同步.


多路径如何管理发向双控的 IO

存储系统提供了双控冗余，主机端如何利用起双控，要依靠多路径软件。通常主机会与 A 控和 B 控至少各保持一条连接，分别从两个控制器上发现到两份同一个物理 Lun 的两份副本，系统中会生成两个盘符，而多路径软件的功能则是负责链路故障后的路径切换、链路正常时的 IO 负载均衡以及冗余盘符的消除。

针对非对称双活，因为有 Lun 的 Ownership 存在，发向对应 Lun 的 IO 要确保走最优路径，也就是不要发送给该 Lun 的非属主控制器，否则将引发内部转发，增加时延，除非在链路带宽达到瓶颈而控制器处理能力未达到瓶颈的时候可以利用这条非最优路径。探测某个 Lun 的最优路径以及其他一些阵列端的运行信息，需要多路径软件与阵列之间做一些信息交互，这些信息可以走带外通道比如以太网，也可以走带内通道也就是数据链路比如 FC/SAS/iSCSI ，通常使用后者，而 SCSI 指令体系内没有针对多路径软件与阵列之间的交互协议做什么规定，所以各个厂商都有自己不同的实现模式，比如通过一些特殊指令序列，或者封装到某些特殊指令内部。正是由于各厂家的交互协议不统一，所以 SCSI 体系最新的规范里定义了 ALUA （Asymmetric Logical Unit Access，非对称逻辑单元存取）协议，期望各厂商按照 ALUA 协议规范来实现多路径软件和阵列之间的交互。

> 对于ALUA, 对于同一个任务来说, 任意时刻只有一个控制器在控制.

而对称式多活由于没有 Lun 属主的概念，多路径软件无需与阵列交互复杂的控制数据，最多是控制阵列控制器的切换，所以这块 SCSI 没有定义规范，但是人们俗称对称式多活为 “SLUA(对称逻辑单元存取)” 以与 ALUA 区分， S 表示 Symmetric.

> SLUA是多路径的基本特性。特点就是：对于特定的LUN来说，在它的路径中，两个存储控制器的目标端口均处于主动/优化状态。**两个控制器之间实现高速互联的通讯，一个I/O发送到控制器，两个控制器可同时参与处理**；当一个控制器繁忙，系统不需要主机端的负载均衡软件参与就可以自动实现负载均衡.


多控存储系统，一般指分布式多控，也就是多控之间并不是共享访问所有后端 JBOD 的，富士通的高端存储除外，其后端采用 SAS Expander 将所有 HDD 呈现在一个大的 SAS 域中，富士通是真多控对称式多活。多数实现都是双控共享一堆 JBOD ，然后多组双控再结合成分布式存储，也就是所谓的 ”Server SAN“. 某厂商最近发布的系统其实是 4 控共享访问后端 JBOD ，当然， JBOD 里只有两片 Expander ，接不了 4 控，所以 4 控和 JBOD 之间还需要增加两片 SAS Expander 作为路径扩充使用。由于并非共享式集群而是分布式集群，所以其原本就是非对称式多活了，各个节点或者节点组各管各的磁盘，但是每个控制器节点都可以接受 IO ，只不过遇到不是给自己管辖磁盘范围的 IO 则需要转发处理。有些厂商为了维持自己传统高端的共享内存架构的形象，在分布式集群内，实现了全局共享缓存，当然这个共享缓存并非传统高端那种真内存地址空间共享了。而有些则是赤裸裸的分布式架构，逼格直逼 ServerSAN.

### 分布式存储
ref:
- [分布式存储的技术趋势（二）：双重RAID机制](https://www.51cto.com/article/649019.html)

三副本和ＥＣ纠删码是分布式存储中常见的两种数据保护机制。由于ＥＣ纠删码存在比较严重的写放大问题，小块数据的写性能严重不足，通常仅适用于视频、备份、容灾等对ＩＯ性能要求不高的大文件业务场景。在虚拟化、私有云、数据库等块存储场景，最常见的是三副本机制，即数据块按某种随机规则，保持在三个不同节点上的不同磁盘上, 用于对小块数据读写性能有一定要求的应用场景.


道熵的铁力士分布式存储采用了双重RIAD机制，即节点内RAID(RAID10或RAID50/60)与跨节点RAID分布式两副本相结合，将磁盘阵列的本地恢复特性与分布式扩展特性融合，既有磁盘阵列的高可靠、高容错的特点，同时具备分布式横向扩展的优势

> 按照道熵的说法, 应是建立在网络可靠性基础上, 因为它只能坏一条链路. 节点内RAID是RAID10的话, 看起来就是4副本了.