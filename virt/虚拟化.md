# 虚拟化
参考:
- [虚拟化的发展历程和实现方式](https://blog.csdn.net/jmilk/article/details/51031118)

虚拟化是一种**资源管理技术**, 趋势是KVM(kernel-based virtual machine)借助硬件辅助的虚拟化技术主要负责比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰.

提供虚拟化的软件层称为 Hypervisor或vmm(virtual machine monitor, 虚拟机监视器). VMM对物理资源的虚拟归纳为三个部分:CPU虚拟化、内存虚拟化、IO虚拟化.

分类方式:
- 软硬件实现Hypervisor

    - 软件虚拟化: bochs/qemu, 基于二进制翻译, 性能较弱
    - 硬件虚拟化(Hardware-Assisted Virtualization): Intel 的 VT-x 和 AMD 的 AMD-V, 及基于VT-x/AMD-V的kvm

        对于 Intel，可以查看 grep 'vmx' /proc/cpuinfo；对于 AMD，可以查看 grep 'svm' /proc/cpuinfo. 该选项一般在bois配置.

        > 也可使用`apt install cpu-checker`的kvm-ok命令查看

        当确认开始了标志位(表示当前是在虚拟机状态下，还是在真正的物理机内核下)之后，通过 KVM，GuestOS 的 CPU 指令不用经过 Qemu 转译，直接运行，大大提高了速度.

        [英特尔VT具体包括分别针对处理器、芯片组、网络的VT-X、VT-D和VT-C技术](https://yifengyou.gitbooks.io/learn-kvm/content/docs/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF.html):
        - 处理器：英特尔虚拟化技术（英特尔VT-x），包括英特尔虚拟化灵活迁移技术（Intel VT FlexMigration）、英特尔VT FlexPriority、英特尔VT 扩展页表（Extended Page Tables）
        - 芯片组：通过intel IOMMU支持直接 I/O 访问的 VT虚拟化技术（英特尔VT-d）
        - I/O设备：支持连接的英特尔虚拟化技术(英特尔VT-c）, 包括虚拟机设备队列（VMDq）、 虚拟机直接互连（VMDc, 基于SR-IOV生成VF设备直接分配给guest）
- 是否需要修改客户机内核

    - 半虚拟化 : 客户机需要更改系统内核/驱动才得以实现虚拟化, 比如virtio,xen
    - 全虚拟化(Full virtualization) : 不需要修改客户机内核, 虚拟化软件会模拟假的 CPU、内存、网络、硬盘等整套设备, 任何操作都经过虚拟化软件, 慢.

    > 半虚拟化的思想就是，修改操作系统内核，替换掉不能虚拟化的指令，通过超级调用（hypercall）直接和底层的虚拟化层hypervisor来通讯，hypervisor 同时也提供了超级调用接口来满足其他关键内核操作，比如内存管理、中断和时间保持.
- 虚拟层是直接位于硬件之上还是hostos上
    
    1. 主机级虚拟化
        1. Type 1类型 : 裸机架构虚拟化, 它是运行在服务器硬件之上, 比如阿里云的神龙架构(X-Dragon), 微软的Hyper-v、VMware vSphere的ESXi和Citrix的XenServer
        1. Type 2类型 : Hypervisor是运行在HostOS之上,  比如KVM, VMware Workstion, Oracle VM VirtualBox
    1. 容器级虚拟化, 比如docker

> xen已被主流的云厂商放弃, 均切换到了kvm.

kvm因为其独特的设计是横跨在Type 1和Type 2之间的, 是基于硬件虚拟化支持的全虚拟化实现, 它将kernel变成了hypervisor.

Linux查看机器是否虚拟机:
```sh
$ dmesg |grep -i virtual
[    0.029424] Booting paravirtualized kernel on KVM # 物理机是`Booting paravirtualized kernel on bare hardware`
[    0.660281] systemd[1]: Detected virtualization kvm.
# virt-what
# systemd-detect-virt
# cat /sys/class/dmi/id/sys_vendor
# dmidecode -t 1
```

虚拟机形式:
1. 进程
1. 模拟器: qemu, bochs
1. 高级语言虚拟机: jvm, python解析器

## 特权指令和敏感指令
特权指令是OS中直接管理关键系统资源的指令.
敏感指令是虚拟机中操作特权资源的指令.

特权指令都是敏感指令，但是敏感指令不一定是特权指令.

判断一个系统是否可虚拟化，核心就在于系统对敏感指令的支持. 如果所有的敏感指令都是特权指令，则可虚拟化.

处理器虚拟化是虚拟化关键，内存和IO虚拟化都依赖与处理器虚拟化. 访问内存和IO指令本身就是敏感指令.

# vmware
VMware vSphere 是 VMware 的虚拟化平台，可将数据中心转换为包括 CPU、存储和网络资源的聚合计算基础架构. vSphere 将这些基础架构作为一个统一的运行环境进行管理，并提供工具来管理加入该环境的数据中心.

![](/misc/img/virt/GUID-5EB66614-1EE8-4F39-8C8B-1E97EEE76791-high.png)
![](/misc/img/virt/2020-07-30_10-23-38.png)

### [VMware ESXi的体系结构](https://warmgrid.github.io/2019/04/09/the-architecture-of-vmware-esxi.html)
VMware ESXi体系结构包括底层操作系统（称为VMkernel）以及在其上运行的进程. VMkernel提供了运行系统上所有进程的方式，包括管理应用程序和代理以及虚拟机.

#### vmkernel
VMkernel是由VMware开发的类似POSIX的操作系统，提供与其他操作系统类似的某些功能，例如进程创建和控制，信号，文件系统和进程线. 它专门用于支持运行多个虚拟机，并提供以下核心功能：
- 资源调度
- I/O堆栈
- 设备驱动程序

# SR-IOV
SR-IOV是PCIE的功能, 可让os将一个设备识别为多个虚拟设备的功能. 在SR-IOV中, 将以往以物理方式存在的设备成为Physical Function(PF), 而通过SR-IOV添加的虚拟设备成为Virtual Function(VF).

查看host os是否存在支持SR-IOV的硬件: `lspci -vvvv|grep "SR-IOV"`, 比如intel 82599es网卡.
限制vf的带宽, 在host os执行: `ip link set etch0 vf <num> rate 100`, 100表示带宽是100Mbps.

# qemu
参考:
- [一文读懂 Qemu 模拟器](https://www.jianshu.com/p/db8c20aa6a69)
- [Qemu KVM(Kernel Virtual Machine)学习笔记](https://github.com/yifengyou/learn-kvm)
- [QEMU-KVM](https://abelsu7.top/2019/07/07/qemu-kvm-live-migration/)
- [QEMU KVM学习笔记](https://yifengyou.gitbooks.io/learn-kvm/content/)
- [qemu支持的platforms](https://wiki.qemu.org/Documentation/Platforms)
- [USING KVM VIRTUALIZATION ON ARM SYSTEMS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/appe-kvm_on_arm)

qemu是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备, 虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件.

因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 **KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化**，两者合作各自发挥自身的优势，相得益彰.

从本质上看，虚拟出的每个虚拟机对应 host 上的一个 Qemu 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应 Qemu 进程的一个线程. 下面通过一个虚拟机启动过程看看 Qemu 是如何与 KVM 交互的:
```c
// 第一步，获取到 KVM 句柄
kvmfd = open("/dev/kvm", O_RDWR);
// 第二步，创建虚拟机，获取到虚拟机句柄, 针对该文件句柄的loctl调用可以对虚拟机做相应的管理.
vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);
// 第三步，为虚拟机映射内存，还有其他的 PCI，信号处理的初始化. ioctl(kvmfd, KVM_SET_USER_MEMORY_REGION, &mem);
// 第四步，将虚拟机镜像映射到内存，相当于物理机的 boot 过程，把镜像映射到内存.
// 第五步，创建 vCPU(虚拟CPU)，并为 vCPU 分配内存空间.
ioctl(kvmfd, KVM_CREATE_VCPU, vcpuid);
vcpu->kvm_run_mmap_size = ioctl(kvm->dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0);
// 第五步，创建 vCPU 个数的线程并运行虚拟机.ioctl(kvm->vcpus->vcpu_fd, KVM_RUN, 0);
// 第六步，线程进入循环，并捕获虚拟机退出原因，做相应的处理.
for (;;) {  
  ioctl(KVM_RUN)  
  switch (exit_reason) {      
    case KVM_EXIT_IO:  /* ... */      
    case KVM_EXIT_HLT: /* ... */  
  }
}
// 这里的退出并不一定是虚拟机关机，
// 虚拟机如果遇到 I/O 操作，访问硬件设备，缺页中断等都会退出执行，
// 退出执行可以理解为将 CPU 执行上下文返回到 Qemu.
```

> vCPU本质是一个结构体. vCPU一般分成两个部分：VMCS(给硬件使用)+非VMCS(Virtual Machine Control Structure,给软件使用)虚拟机控制结构.

> VMM创建虚拟机，首先创建vCPU，然后由VMM调度运行. 虚拟机的运行，本质是VMM调用vCPU运行.

## Qemu 源码结构

Qemu 软件虚拟化实现的思路是采用二进制指令翻译技术，主要是提取 guest 代码，然后将其翻译成 TCG 中间代码，最后再将中间代码翻译成 host 指定架构的代码.

所以，从宏观上看，源码结构主要包含以下几个部分：
- /vl.c：最主要的模拟循环，虚拟机环境初始化，和 CPU 的执行
- /target-arch/translate.c：将 guest 代码翻译成不同架构的 TCG 操作码
- /tcg/tcg.c：主要的 TCG 代码
- /tcg/arch/tcg-target.c：将 TCG 代码转化生成主机代码
- /cpu-exec.c：主要寻找下一个二进制翻译代码块，如果没有找到就请求得到下一个代码块，并且操作生成的代码块

> 系统镜像可使用[Aliyun Linux 2 LTS(qcow2)](https://www.alibabacloud.com/help/zh/doc-detail/155430.htm).

# kvm
kvm(Kernel-based Virtual Machine), 是基于内核的虚拟化, 是仅支持硬件辅助的虚拟化, 是采用硬件虚拟化(intel vt/amd v)的全虚拟化解决方案. kvm对硬件的最低依赖是cpu硬件虚拟化支持.

![kvm和qemu架构](/misc/img/os/virt/xLEwiKumTSA5HvW.png)


一个kvm vm对应一个linux进程, 每个vCPU是该进程下的一个线程, 还有单独的io处理线程. 因此可通过linux的各种进程调度来实现不同vm的权限限定, 优先级等.

kvm vm看到的硬件是qemu模拟的(不包括VT-d透传的设备), 由qemu截获并转换为对实际设备的驱动操作.

![Xen, KVM, QEMU架构对比](/misc/img/os/virt/c5TkF1QHGyvS3dz.png)

相比于Xen架构，KVM架构有三大的优势：
1. 同等硬件资源环境下，KVM的性能表现更优
1. KVM架构天然的继承Linux内核更新迭代带来的系统优化，几乎不费力气，就完成了一次功能升级, 但对于Xen架构来说，每一次Xen Hypervisor内核或者Linux内核版本升级，Xen架构需要同步优化联调Xen Hypervisor内核和特权域(基于Linux的内核)，才能实现整个虚拟化内核的升级
1. 目前云厂商均使用kvm, 是趋势. 

![VirtIO](/misc/img/os/virt/WqYaSrQ3C8eEKuh.png)

## kvm生态组成
1. kvm内核模块: 主要负责cpu和内存的虚拟化

  由两部分组成:
  - kvm.ko: 处理器架构无关部分
  - kvm_xxx: 处理器相关部分, 比如intel的kvm_intel

  KVM的主要功能是初始化CPU硬件, 打开虚拟化模式, 然后将虚拟客户机运行在虚拟机模式下, 并对虚拟客户机的运行提供一定的支持.

  以intel举例, 在被kvm内核加载的时候, KVM模块会先初始化内部的数据结构; 做好准备之后, KVM模块检测系统当前的CPU, 然后打开CPU控制寄存器CR4中的虚拟化模式开关, 并通过执行VMXON指令将宿主操作系统（包括KVM模块本身） 置于CPU执行模式的虚拟化模式中的根模式； 最后， KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令(by ioctl). 接下来, 虚拟机的创建和运行将是一个用户空间的应用程序（QEMU） 和KVM模块相互配合的过程.

  针对虚拟处理器的最重要的loctl调用就是"执行虚拟处理器". 通过它， 用户空间准备好的虚拟机在KVM模块的支持下， 被置于虚拟化模式中的非根模式下， 开始执行二进制指令. 在非根模式下， 所有敏感的二进制指令都会被处理器捕捉到， 处理器在保存现场之后自动切换到根模式, 由KVM决定如何进一步处理（要么由KVM模块直接处理， 要么返回用户空间交由用户空间程序处理）.

  除了处理器的虚拟化， 内存虚拟化也是由KVM模块实现的, 比如intel ept.

  处理器对设备的访问主要是通过I/O指令和MMIO, 其中I/O指令会被处理器直接截获， MMIO会通过配置内存虚拟化来捕捉, 但是， 外设的模拟一般不由KVM模块而是qemu负责. 一般来说, 只有对性能要求比较高的虚拟设备才会由KVM内核模块来直接负责， 比如虚拟中断控制器和虚拟时钟， 这样可以大量减少处理器模式切换的开销.

2. qemu用户态工具: 提供设备模拟功能

  每一个虚拟客户机在宿主机中就体现为一个QEMU进程, 而客户机的每一个vCPU就是一个QEMU线程. 虚拟机运行期间, QEMU会通过KVM模块提供的系统调用进入内核, 由KVM模块负责将虚拟机置于处理器的特殊模式下运行. 遇到虚拟机进行I/O操作时， KVM模块会从上次的系统调用出口处返回QEMU, QEMU来负责解析和模拟这些设备.

  QEMU还提供了叫作virtio-blk-data-plane的一种高性能的块设备I/O方式， 它最初在QEMU 1.4版本中被引入. virtio-blk-data-plane与传统virtioblk相比， 它为每个块设备单独分配一个线程用于I/O处理, data-plane线程不需要与原QEMU执行线程同步和竞争锁， 而且它使用ioeventfd/irqfd机制， 同时利用宿主机Linux上的AIO（异步I/O） 来处理客户机的I/O请求， 使得块设备I/O效率进一步提高.

## kvm特性
1. 内存管理

    vm的"物理内存"就是对应宿主机进程的虚拟内存.
    vm自身内存访问落实到真实的宿主机的物理内存的机制叫影子页表(shadow page table), kvm Hypervisor为每个vm准备一份影子页表, 与vm自身页表建立一一对应关系. 因为性能原因, 目前影子页表的映射转换(`GPA->HPA`)已由硬件完成(intel ept/amd npt), 默认开启.

    > - kvm内存映射转换: GVA(guest virtual address, 客户机虚拟地址)->GPA(guest physical address)->HVA(host virtual address, 宿主机虚拟地址)->HPA(host physical address).

    > - vm自身页表描述的是GVA->GPA, 通过CR3寄存器完成; 影子页表描述的是GPA->HPA.
1. kvm image格式

    确切的说是qemu的功能.
    kvm的image格式是qcow2, 支持稀疏文件, 快照(包括多级快照), 压缩和加密.
1. 实时迁移

    在宿主机间迁移vm而不中断服务.
1. 设备驱动程序

    - 支持半虚拟化的驱动程序(virtio标准)来提高io性能.
    - 支持intel VT-d, 通过将宿主机的pci总线上的设备透传(pass-through)给vm.
1. 性能和可伸缩性

    继承了linux的性能和可伸缩性

## 核心
### cpu
在普通的Linux系统中， 进程一般有两种执行模式： 内核模式和用户模式. 而在KVM环境中， 增加了第3种模式： 客户模式.

vCPU在3种执行模式下的不同分工如下:
1. 用户模式（User Mode）

  主要处理I/O的模拟和管理， 由QEMU的代码实现

1. 内核模式（Kernel Mode）

  主要处理特别需要高性能和安全相关的指令， 如处理客户模式到内核模式的转换， 处理客户模式下的I/O指令或其他特权指令引起的退出（VM-Exit） ， 处理影子内存管理（shadow MMU）

1. 客户模式（Guest Mode）

  主要执行Guest中的大部分指令， I/O和一些特权指令除外（它们会引起VM-Exit， 被Hypervisor截获并模拟）

![vCPU在KVM中的这3种执行模式下的转换](/misc/img/virt/20150510150314072.png)

有两种操作来实现客户机的SMP(symetric multi-processor, 共享内存资源的多处理器系统, 与NUMA相对), 一是将不同的vCPU的进程交换执行（分时调度， 即使物理硬件非SMP， 也可以为客户机模拟出SMP系统环境）; 二是将在物理SMP硬
件系统上同时执行多个vCPU的进程.

在qemu命令行中， `-smp [cpus=]n[,maxcpus=cpus][,cores=cores][,threads=threads][,sockets=sockets]`参数即是配置客户机的SMP系统:
- n用于设置客户机中使用的逻辑CPU数量（默认值是1） 
- maxcpus用于设置客户机中最大可能被使用的CPU数量， 包括启动时处于下线（offline） 状态的CPU数量（可用于热插拔hot-plug加入CPU， 但不能超过maxcpus这个上限）

  当`n<maxcpus`时可模拟cpu热插拔. 比如在qemu monitor执行`cpu-add <id>`(id可与`info cpu`已有序号不连续), 新加的vcpu在guest里id是连续的.
- cores用于设置每个CPU的core数量（默认值是1） 
- threads用于设置每个core上的线程数即超线程选项（默认值是1） 
- sockets用于设置客户机中看到的总的CPU socket数量

> 不加`-smp`即使用默认值1

> 通过qemu monitor的`info cpu`可查看guest的cpu数量及其对应的qemu线程id. 输出带`*`的是BSP(boot strap process, 系统最初启动时在SMP生效前使用的cpu)

#### cpu过载
推荐的做法是对多个单CPU的客户机使用over-commit. 最不推荐的做法是让某一个客户机的vCPU数量超过物理系统
上存在的CPU数量.

> 不过， 在并非100%满负载的情况下， 一个（或多个） 有4个vCPU的客户机运行在拥有4个逻辑CPU的宿主机中并不会带来明显的性能损失

总结: **KVM允许CPU的过载使用， 但是并不推荐在实际的生产环境（特别是负载较重的环境） 中过载使用CPU**

#### cpu model
查询当前qemu支持的所有cpu model: `qemu-system-x86_64 -cpu ?`, 源码在`https://github.com/qemu/qemu/blob/master/target/i386/cpu.c的builtin_x86_defs`里.

在x86_64下, qemu不添加`-cpu xxx`启动时, 默认使用qemu64.

#### 进程的处理器亲和性和vCPU的绑定
进程的处理器亲和性（Processor Affinity） 即CPU的绑定设置, 是指将进程绑定到特定的一个或多个CPU上去执行， 而不允许将进程调度到其他的CPU上。 Linux内核对进程的调度算法也是遵守进程的处理器亲和性设置的. 设置进程的处理器亲和性带来的好处是可以减少进程在多个CPU之间交换运行带来的缓存命中失效（cache missing） ， 从该进程运行的角度来看， 可能带来一定程度上的性能提升。 换个角度来看， 对进程亲和性的设置也可能带来一定的问题， 如破坏了原有SMP系统中各个CPU的负载均衡（load balance） ，
这可能会导致整个系统的进程调度变得低效。 特别是在使用多处理器、 多核、 多线程技术的情况下， 在NUMA结构的系统中， 如果不能对系统的CPU、 内存等有深入的了解， 对进程的处理器亲和性进行设置可能导致系统的整体性能的下降而非提升.

每个vCPU都是宿主机中一个普通的QEMU线程， 可以使用taskset工具对其设置处理器亲和性， 使其绑定到某一个或几个固定的CPU上去调度.

需求: Iaas为客户提供一个有两个逻辑CPU计算能力的一个客户机, 要求CPU资源独立被占用, 不受宿主机中其他客户机的负载水平的影响.
解决方案:
1. host启动kernel时添加`isolcpus=`以实现cpu的隔离

  通过`ps -eLo psr | grep -e "^[[:blank:]]*<cpu_id>$" | wc -l`查看该cpu上的线程, 隔离后的cpu上跑的线程很少(5~6, 这些线程通常都是内核对各个CPU的守护进程), 可通过`ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==<cpu_id>) print $0}'`查看具体线程, 这些线程有:
  - migration线程（用于进程在不同CPU间迁移）
  - 两个kworker线程（用于处理workqueues）
  - ksoftirqd线程（用于调度CPU软中断的进程）
  - watchdog线程
1. 启动qemu后找到该进程, 在使用`taskset -pc <cpu_list> <pid>`绑定到隔离出的cpu上.

总结: 在KVM环境中， 一般并不推荐手动设置QEMU进程的处理器亲和性来绑定vCPU， 但是， 在非常了解系统硬件架构的基础上， 根据实际应用的需求， 可以将其绑定到特定的CPU上， 从而提高客户机中的CPU执行效率或实现CPU资源独享的隔离性.

### 内存
qemu不设置`-m`默认就是128M, 见`hw/core/machine.c的machine_class_init`. `-m`默认单位是M, 可加上`M/G`后缀来作为内存分配单位

KVM中客户机是一个QEMU进程， 宿主机系统没有特殊对待它而分配特定的内存给QEMU， 只是把它当作一个普通Linux进程. 所以是在客户机操作系统请求更多内存时， KVM才向其分配更多的内存.

#### 内存过载
有如下3种方式来实现内存的过载使用:
1. 内存交换（swapping） ： 用交换空间（swap space） 来弥补内存的不足

  内存交换的方式是最成熟的, 但相比KSM和ballooning的方式效率较低一些.
1. 气球（ballooning） ： 通过virio_balloon驱动来实现宿主机Hypervisor和客户机之间的协作
1. 页共享（page sharing） ： 通过KSM（Kernel Samepage Merging） 合并多个客户机进程使用的相同内存页

KVM允许内存过载使用, 但不建议过多地过载使用内存.

### 存储配置
1. 基本配置参数:

  - -hd<N> file

    将file镜像文件作为客户机中的第1个IDE设备（序号0）, 在客户机中表现为/dev/hda设备（若客户机中使用PIIX_IDE驱动）或/dev/sda设备（若客户机中使用ata_piix驱动）.
  - -fd<N> file

    将file作为客户机中的第1个软盘设备（序号0）, 在客户机中表现为/dev/fd0设备. 也可以将宿主机中的软驱（/dev/fd0） 作为-fda的file来使用
  - -cdrom file

    将file作为客户机中的光盘CD-ROM， 在客户机中通常表现为/dev/cdrom设备. 也可以将宿主机中的光驱（/dev/cdrom） 作为-cdrom的file来使用

    **-cdrom参数不能和-hdc参数同时使用， 因为`-cdrom`就是客户机中的第3个IDE设备**
  - -mtdblock file

    使用file文件作为客户机自带的一个Flash存储器（通常说的闪存）
  - -sd file

    使用file文件作为客户机中的SD卡（Secure Digital Card）
  - -pflash file

    使用file文件作为客户机的并行Flash存储器（Parallel Flash Memory）
1. 支持详细配置的`-drive option[,option[,option[,...]]]`参数, 它定义一个存储驱动器

  - file=file

    使用file文件作为镜像文件加载到客户机的驱动器中
  - if=interface

    指定驱动器使用的接口类型， 可用的类型有： ide、 scsi、 sd、 mtd、 floopy、 pflash、virtio等等.
  - bus=bus， unit=unit

    设置驱动器在客户机中的总线编号和单元编号
  - index=index

    设置在同一种接口的驱动器中的索引编号
  - media=media

    设置驱动器中媒介的类型， 其值为`disk`或`cdrom`
  - snapshot=snapshot

    设置是否启用"-snapshot"选项， 其可选值为"on"或"off". 当snapshot启用时， QEMU不会将磁盘数据的更改写回镜像文件中， 而是写到临时文件中。 当然可以在QEMU monitor中使用"commit"命令强制将磁盘数据的更改保存回镜像文件中.

  - cache=cache


    设置宿主机对块设备数据（包括文件或一个磁盘） 访问中的cache情况， 可以设置为"none"（或"off"） "writeback""writethrough"等. 其默认值是"writethrough".

    - writethrough

      即"直写模式", 在调用write写入数据的同时将数据写入磁盘缓存（disk cache） 和后端块设备（block device） 中， 其优点是操作简单， 其缺点是写入数据速度较慢.
    - writeback

      即"回写模式", 在调用write写入数据时只将数据写入磁盘缓存中即返回， 只有在数据被换出缓存时才将修改的数据写到后端存储中。 其优点是写入数据速度较快， 其缺点是一旦更新数据在写入后端存储之前遇到系统掉电， 数据会无法恢复.
    - none

      关闭缓存的方式, 此时QEMU将在调用open系统调用打开镜像文件时使用"O_DIRECT"标识， 所以其读写数据都是绕过缓存直接从块设备中读写的.
    
    writethrough和writeback在读取数据时都尽量使用缓存. 一些块设备文件（比如qcow2格式文件） 在"writethrough"模式下性能表现很差， 如果这时对性能要求比正确性更高, 建议使用"writeback"模式.

  - aio=aio

    选择异步IO（Asynchronous IO） 的方式， 有"threads"和"native"两个值可选. 其默认值为"threads"， 即让一个线程池去处理异步IO。 而"native"只适用于"cache=none"的情况，就是使用Linux原生的AIO.

  - format=format

    指定使用的磁盘格式， 在默认情况下QEMU自动检测磁盘格式
  - serial=serial

    指定分配给设备的序列号\
  - addr=addr

    分配给驱动器控制器的PCI地址， 该选项只有在使用virtio接口时才适用。
  - id=name

    设置该驱动器的ID， 这个ID可以在QEMU monitor中用"info block"看到
  - readonly=on|off

    设置该驱动器是否只读

1. 配置客户机启动顺序的参数: `-boot [order=drives][,once=drives][,menu=on|off] [,splash=splashfile] [,splash-time=sp-time]`

  在QEMU模拟的x86 PC平台中, 用"a""b"分别表示第1个和第2个软驱, 用"c"表示第1个硬盘， 用"d"表示CD-ROM光驱， 用"n"表示从网络启动. 其中， 默认从硬盘启动， 要从光盘启动可以设置"-boot order=d". "once"表示设置第1次启动的启动顺序, 在系统重启（reboot） 后该设置即无效, 如"-boot once=d"设置表示本次从光盘启动， 但系统重启后从默认的硬盘启动. "memu=on|off"用于设置交互式的启动菜单选项（前提是使用的客户机BIOS支持） ， 它的默认值是"menu=off"， 表示不开启交互式的启动菜单选择. "splash=splashfile"和"splash-time=sp-time"选项都是在"menu=on"时才有效， 将名为splashfile的图片作为logo传递给BIOS来显示； 而sp-time是BIOS显示splash图片的时间， 其单位是毫秒（ms） 。 比如在使用"-boot order=dc， menu=on"设置后， 需在客户机启动窗口中按F12键进入的启动菜单.

### 网络
通过QEMU的支持， 常见的可以实现以下4种网络形式：
1. 基于网桥（bridge） 的虚拟网络

  可以让客户机和宿主机共享一个物理网络设备连接网络, 客户机有自己的独立IP地址, 可以直接连接与宿主机一模一样的网络, 客户机可以访问外部网络,  外部网络也可以直接访问客户机（就像访问普通物理主机一样）. 即使宿主机只有一个网卡设备， 使用bridge模式也可让多个客户机与宿主机共享网络设备.

  bridge模式的网络（后端） 参数配置`-netdev tap,id=id[,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper]`或`-net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper]`. 这两个配置方法效果等价， 但用于不同场合：`-netdev tap， id=id[， ...]与-device参数配套使用`;`-net tap[， ...]与-net nic`参数配套使用, 第2种方法可以认为是第一种的简化版， 因为免去了后端设备id的指定, 具体参数:
  - tap参数， 表明使用TAP设备

    TAP是虚拟网络设备， 它仿真了一个数据链路层设备（ISO七层网络结构的第2层）, 它像一个网桥一样处理第2层数据报. 而TUN与TAP类似，也是一种虚拟网络设备， 它是对网络层设备的仿真. TAP用于创建一个网桥， 而TUN则与路由相关（工作在IP层）.

    > 用tap模式时候, helper方式很不好用， 因为没法在参数里指定bridge名字， 而它只能使用名为br0的bridge连接tap设备.

  - vlan=n， 设置该设备连入VLAN n， 默认值为0（即没有VLAN）
  - name=name， 设置名称， 在QEMU monior中可能用到， 一般由系统自动分配即可
  - fd=h， 连接到现在已经打开着的TAP接口的文件描述符。 一般不要设置该选项， 而是让QEMU自动创建一个TAP接口. 在使用了fd=h选项后， ifname、 script、 downscript、helper、 vnet_hdr等选项都不可用了（不能与fd选项同时出现在命令行中）.
  - ifname=name， 设置在宿主机中添加的TAP虚拟设备的名称（如tap1、 tap5等） 。 当不设置这个参数时， QEMU会根据系统中目前的情况，产生一个TAP接口的名称.
  - script=file， 设置宿主机在启动客户机时自动执行的网络配置脚本. 如果不指定， 其默认值为"/etc/qemu-ifup"这个脚本. 可指定自己的脚本路径以取代默认值； 如果不需要执行脚本， 则设置为"script=no"
  - downscript=dfile， 设置宿主机在客户机关闭时自动执行的网络配置脚本.

    如果不设置， 其默认值为"/etc/qemu-ifdown"； 若客户机关闭时宿主机不需要执行脚本， 则设置为"downscript=no"

    由于QEMU在客户机关闭时会解除TAP设备的bridge绑定， 也会自动删除已不再使用的TAP设备， 所以qemu-ifdown这个脚本不是必需的， 最好设置为"downscript=no".
  - helper=helper， 设置启动客户机时在宿主机中运行的辅助程序， 包括建立一个TAP虚拟设备， 默认值为/usr/local/libexec/qemu-bridge-helper.  此处一般不用定义.

  使用bridge步骤:
  ```bash
  # dnf install bridge-utils tunctl
  # modprobe tun bridge
  # ll /dev/net/tun # 确保当前用户对该文件有可读写权限
  # brctl addbr virbr0 # 创建名为virbr0的bridge, 配置在/etc/sysconfig/network-scripts/ifcfg-virbr0
  # brctl addif virbr0 eno2 # 将将eno2绑定到virbr0上
  # ifup virbr0
  ```

  新版qemu由更直接的网桥配置选项: `-netdev bridge,id=id[,br=bridge][,helper=helper]`或`-net bridge[,vlan=n][,name=name][,br=bridge][,helper=helper]`. 它们比上面的方式省掉了关于tap name、 script等的参数指定, 而只需要指定网桥就可以了. 其他的都封装在helper程序里面自动帮忙做掉了， 包括自动命名和创建tap设备、 自动启动tap设备（即原来的script脚本要完成的工作）、绑定网桥等. 这些本来在多数应用场景中就不需要特别指定的.

2. 基于NAT（Network Addresss Translation） 的虚拟网络

  在QEMU/KVM中， 默认使用IP伪装的方式实现NAT， 而不是使用SNAT（SourceNAT） 或DNAT（Destination-NAT） 的方式.

  在KVM中配置客户机的NAT网络方式， 需要在宿主机中运行一个DHCP服务器给宿主机分配NAT内网的IP地址, 可以使用dnsmasq工具来实现.

  > libvirt的3种虚拟网络的实现（NAT、 Routed、 Isolated）, 在Hypervisor是QEMU/KVM的情况下， 就是通过QEMU的bridge网络模式来实现的.
3. QEMU内置的用户模式网络（user mode networking）

  提供了一种由qemu自身实现的用户模式（user-mode） 的网络模拟, 不依赖于其他的工具（如前面提到的bridge-utils、 dnsmasq、 iptables等） ， 而且不需要root用户权限. QEMU使用Slirp在源码的`slirp`目录实现了一整套TCP/IP协议栈， 并且使用这个协议栈实现了一套虚拟的NAT网络.

  用户模式网络有以下3个缺点：
  1. 由于其在QEMU内部实现所有网络协议栈， 因此其性能较差。
  1. 不支持部分网络功能（如ICMP） ， 所以不能在客户机中使用ping命令测试外网连通性
  1. 不能从宿主机或外部网络直接访问客户机

  使用方法: `-netdev user,id=id[,option][,option][,...]`或`-net user[,option][,option][,...]`, 常用选项有:
  - vlan=n : 将用户模式网络栈连接到编号为n的VLAN中（默认值为0）
  - name=name : 分配一个在QEMU monitor中会用到的名字（如在monitor的"info network"命令中可看到这个网卡的name）
  - net=addr[/mask] : 设置客户机可以看到的网络地址（客户机所在子网） ， 其默认值是10.0.2.0/24. 其中， 子网掩码（mask） 有两种形式可选，一种是类似于255.255.255.0这样的地址， 另一种是32位IP地址中前面被置位为1的位数（如10.0.2.0/24）.
  - host=addr， 指定客户机可见宿主机的地址， 默认值为客户机所在网络的第2个IP地址（如10.0.2.2）
  - ipv6-net=addr[/int]， 设置客户机看到的IPv6网络地址， 默认是`fec0::/64`
  - ipv6-host=addr : 设置客户机的IPv6地址， 默认是第2个IP地址`xxxx::2`
  - restrict=y|yes|n|no， 如果将此选项打开（ 为y或yes） ， 则客户机将会被隔离， 客户机不能与宿主机通信，其IP数据包也不能通过宿主机而路由到外部网络中。 这个选项不会影响"hostfwd"显式地指定的转发规则， 即hostfwd"选项始终会生效。 默认值为n或no， 不会隔离客户机。
  - hostname=name: 设置在内置的DHCP服务器中保存的客户机主机名
  - dhcpstart=addr: 设置能够分配给客户机的第1个IP， 在QEMU内嵌的DHCP服务器有16个IP地址可供分配。 在客户机中IP地址范围的默认值是子网中的第15～30个IP地址（ 如10.0.2.15～10.0.2.30）
  - dns=addr : 指定虚拟DNS的地址， 这个地址必须与宿主机地址（ 在"host=addr"中指定的） 不相同， 其默认值是网络中的第3个IP地址（ 如10.0.2.3）
  - tftp=dir : 激活QEMU内嵌的TFTP服务器， 目录dir是TFTP服务的根目录. 在客户机使用TFTP客户端连接TFTP服务后需要使用binary模式来操作。
  - bootfile=file， 与tftp=dir配合使用， 可以实现虚拟的PXE boot. 比如"-boot n-net user， tftp=/path/to/tftp/files，bootfile=/pxelinux.0"就指定了客户机PXE启动文件， 它位于QEMU虚拟的tftp服务器路径下.
  - smb=dir[， smbserver=addr] : QEMU可以激活（ 模拟） 一个内置的Samba服务器，作为连接host和Windows客户机的文件传输的纽带。"dir"指示的就是host上存放共享文件的文件夹， "smbserver"（ 可选） 指定对客户机而言的Samba server的IP地址， 默认是net的第4个IP， 即x.x.x.4. 注意， Windows客户机需要把这个Samba server的IP地址静态解析（ "10.0.2.4 smbserver"这样一行） 写入它的LMHOSTS文件中， 比如Windows 10系统的C：\Windows\System32\drivers\etc\lmhosts.sam文件. 这样， 在客户机中， 就可以通过`\\smbserver\qemu`访问宿主机的"dir"文件夹了。 注意, 宿主机要求安装好Samba服务并启动.
  - ipv6-dns=addr: 指定虚拟的IPv6 DNS的地址， 这个地址必须与宿主机地址（ 在"ipv6-host=addr"中指定的） 不相同，其默认值是网络中的第3个IP地址（ 如xxxx： ： 3）.
  - dnssearch=domain : 内置的DHCP server在分配IP给客户机的时候， 会附带DNS域的信息. 这个参数就是指定域列表（ 可以是多个）
  - hostfwd=`[tcp|udp]:[hostaddr]:hostport-[guestaddr]:guestport`， 将访问宿主机的hostpot端口的TCP/UDP连接重定向到客户机（ IP为guestaddr） 的guestport端口上. 如果没有设置guestaddr， 那么默认使用x.x.x.15（ DHCP服务器可分配的第1个IP地址）.如果指定了hostaddr的值， 则可以根据宿主机上的一个特定网络接口的IP端口来重定向. 如果没有设置连接类型为TCP或UDP， 则默认使用TCP连接。 "hostfwd=…"这个选项在一个命令行中可以多次重复使用.
  - guestfwd=`[tcp]:server:port-dev`及`guestfwd=[tcp]:server:port-cmd:command`, 将客户机中访问IP地址为server的port端口的连接转发到宿主机的dev这个字符设备上. 或者每次访问这个server:port就执行一次命令（cmd即具体命令）. "guestfwd=…"这个选项也可以在一个命令行中多次重复使用

  其他不常用网络选项:
  - `-net socket[,vlan=n][,name=name][,fd=h][,listen=[host]:port][,connect=host:port]`: 使用TCP socket连接客户机的VLAN

    使用TCP socket连接将n号VLAN连接到一个远程的QEMU虚拟机的VLAN。 如果有"listen=…"参数， 那么QEMU会等待对port端口的连接， 而host参数是可选的（默认值为本机回路IP地址127.0.0.1） 。 如果有"connect=…"参数， 则表示连接远端的已经使用"listen"参数的QEMU实例。 可使用"fd=h"（文件描述符h） 指定一个已经存在的TCP socket
  - `-net socket[,vlan=n][,name=name][,fd=h][,mcast=maddr:port]`: 使用UDP的多播socket建立客户机间的连接

    建立n号VLAN， 使用UDP多播socket连接使其与另一个QEMU虚拟机共享， 用相同的多播地址（maddr） 和端口（port） 为每个QEMU虚拟机建立同一个总线. 多播的支持还与[用户模式Linux（User-Mode Linux）](http://user-mode-linux.sourceforge.net/)兼容
  - `-net vde[,vlan=n][,name=name][,sock=socketpath][,port=n][,group=groupname][,mode= octalmode]`:使用VDE swith的网络连接

    连接n号VLAN到一个VDE switch的n号端口， 这个VDE switch运行在宿主机上并且监听着在socketpath上进来的连接. 它使用groupname和octalmode（八进制模式的权限设置） 去更改通信端口的拥有组和权限。 这个选项只有在QEMU编译时有了对VDE的支持后才可用。
  - `-net dump[,vlan=n][,file=file][,len=len]` : 转存（dump） 出VLAN中的网络数据

    将编号为n的VLAN中的网络流量转存（dump） 出来保存到file文件中（默认为当前目录中的qemu-vlan0.pcap文件）. 最多截取并保存每个数据包中的前len（默认值为64） 个字节的内容. 保存的文件格式为libcap， 故可以使用tcpdump、 Wireshark等工具来分析转存出来的文件.
  - `-net none` : 不分配任何网络设备

    单独使用它时, 表示不给客户机配置任何网络设备，可用于覆盖默认的`-net nic-net user`

4. 直接分配网络设备从而直接接入物理网络（包括VT-d和SR-IOV）

qemu使用`-net`进行网络配置, 默认使用`-net nic-net user`即使用完全基于QEMU内部实现的用户模式下的网络协议栈. **在新的QEMU中, 推荐用-device+-netdev组合的方式**. 因为QEMU正逐渐规范统一的设备模型的参数使用, -device囊括了所有QEMU模拟的前端的参数指定， 也就是客户机
里看到的设备（包括本章的网卡设备）; -netdev指定的是网卡模拟的后端方式， 也就是本节后面要讲的各种QEMU实现网络的方式.

> `qemu-system-x86_64 -device help`显示支持哪些device

> 通过`qemu-system-x86_64 -net nic,model=?`查看当前qemu支持模拟的网卡设备. `e1000`是qemu默认提供的intel e1000系列的网卡模拟. 在qemu monitor可用`info qtree/network`查看模拟网卡的详细信息

`-net nic[,vlan=n][,macaddr=mac][,model=type][,name=name][,addr=addr][,vectors=v]`选项:
- -net nic : 必需的参数, 表明这是一个网卡的配置
- vlan=n : 表示将网卡连入VLAN n， 默认为0（即没有VLAN）
- macaddr=mac， 设置网卡的MAC地址， 默认根据宿主机中网卡的地址来分配. 若局域网中客户机太多, 建议自己设置MAC地址, 以防止MAC地址冲突. 在实际使用
中建议配置自己的MAC地址, 否则QEMU会默认分配一个相同的MAC地址（`52:54:00:12:34:56`）, 此时几个虚拟机同时运行则可能会通过DHCP分配到相同的IP, 从而引发IP地址冲突.
- model=type : 设置模拟的网卡的类型, 默认为e1000
- name=name : 为网卡设置一个易读的名称， 该名称仅在QEMU monitor中可能用到
- addr=addr : 设置网卡在客户机中的PCI设备地址为addr
- vectors=v， 设置该网卡设备的MSI-X向量的数量为v， 该选项仅对使用virtio驱动的网卡有效. `vectors=0`是关闭virtio网卡的MSI-X中断方式

### 图形显示
在QEMU模拟器中的图形显示默认使用的就是SDL, 需要在编译QEMU时配置对SDL的支持. 它也有局限性, 那就是在创建客户机并以SDL方式显示时会直接弹出一个窗口， 所以SDL方式只能在图形界面中使用.

> SDL（Simple DirectMedia Layer） 是一个用C语言编写的、 跨平台的、 免费和开源的多媒体程序库， 它提供了一个简单的接口用于操作硬件平台的图形显示、 声音、 输入设备等.

> QEMU默认使用Ctrl+Alt组合键来实现鼠标在客户机与宿主机中的切换

> QEMU命令行提供了"-no-quit"参数来去掉SDL窗口的直接关闭功能, 避免误操作而关闭guest. 在加了"-no-quit"参数后， SDL窗口中的"关闭"按钮的功能将会失效， 而最小化、
最大化（或还原） 等功能正常

#### vnc
VNC（Virtual Network Computing） 是图形化的桌面分享系统， 它使用RFB（Remote FrameBuffer） 协议来远程控制另外一台计算机系统. 尽管QEMU仍然采用SDL作为默认的图形显示方式， 但VNC的管理方式在虚拟化环境中使用得更加广泛.

在qemu命令行中， 添加"-display vnc=displayport"参数就能让VGA显示输出到VNC会话中而不是SDL中. 如果在进行QEMU编译时没有SDL的支持， 有VNC的支持， 则qemu命令行在启动客户机时不需要"-vnc"参数也会自动使用VNC而不是SDL.

displayport参数取值:
- host： N

  表示仅允许从host主机的N号显示窗口来建立TCP连接到客户机。 在通常情况下，QEMU会根据数字N建立对应的TCP端口， 其端口号为5900+N。 而host值在这里是一个主机名或一个IP地址， 是可选的， 如果host值为空， 则表示QEMU建立的Server端接受来自任何主机的连接。 增加host参数值， 可以阻止来自其他主机的VNC连接请求， 从而在一定程度上提高了使用QEMU的VNC服务的安全性。
- to=L

  QEMU在上面指定的端口（5900+N） 已被被其他应用程序占用的情况下， 会依次向后递增尝试。 这里to=L， 就表示递增到5900+L端口号为止， 不再继续往后尝试。 默认为0， 即不尝试.
- unix： path

  表示允许通过Unix domain socket连接到客户机， 而其中的path参数是一个处于监听状态的socket的位置路径。 这种方式不常用.
- none

  表示VNC已经被初始化， 但是并不在开始时启动。 而在需要真正使用VNC之时， 可以在QEMU monitor中用change命令启动VNC连接

作为可选参数的option则有如下几个可选值， 每个option标志用逗号隔开:
1. reverse

  表示"反向"连接到一个处于监听中的VNC客户端， 这个客户端是由前面的display参数（host： N） 来指定的。 需要注意的是， 在反向连接这种情况下， display中的端口号N是对端（客户端） 处于监听中的TCP端口， 而不是现实窗口编号， 即如果客户端（IP地址为IP_Demo） 已经监听的命令为"vncviewer-listen： 2"， 则这里的VNC反向连接的参数为"-vnc IP_Demo： 5902， reverse"， 而不是用2这个编号
1. password

  表示在客户端连接时需要采取基于密码的认证机制， 但这里只是声明它使用密码验证， 其具体的密码值必须在QEMU monitor中用change命令设置
1. "tls""x509=/path/to/certificate/dir""x509verify=/path/to/certificate/dir""sasl"和"acl"

  这5个选项都是与VNC的验证、 安全相关的选项

> QEMU monitor中的"change vnc xxx"命令可在guest设置VNC参数并启动后动态地修改VNC的参数.

> VNC显示中的鼠标偏移: 窗口显示2个鼠标且通常不重合. 解决方法: 将"-usb"和"-usbdevice tablet"这两个USB选项一起使用. "tablet"类型的设备是一个使
用绝对坐标定位的指针设备， 就像在触摸屏中那样定位， 这样可以让QEMU能够在客户机不抢占鼠标的情况下获得鼠标的定位信息. 在最新的QEMU中， 与"-usb-usbdevice tablet"参数功能相同， 也可以用"-device piix3-usb-uhci-device usb-tablet"参数.

在qemu命令行中， 添加"-nographic"参数可以完全关闭QEMU的图形界面输出， 从而让QEMU在该模式下成为简单的命令行工具. 此时需要修改客户机的grub配置(by kernel boot args:`console=ttyS`)， 使其在kernel行中加上将console输出重定向到串口ttyS0.

其他图形选项:
-curses

  让QEMU将VGA显示输出到使用curses/ncurses接口支持的文本模式界面， 而不是使用SDL来显示客户机. 与"-nographic"模式相比， 它的好处在于， 由于它是接收客户机VGA的正常输出而不是串口的输出信息， 因此不需要额外更改客户机配置将控制台重定向到串口. 为了使用"-curses"选项，在宿主机中必须有"curses或ncurses"这样的软件包提供显示接口的支持并编译QEMU时启用了它.
- -vga type

  选择为客户机模拟的VGA卡的类别， 可选类型有如下6种:
  1. cirrus

    为客户机模拟出"Cirrus Logic GD5446"显卡， 在客户机启动后， 可以在客户机中看到VGA卡的型号， 如在Linux中可以用"lspci"查看到VGA卡的信息。 这个选项对图形显示的体验并不是很好， 它的彩色是16位的， 分辨率也不高， 仅支持2D显示， 不支持3D。 不过绝大多数的系统（包括Windows 95） 都支持这个系列的显卡
  1. std

    模拟标准的VGA卡， 带有Bochs VBE扩展。 当客户机支持VBE 2.0及以上标准时（ 目前流行的操作系统多数都支持） ， 如果需要支持更高的分辨率和彩色显示深度， 就会使用这个选项.
  1. VMware

    提供对"VMware SVGA-II"兼容显卡的支持
  1. virtio

    半虚拟化的VGA显卡模拟
  1. qxl

    它也是一种半虚拟化的模拟显卡， 与VGA兼容。 当使用spice display的时候， 推荐选择这种显卡.
  1. none

    关闭VGA卡， 使SDL或VNC窗口中无任何显示。 一般不使用这个参数.
- -no-frame

  使SDL显示时没有边框
- -full-screen

  在启动客户机时， 自动使用全屏显示
- -alt-grab

  使用"Ctrl+Alt+Shift"组合键去抢占和释放鼠标， 从而使"Ctrl+Alt+Shift"组合键成为QEMU中的一个特殊功能键。 在QEMU中默认使用"Ctrl+Alt"组合键， 所以本书常提到在SDL或VNC中用"Ctrl+Alt+2"组合键切换到QEMU monitor的窗口， 而使用了"-alt-grab"选项后， 应该相应改用"Ctrl+Alt+Shift+2"组合键切换到QEMU monitor窗口。
- -ctrl-grab

  使用右"Ctrl"键去抢占和释放鼠标， 使其成为QEMU中的特殊功能键。 这与前面的"-alt-grab"的功能类似

## vt-d
在QEMU/KVM中， 客户机可以使用的设备大致可分为如下3种类型:
- Emulated device： QEMU纯软件模拟的设备， 比如-device rt8139等
- virtio device： 实现VIRTIO API的半虚拟化驱动的设备， 比如-device virtio-net-pci等
- PCI device assignment： PCI设备直接分配

> 较新的x86架构的主要硬件平台（包括服务器级、 桌面级） 都已经支持设备直接分配， 其中Intel定义的I/O虚拟化技术规范为“Intel(R)Virtualization Technology for
Directed I/O”（VT-d） ， 而AMD的I/O虚拟化技术规范为“AMD-Vi”（也叫作IOMMU）.

## kvm调优
- cpu调优

  vm对物理机cpu逻辑核的手工绑定, 需要了解NUMA

  步骤:
  1. `numactl --hardware`, 查看硬件配置
  1. `numastat -c qemu-system-x86_64`, 查看相关进程的numa内存统计
  1. `virsh numatune`修改vm的numa配置

    <vcpu>和<numatune>需保持一致, <numatune>配置的是物理cpu, <vcpu>配置的是逻辑cpu(包括超线程产生的核). <numatune>使用static模式时<nodeset>也必须是.

- 内存调优

  KSM, 即相同内存页合并, 内存气球技术及大页内存的使用

## Libvirt
Libvirt是由Redhat开发的一套开源的软件工具，目标是提供一个通用和稳定的软件库来高效、安全地管理一个节点上的虚拟机，并支持远程操作. libvirt 已经成为使用最为广泛的对各种虚拟机进行管理的工具和应用程序接口（API），而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口.

基于可移植性和高可靠性的考虑，Libvirt采用C语言开发，但是也提供了对其他编程语言的绑定，包括Python、Perl、OCaml、Ruby、Java和PHP. 同时Libvirt支持多种VMM，包括LXC、KVM/QEMU、Xen、VirtualBox等.

为了支持多种VMM，Libvirt采用了基于Driver的架构(中间人)，每种VMM需要提供一个驱动和Libvirt进行通信来操控特定的VMM. 在初始化过程中，所有的驱动被枚举和注册. 每一个驱动都会加载特定的函数为Libvirt API调用.

### Libvirt API
Libvirt定义了各种各样的API，涉及虚拟化的方方面面，主要分为以下几类:
- 虚拟机快照：快照是包括内存、硬盘等信息在内的完整虚拟机状态。这些API就是用于创建、删除和恢复快照的。
- 虚拟机管理：这一类API用于管理虚拟机，也是Libvirt里面使用最频繁的功能。例如：创建、销毁、重启、迁移虚拟机，以及操作虚拟机的磁盘镜像等。
- 事件：事件是Libvirt定义的一套监测特定情况发生的机制，用户可以通过相应的API告诉Libvirt，想要监测什么样的事件，或者事件发生时采取什么样的操作。
- 存储管理

  Libvirt在任何运行了Libvirt daemon的主机端通过管理存储池和卷来为虚拟机提供存储资源，可以支持包括本地文件系统、网络文件系统、iSCSI、LVM等多种后端存储系统.
  虚拟机磁盘格式上支持qcow2、vmdk、raw等格式.

- 宿主机：用于获取宿主机的各种信息，包括机器名、CPU状态等，也用于和特定的VMM建立连接。
- 网络接口：实现网络接口的相应操作，如定义一个新的网络接口

  支持Linux桥、vlan、多网卡绑定管理，比较新的版本还支持open vswitch. libvirt还支持nat和路由方式的网络，可以通过防火墙让虚拟机通过宿主机建立网络通道和外部网络进行通信.
- 错误管理：提供了Libvirt本身的错误管理机制，比如获取最近一次的Libvirt错误。
- 其他设备管理：包括对网络、PCI、USB等设备的管理。

### Libvirt存储池和存储卷
为了提供统一的接口给虚拟机来访问不同的后端存储设备，Libvirt将存储管理分为两个方面：存储卷和存储池. 存储卷可以作为存储设备分配给虚拟机使用，物理上可以是一个虚拟机磁盘文件或一个真实的磁盘分区. 存储池可以被理解为本地目录，或者通过各类分布式存储系统分配过来的目录等，存储卷可以从存储池中生成，Libvirt可以支持多种存储池类型，内容如下:
- 目录池（Directory Pool）：以主机的一个目录作为存储池
- 本地文件系统池（Filesystem Pool）：使用主机已经格式化好的块设备作为存储池，支持的文件系统类型包括ext4、XFS等
- 网络文件系统池（Network Filesystem Pool）：使用远端网络文件系统服务器的导出目录作为存储池
- 逻辑卷池（Logical Volume Pool）：使用已经创建好的LVM卷组，或者基于一系列生成卷组的源设备生成卷组，生成存储池
- 磁盘卷池（Disk Pool）：使用磁盘作为存储池
- iSCSI卷池（iSCSI Pool）：使用iSCSI设备作为存储池
- SCSI卷池（SCSI Pool）：使用SCSI设备作为存储池
- 多路设备池（Multipath Pool）：使用多路设备作为存储池
- RBD Pool：包括一个RADOS池的所有RBD image

Libvirt中的存储管理独立于虚拟机的管理，存储卷从存储池中被划分出来，存储卷分配给虚拟机成为可用的存储设备. 通过virsh工具的pool命令可以查看、创建、激活、注册、删除存储池，因此创建存储资源时，并不需要有虚拟机的存在.

### libvirt cmds
参考:
- [virt-install和virsh详解](https://yq.aliyun.com/articles/529107)

```bash
# apt install libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager
# lsmod | grep -i kvm
# sudo systemctl is-active libvirtd
# usermod -aG libvirt $USER
# usermod -aG kvm $USER
# brctl show # a virtual bridge "virbr0" is created automatically, but this is only used for testing purpose.
# cat << EOF > /etc/netplan/00-installer-config.yaml # To create a network bridge, access the guests from outside the local network
# This is the network config written by 'subiquity'
network:
  ethernets:
    enp0s3:
      dhcp4: no
      dhcp6: no
  version: 2
  bridges:
    br0:
      interfaces: [enp0s3]
      addresses: [172.20.10.9/28]
      gateway4: 172.20.10.1
      nameservers:
        addresses: [4.2.2.2, 8.8.8.8]
EOF
# netplan apply # activate the bride br0
# networkctl status br0 # verify the status of br0 bridge
# ip a s
```

- libvirt-daemon-system : configuration files to run the libvirt daemon as a system service.
- libvirt-clients : software for managing virtualization platforms.
- bridge-utils : a set of command-line tools for configuring ethernet bridges.
- virtinst : a set of command-line tools for creating virtual machines.
- virt-manager : an easy-to-use GUI interface and supporting command-line utilities for managing virtual machines through libvirt.

#### virt-install
通过命令行创建kvm vm的工具, 最终生成一个xml格式的vm配置文件.

```bash
# virt-install --name=testvm --rame=2048 --vCPUs=4 --os-type=linux --hvm --cdrom=/xxx.iso --file=/xxx.img --file-size=10 --bridge=br0 --vnc --vncport=5920
```

解析:
- --bridge=xxx: 连接到名为xxx的网桥上
- --cdrom : 使用cdrom iso安装系统
- --file : vm硬盘文件路径
- --file-size : vm硬盘文件大小, 单位GB
- -p/--paravirt : 指定使用半虚拟化
- -v/--hvm : 指定使用全虚拟化
- --virt-type：使用的hypervisor，如kvm、qemu、xen等；所有可用值可以使用`virsh capabilities`命令获取
- --vnc : 开启vnc支持
- --vncport : 配置vnc端口

## FAQ
### 虚拟机XML格式
ref:
- [<<KVM实战>>]

```xml
<domain type='kvm' id='29'>
//domain 是一个所有虚拟机都需要的根元素，它有两个属性: 1. type定义使用哪个虚拟机管理程序，值可以是：xen、kvm、qemu、lxc、kqemu, vmvare中的一个;2. id，它唯一的标示一个运行的虚拟机. 如果不设置id属性， libvirt会按顺序分配一个最小的可用ID.
  <name>i-000039</name>
//name参数为虚拟机定义了一个简短的名字，在同一host上必须唯一

  <uuid>d59b03ce-2e78-4d35-b731-09d9ca9653af</uuid>
//uid为虚拟机定义了一个全球唯一的标示符，uuid的格式必须遵循RFC 4122指定的格式，当创建虚拟机没有指定uuid时会随机的生成一个uuid. 自己可用命令行工具 uuidgen生成.

  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
  <memtune>
    <hard_limit unit='KiB'>4194304</hard_limit>
  </memtune>
    //memory 定义客户端启动时可以分配到的最大内存，内存单位由unit定义，单位可以是：K、KiB、M、MiB、G、GiB、T、TiB。默认是KiB
    // currentMemory标签中的内存表示启动时即分配给客户机使用的内存. 在使用QEMU/KVM时， 一般将二者设置为相同的值
  
  
  <vcpu placement='static' cpuset="1-4,^3,6" current="1">2</vcpu>
  //vcpu的内容是为虚拟机最多分配几个cpu，值处于1~maxcpu之间，可选参数：cpuset参数指定虚拟cpu可以映射到那些物理cpu上，物理 cpu用逗号分开，单个数字的标示单个cpu，
  //也可以用range符号标示多个cpu，数字前面的脱字符标示排除这个cpu，current参数指定虚拟 机最少，placement参数指定一个domain的cpu的分配模式，值可以是static、auto. 比如`1-4,^3,6`表示客户机的两个vCPU被允许调度到
1、 2、 4、 6号物理CPU上执行（^3表示排除3号）
  // current表示启动客户机时只给1个vCPU， 最多可以增加到使用2个vCPU
  
  // 对CPU的分配进行更多调节
  // vcpupin标签表示将虚拟CPU绑定到某一个或多个物理CPU上， 如"<vcpupin vcpu="2"cpuset="4"/>"表示客户机2号虚拟CPU被绑定到4号物理
CPU上； "<emulatorpin cpuset="1-3"/>"表示将QEMU emulator绑定到1~3号物理CPU上。 在不设置任何vcpupin和cpuset的情况下, 客户机的虚拟CPU可能会被调度到任何一个物理CPU上去运行
  // shares表示客户机占用CPU时间的加权配置. 一个配置为2048的域获得的CPU执行时间是配置为1024的域的两倍. 如果不设置shares值， 就会使
用宿主机系统提供的默认值
  <cputune>
    <vcpupin vcpu="0" cpuset="1"/>
    <vcpupin vcpu="1" cpuset="2,3"/>
    <vcpupin vcpu="2" cpuset="4"/>
    <vcpupin vcpu="3" cpuset="5"/>
    <emulatorpin cpuset="1-3"/>
    <shares>1024</shares>
    <period>100000</period>
    <quota>-1</quota>
  </cputune>
  <resource>
    <partition>/machine</partition>
  </resource>
  
  
 //操作系统启动介绍
  <os>
    <type arch='x86_64' machine='pc-i440fx-2.8'>hvm</type>
    //type参数指定了虚拟机操作系统的类型，内容：hvm(hardware virtual machine)表明表示在硬件辅助虚拟化技术（Intel VT或AMD-V等）的支持下不需要修改客户机操作系统就可以启动客户机. 因为KVM一定要依赖于硬件虚拟化技术的支持, 所以在KVM中, 客户机类型应该总是hvm, **常用**.
    //type同样有两个可选参数：arch指定虚拟机的CPU构架，machine指定机器的类型.
    //<boot dev='hd'/>dev属性的值可以是：fd、hd、cdrom、network，它经常被用来指定下一次启动. boot的元素可以被设置多个用来建立一个启动优先规则.
    <boot dev='hd'/>
    <boot dev='cdrom'/>
    <bootmenu enable='yes' timeout='0'/>
    <bios useserial='yes'/>
  </os>
  
  Hypervisor的特性
  <features>
    <acpi/>
    <apic/>
    <!-- <pae/> -->
  </features>
  Hypervisors允许特定的CPU/机器特性打开或关闭，所有的特性都在fearures元素中，以下介绍一些在全虚拟化中常用的标记：
pae：扩展物理地址模式，使32位的客户端支持大于4GB的内存
acpi：用于电源管理
hap：Enable use of Hardware Assisted Paging if available in the hardware.
  
  
  
  //cpu分配
  // CPU模型： Haswell-noTSX， 可以在文件/usr/share/libvirt/cpu_map.xml中查看详细描述
  // custom模式：基于某个基础的CPU模型， 再做个性化的设置
  // host-model模式： 根据物理CPU的特性， 选择一个与之最接近的标准CPU型号， 如
果没有指定CPU模式， 默认也是使用这种模式。 xml配置文件为： <cpu mode='hostmodel'/>
  // host-passthrough模式： 直接将物理CPU特性暴露给虚拟机使用， 在虚拟机上看到
的完全就是物理CPU的型号。 xml配置文件为： <cpu mode='host-passthrough'/>
  <cpu mode='custom' match='exact'>
    <model fallback='allow'>Haswell-noTSX</model>
    <topology sockets='1' cores='1' threads='1'/>
    <numa>
      <cell id='0' cpus='0' memory='4194304' unit='KiB'/>
    </numa>
  </cpu>
  
  时间设置
  <clock offset='variable' adjustment='0' basis='utc'>
    <timer name='rtc' track='guest'/>
  </clock>
  客户端的时间初始化来自宿主机的时间，大多数操作系统期望硬件时钟保持UTC格式，UTC也是默认格式，然而Windows机器却期望它是’localtime’
clock的offset属性支持四种格式的时间：UTC localtime timezone variable
UTC：当引导时客户端时钟同步到UTC时钟
localtime：当引导时客户端时钟同步到主机时钟所在的时区
timezone：The guest clock will be synchronized to the requested timezone using the timezone attribute.

  
  //控制周期：
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>preserve</on_crash>
  <on_lockfailure>poweroff</on_lockfailure>
  //当一个客户端的OS触发lifecycle时，它将采取新动作覆盖默认操作，具体状态参数如下：
 //on_poweroff：当客户端请求poweroff时执行特定的动作
  //on_reboot：当客户端请求reboot时执行特定的动作
  // on_crash：当客户端崩溃时执行的动作
   //每种状态下可以允许指定如下四种行为：
    //destory：domain将会被完全终止，domain的所有资源会被释放
    //restart：domain会被终止，然后以相同的配置重新启动
    //preserver：domain会被终止，它的资源会被保留用来分析
    //rename-restart：domain会被终止，然后以一个新名字被重新启动
  
  
  <devices>
  //所有的设备都是一个名为devices元素的子设备(All devices occur as children of the main devices element.)，以下是一个简单的配置：
//<emulator>/usr/bin/kvm</emulator>
//emulator元素指定模拟设备二进制文件的全路径
    <emulator>/usr/libexec/qemu-kvm</emulator>
    // <disk>标签是客户机磁盘配置的主标签, 其中包含它的属性和一些子标签. 它的type属性表示磁盘使用哪种类型作为磁盘的来源， 其取值为file、 block、 dir或network中的一个， 分别表示使用文件、 块设备、 目录或网络作为客户机磁盘的来源. 它的device属性表示让客户机如何来使用该磁盘设备， 其取值为floppy、 disk、 cdrom或lun中的一个， 分别表示软盘、 硬盘、 光盘和LUN（逻辑单元号）, 默认值为disk（硬盘）.
    // <disk>的<driver>子标签用于定义Hypervisor如何为该磁盘提供驱动， 它的name属性用于指定宿主机中使用的后端驱动名称， QEMU/KVM仅支持name='qemu'， 但是它支持的类型type可以是多种， 包括raw、 qcow2、 qed、 bochs等. 而这里的cache属性表示在宿主机中打开该磁盘时使用的缓存方式， 可以配置为default、 none、 writethrough、 writeback、directsync和unsafe等多种模式.
    // <disk>的<source>子标签表示磁盘的来源， 当<disk>标签的type属性为file时， 应该配置为<source file='/var/lib/libvirt/images/centos7u2-1.img'/>这样的模式， 而当type属性为block时， 应该配置为<source dev='/dev/sda'/>这样的模式
    // <disk>的<target>子标签表示将磁盘暴露给客户机时的总线类型和设备名称. 其dev属性表示在客户机中该磁盘设备的逻辑设备名称， 而bus属性表示该磁盘设备被模拟挂载的总线类型， bus属性的值可以为ide、 scsi、 virtio、 xen、 usb、 sata等. 如果省略了bus属性， libvirt
则会根据dev属性中的名称来"推测"bus属性的值， 例如， sda会被推测是scsi， 而vda被推测是virtio.
  // <disk>的<address>子标签表示该磁盘设备在客户机中的PCI总线地址, 这个标签在前面网络配置中也是多次出现的, 如果该标签不存在, libvirt会自动分配一个地址
    <disk type='file' device='cdrom'>
      <backingStore/>
      <target dev='hdd' bus='ide'/>
      <readonly/>
      <boot order='2'/>
      <alias name='ide0-1-1'/>
      <address type='drive' controller='0' bus='1' target='0' unit='1'/>
    </disk>
    // 使用qcow2格式的镜像文件作为客户机的磁盘, 其在客户机中使用virtio总线（使用virtio-blk驱动）, 设备名称为/dev/vda, 其PCI地址为0000:01:01.0
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/ae386362-eed6-43a8-b5a8-11a42fabc0ed'/>
      <backingStore/>
      <target dev='vda' bus='virtio'/>
      <boot order='1'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'/>
    </disk>
    // 根据客户机架构的不同， libvirt默认会为客户机模拟一些必要的PCI控制器（而不需要在XML配置文件中指定）, 而一些PCI控制器需要显式地在XML配置文件中配置.
    // libvirt默认还会为客户机分配一些必要的PCI设备， 如PCI主桥（Host bridge）, ISA桥等
    <controller type='usb' index='0' model='ich9-ehci1'>
      <alias name='usb'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='pci-ohci'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </controller>
    <controller type='ide' index='0'>
      <alias name='ide'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'>
      <alias name='pci.0'/>
    </controller>
    <controller type='pci' index='1' model='pci-bridge'>
      <model name='pci-bridge'/>
      <target chassisNr='1'/>
      <alias name='pci.1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <lease>
      <lockspace>6ee684f1-8b25-4f0a-9721-fe96540c1870</lockspace>
      <key>ae386362-eed6-43a8-b5a8-11a42fabc0ed</key>
      <target path='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/.6ee684f1-8b25-4f0a-9721-fe96540c1870/.leases' offset='4194304'/>
    </lease>
    
    
    网络接口：
有好几种网络接口访问客户端:Virtual network、Bridge to LAN、Userspace SLIRP stack、Generic ethernet connection、Direct attachment to physical interface。
Virtual network：这种推荐配置一般是对使用动态/无线网络环境访问客户端的情况。
Bridge to LAN：这种推荐配置一般是使用静态有限网络连接客户端的情况。
    // type='bridge'表示使用桥接方式使客户机获得网络, 可用`brctl show`查看host bridge. address用于配置客户机中网卡的MAC地址. <source bridge='xlansw-000003'/>表示使用宿主机中的xlansw-000003网络接口来建立网桥， <model type='virtio'/>表示在客户机中使用virtio-net驱动的网卡设备, 也配置了该网卡在客户机中的PCI设备编号为`0000:00:10.0`
    // type='network'和<source network='default'/>表示使用NAT的方式， 并使用默认的网络配置， 客户机将会分配到192.168.122.0/24网段中的一个IP地址. 使用NAT必须保证宿主机中运行着DHCP和DNS服务器, 一般默认使用dnsmasq软件查询. 由于配置使用了默认的NAT网络配置(<source network='default'/>), 可以在libvirt相关的网络配置中看到一个default.xml文件（/etc/libvirt/qemu/networks/default.xml）, 它具体配置了默认的连接方式.
    // type='user'表示该客户机的网络接口是用户模式网络， 是完全由QEMU软件模拟的一个网络协议栈.
    // type='hostdev'(仅支持libvirt 0.9.11以上的版本其仅支持SR-IOV特性中的VF的直接配置; 旧版本直接使用<hostdev>标签)表示使用网卡设备直接分配（VT-d）即将PCI/PCI-e网卡将设备直接分配给客户机使用. <driver name='vfio'/>指定使用哪一种分配方式（默认是VFIO， 如果使用较旧的传统的device assignment方式, 这个值可配为'kvm'）, 用<source>标签来指示将宿主机中的哪个VF分配给宿主机使用, 还可使用<mac address='52:54:00:6d:90:02'>来指定在客户机中看到的该网卡设备的MAC地址. 示例配置如下所示， 它表示将宿主机的`0000:08:10.0`这个VF网卡直接分配给客户机使用， 并规定该网卡在客户机中的MAC地址为"52:54:00:6d:90:02"
    // <hostdev>标签是libvirt 0.9.11版本之前对设备直接分配的唯一使用方式, 而且对设备的支持较为广泛, 既支持有SR-IOV功能的高级网卡的VF的直接分配， 也支持无SR-IOV功能的普通PCI或PCI-e网卡的直接分配. 这种方式并**不支持对直接分配的网卡在客户机中的MAC地址的设置**, 在客户机中网卡的MAC地址与宿主机中看到的完全相同. 示例如下所示, 它表示将宿主机中的PCI 0000:08:00.0设备直接分配给客户机使用
    <interface type='bridge'>
      <mac address='00:16:3e:bd:8e:f3'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='1fb6d3e0-c0c0-4fdd-9edb-c64b1327763f'/>
      </virtualport>
      <target dev='vnd59b03ce0'/>
      <model type='virtio'/>
      <boot order='3'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x10' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:59:09:2d'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f8a225e1-028f-4396-8107-879f5b77152c'/>
      </virtualport>
      <target dev='vnd59b03ce1'/>
      <model type='rtl8139'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x11' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:97:5b:c0'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f688fbc6-c4e3-4348-9408-12dc903dab06'/>
      </virtualport>
      <target dev='vnd59b03ce2'/>
      <model type='virtio'/>
      <alias name='net2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x12' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:32:7d:f6'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='user'>
      <mac address="00:11:22:33:44:55"/>
    </interface>
    <interface type='hostdev'>
      <driver name='vfio'/>
      <source>
      <address type='pci' domain='0x0000' bus='0x08' slot='0x10' function= '0x0'/>
      </source>
      <mac address='52:54:00:6d:90:02'>
    </interface>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
      <address domain='0x0000' bus='0x08' slot='0x00' function='0x0'/>
      </source>
  </hostdev>
    
    // 串口和控制台是非常有用的设备， 特别是在调试客户机的内核或遇到客户机宕机的情况下， 一般都可以在串口或控制台中查看到一些利于系统管理员分析问题的日志信息
    //<serial>设置了客户机的编号为0的串口（即/dev/ttyS0）, 使用宿主机中的伪终端（pty）. 没有<source>时libvirt会自己选择一个空闲的虚拟终
端（可能为/dev/pts/下的任意一个）. 当然也可以加上<source path='/dev/pts/1'/>配置来明确指定使用宿主机中的哪一个虚拟终端
    // 在通常情况下, 控制台（console） 配置在客户机中的类型为'serial'时， 如果没有配置串口（serial） ， 则会将控制台的配置复制到串
口配置中， 如果已经配置了串口（本例即是如此） ， 则libvirt会忽略控制台的配置项.
    // 当然为了让控制台有输出信息并且能够与客户机交互， 也需在客户机中配置将信息输出到串口， 如在Linux客户机内核的启动行中添加"console=ttyS0"这样的配置
    <serial type='pty'>
      <source path='/dev/pts/14'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty'>
    <target type='serial' port='0'/>
    </console>
    在每组指令中，最顶层的指令(parallel, serial, console, channel)描述设备怎样出现在客户端中，客户端接口通过target配置。
The interface presented to the host is given in the type attribute of the top-level element. The host interface is configured by the source element


    <console type='pty' tty='/dev/pts/14'>
      <source path='/dev/pts/14'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.vmtools'/>
      <target type='virtio' name='com.inspur.ics.vmtools' state='disconnected'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0' state='disconnected'/>
      <alias name='channel1'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    
    // <input>进行交互的输入设备的配置
    // 这里的配置会让QEMU模拟PS2接口的鼠标和键盘， 还提供了tablet这种类型的设备，让光标可以在客户机获取绝对位置定位
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'>
      <alias name='input1'/>
    </input>
    <input type='keyboard' bus='ps2'>
      <alias name='input2'/>
    </input>
    输入设备：
输入设备允许使用图形化界面和虚拟机交互，当有图形化framebuffer的时候，输入设备会被自动提供的。
<input type='mouse' bus='ps2'/>
input元素：input元素含有一个强制的属性，type属性的值可以是mouse活tablet，前者使用想对运动，后者使用绝对运动。bus属性指定一个明确的设备类型，值可以是：xen、ps2、usb。

    // <graphics>是对连接到客户机的图形显示方式的配置
    <graphics type='sdl' display=':0.0'/>
    <graphics type='rdp' autoport='yes' multiUser='yes' />
    <graphics type='desktop' fullscreen='yes'/>
    <graphics type='spice'>
    <listen type='network' network='rednet'/>
    </graphics>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <graphics type='vnc' port='5905' autoport='yes' listen='0.0.0.0' keymap='en-us' sharePolicy='force-shared'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    graphics元素：graphics含有一个强制的属性type，type的值可以是：sdl、vnc、rdp、desktop。vnc则启动vnc 服务，port属性指定tcp端口，如果是-1，则表示自动分配，vnc的端口自动分配的话是从5900向上递增。listen属性提供一个IP地址给服 务器监听，可以单独在listen元素中设置。passwd属性提供一个vnc的密码。keymap属性提供一个keymap使用。
Rather than putting the address information used to set up the listening socket for graphics types vnc and spice in the <graphics> listen attribute, a separate subelement of <graphics>, called <listen> can be specified (see the examples above)since 0.9.4. <listen> accepts the following attributes:
listen元素：listen元素专门针对vnc和spice设置监听端口等。它包含以下属性：type、address、network。type的 值可以是address或network。如果设置了type=address，那么address属性设置一个ip地址或者主机名来监听。如果 type=network，则network属性设置一个网络名称在libvirt‘s的网络配置文件中。

字符设备提供同虚拟机进行交互的接口，Paravirtualized consoles, serial ports, parallel ports and channels 都是字符设备，它们使用相同的语法。
    
    
    <video>
      <model type='cirrus' vram='16384' heads='1' primary='yes'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <video>
      <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    video元素：是显卡配置，为了向后完全兼容，如果没有设置video但是有graphics在xml配置文件中，这时libvirt会按照 客户端类型增加一个默认的video，。model元素有一个强制的type属性，它的值可以是：vga、cirrus、vmvga、xen、vbox、 qxl. 例如一个客户端类型为kvm，那么默认的type值是cirrus. vram属性表示虚拟显卡的显存容量（单位为KB） ， heads属性表示显示屏幕的序号
    
    // <sound>标签表示的是声卡配置， 其中model属性表示为客户机模拟出来的声卡的类型， 其取值为es1370、 sb16、 ac97和ich6中的一个
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    
    // 内存的ballooning相关的配置包含在devices这个标签的memballoon子标签中, 该标签配置了该客户机的内存气球设备
    // 该配置将为客户机分配一个使用virtio-balloon驱动的设备, 以便实现客户机内存的ballooning调节. 该设备在客户机中的PCI设备编号为`0000:00:07.0`
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </memballoon>
    <panic model='isa'>
      <address type='isa' iobase='0x505'/>
    </panic>
  </devices>
  
  
  <seclabel type='none' model='none'/>
  <seclabel type='dynamic' model='dac' relabel='yes'>
    <label>+0:+0</label>
    <imagelabel>+0:+0</imagelabel>
  </seclabel>
</domain>
```

为所有虚拟机所需的根元素命名是domain. 它有两个属性，在 type指定用于运行域管理程序. 允许的值是特定于驱动程序的，但包括"xen"，"kvm"即使用kvm，"qemu"即完全虚拟化，"lxc"和"kqemu". 第二属性是id它是正在运行的guest机的唯一整数标识符, 但非活动计算机没有id值.

想在该xml中使用`qemu:commandline`, 需要设置namespace(即`<domain type='qemu' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>`) , 没有这个 XML 的 namespace， 这个参数 libvirt 不认.

`virsh domxml-from-native`提供一个方法将已存在的一组QEMU参数转成可以被libvirt使用Domain XML文件.
`virsh domain-to-natice`可以将libvirt的Domian XML文件转化成一组QEMU参数.

使用该xml:
```bash
# virsh define demo.xml # 将xml文件导入到libvirt虚拟机管理软件中
virsh start qemu-ubuntu # 启动虚拟机. 其中qemu-ubuntu为xml中定义的name(虚拟机别名)
```

### 如何快速大量创建vm
[镜像差量/磁盘差量技术](https://cloud.tencent.com/developer/article/1452081)

### `apt install qemu qemu-kvm qemu-system-x86`中的三者区别
在早先版本中有单独的qemu-kvm模块存在，结合qemu一起做虚拟机工作. qemu 1.3版本后QEMU和QEMU-KVM合二为一了, 因此当需要使用kvm特性时候，只需要增加参数`--enable-kvm`参数使能即可.

qemu-system-x86 是新版，qemu 是旧版本.