# 虚拟化
参考:
- [虚拟化的发展历程和实现方式](https://blog.csdn.net/jmilk/article/details/51031118)

虚拟化是一种**资源管理技术**, 趋势是KVM(kernel-based virtual machine)借助硬件辅助的虚拟化技术主要负责比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰.

提供虚拟化的软件层称为 Hypervisor或vmm(virtual machine monitor, 虚拟机监视器). VMM对物理资源的虚拟归纳为三个部分:CPU虚拟化、内存虚拟化、IO虚拟化.

分类方式:
- 软硬件实现Hypervisor

    - 软件虚拟化: bochs/qemu, 基于二进制翻译, 性能较弱
    - 硬件虚拟化(Hardware-Assisted Virtualization): Intel 的 VT-x 和 AMD 的 AMD-V, 及基于VT-x/AMD-V的kvm

        对于 Intel，可以查看 grep 'vmx' /proc/cpuinfo；对于 AMD，可以查看 grep 'svm' /proc/cpuinfo. 该选项一般在bois配置.

        > 也可使用`apt install cpu-checker`的kvm-ok命令查看

        当确认开始了标志位(表示当前是在虚拟机状态下，还是在真正的物理机内核下)之后，通过 KVM，GuestOS 的 CPU 指令不用经过 Qemu 转译，直接运行，大大提高了速度.

        [英特尔VT具体包括分别针对处理器、芯片组、网络的VT-X、VT-D和VT-C技术](https://yifengyou.gitbooks.io/learn-kvm/content/docs/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF.html):
        - 处理器：英特尔虚拟化技术（英特尔VT-x），包括英特尔虚拟化灵活迁移技术（Intel VT FlexMigration）、英特尔VT FlexPriority、英特尔VT 扩展页表（Extended Page Tables）
        - 芯片组：通过intel IOMMU支持直接 I/O 访问的 VT虚拟化技术（英特尔VT-d）
        - I/O设备：支持连接的英特尔虚拟化技术(英特尔VT-c）, 包括虚拟机设备队列（VMDq）、 虚拟机直接互连（VMDc, 基于SR-IOV生成VF设备直接分配给guest）
- 是否需要修改客户机内核

    - 半虚拟化 : 客户机需要更改系统内核/驱动才得以实现虚拟化, 比如virtio,xen
    - 全虚拟化(Full virtualization) : 不需要修改客户机内核, 虚拟化软件会模拟假的 CPU、内存、网络、硬盘等整套设备, 任何操作都经过虚拟化软件, 慢.

    > 半虚拟化的思想就是，修改操作系统内核，替换掉不能虚拟化的指令，通过超级调用（hypercall）直接和底层的虚拟化层hypervisor来通讯，hypervisor 同时也提供了超级调用接口来满足其他关键内核操作，比如内存管理、中断和时间保持.
- 虚拟层是直接位于硬件之上还是hostos上
    
    1. 主机级虚拟化
        1. Type 1类型 : 裸机架构虚拟化, 它是运行在服务器硬件之上, 比如阿里云的神龙架构(X-Dragon), 微软的Hyper-v、VMware vSphere的ESXi和Citrix的XenServer
        1. Type 2类型 : Hypervisor是运行在HostOS之上,  比如KVM, VMware Workstion, Oracle VM VirtualBox
    1. 容器级虚拟化, 比如docker

> xen已被主流的云厂商放弃, 均切换到了kvm.

kvm因为其独特的设计是横跨在Type 1和Type 2之间的, 是基于硬件虚拟化支持的全虚拟化实现, 它将kernel变成了hypervisor.

Linux查看机器是否虚拟机:
```sh
$ dmesg |grep -i virtual
[    0.029424] Booting paravirtualized kernel on KVM # 物理机是`Booting paravirtualized kernel on bare hardware`
[    0.660281] systemd[1]: Detected virtualization kvm.
# virt-what
# systemd-detect-virt
# cat /sys/class/dmi/id/sys_vendor
# dmidecode -t 1
```

虚拟机形式:
1. 进程
1. 模拟器: qemu, bochs
1. 高级语言虚拟机: jvm, python解析器

## 特权指令和敏感指令
特权指令是OS中直接管理关键系统资源的指令.
敏感指令是虚拟机中操作特权资源的指令.

特权指令都是敏感指令，但是敏感指令不一定是特权指令.

判断一个系统是否可虚拟化，核心就在于系统对敏感指令的支持. 如果所有的敏感指令都是特权指令，则可虚拟化.

处理器虚拟化是虚拟化关键，内存和IO虚拟化都依赖与处理器虚拟化. 访问内存和IO指令本身就是敏感指令.

# vmware
VMware vSphere 是 VMware 的虚拟化平台，可将数据中心转换为包括 CPU、存储和网络资源的聚合计算基础架构. vSphere 将这些基础架构作为一个统一的运行环境进行管理，并提供工具来管理加入该环境的数据中心.

![](/misc/img/virt/GUID-5EB66614-1EE8-4F39-8C8B-1E97EEE76791-high.png)
![](/misc/img/virt/2020-07-30_10-23-38.png)

### [VMware ESXi的体系结构](https://warmgrid.github.io/2019/04/09/the-architecture-of-vmware-esxi.html)
VMware ESXi体系结构包括底层操作系统（称为VMkernel）以及在其上运行的进程. VMkernel提供了运行系统上所有进程的方式，包括管理应用程序和代理以及虚拟机.

#### vmkernel
VMkernel是由VMware开发的类似POSIX的操作系统，提供与其他操作系统类似的某些功能，例如进程创建和控制，信号，文件系统和进程线. 它专门用于支持运行多个虚拟机，并提供以下核心功能：
- 资源调度
- I/O堆栈
- 设备驱动程序

# SR-IOV
SR-IOV是PCIE的功能, 可让os将一个设备识别为多个虚拟设备的功能. 在SR-IOV中, 将以往以物理方式存在的设备成为Physical Function(PF), 而通过SR-IOV添加的虚拟设备成为Virtual Function(VF).

查看host os是否存在支持SR-IOV的硬件: `lspci -vvvv|grep "SR-IOV"`, 比如intel 82599es网卡.
限制vf的带宽, 在host os执行: `ip link set etch0 vf <num> rate 100`, 100表示带宽是100Mbps.

# qemu
参考:
- [一文读懂 Qemu 模拟器](https://www.jianshu.com/p/db8c20aa6a69)
- [Qemu KVM(Kernel Virtual Machine)学习笔记](https://github.com/yifengyou/learn-kvm)
- [QEMU-KVM](https://abelsu7.top/2019/07/07/qemu-kvm-live-migration/)
- [QEMU KVM学习笔记](https://yifengyou.gitbooks.io/learn-kvm/content/)
- [qemu支持的platforms](https://wiki.qemu.org/Documentation/Platforms)
- [USING KVM VIRTUALIZATION ON ARM SYSTEMS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/appe-kvm_on_arm)

qemu是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备, 虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件.

因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 **KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化**，两者合作各自发挥自身的优势，相得益彰.

从本质上看，虚拟出的每个虚拟机对应 host 上的一个 Qemu 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应 Qemu 进程的一个线程. 下面通过一个虚拟机启动过程看看 Qemu 是如何与 KVM 交互的:
```c
// 第一步，获取到 KVM 句柄
kvmfd = open("/dev/kvm", O_RDWR);
// 第二步，创建虚拟机，获取到虚拟机句柄, 针对该文件句柄的loctl调用可以对虚拟机做相应的管理.
vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);
// 第三步，为虚拟机映射内存，还有其他的 PCI，信号处理的初始化. ioctl(kvmfd, KVM_SET_USER_MEMORY_REGION, &mem);
// 第四步，将虚拟机镜像映射到内存，相当于物理机的 boot 过程，把镜像映射到内存.
// 第五步，创建 vCPU(虚拟CPU)，并为 vCPU 分配内存空间.
ioctl(kvmfd, KVM_CREATE_VCPU, vcpuid);
vcpu->kvm_run_mmap_size = ioctl(kvm->dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0);
// 第五步，创建 vCPU 个数的线程并运行虚拟机.ioctl(kvm->vcpus->vcpu_fd, KVM_RUN, 0);
// 第六步，线程进入循环，并捕获虚拟机退出原因，做相应的处理.
for (;;) {  
  ioctl(KVM_RUN)  
  switch (exit_reason) {      
    case KVM_EXIT_IO:  /* ... */      
    case KVM_EXIT_HLT: /* ... */  
  }
}
// 这里的退出并不一定是虚拟机关机，
// 虚拟机如果遇到 I/O 操作，访问硬件设备，缺页中断等都会退出执行，
// 退出执行可以理解为将 CPU 执行上下文返回到 Qemu.
```

> vCPU本质是一个结构体. vCPU一般分成两个部分：VMCS(给硬件使用)+非VMCS(Virtual Machine Control Structure,给软件使用)虚拟机控制结构.

> VMM创建虚拟机，首先创建vCPU，然后由VMM调度运行. 虚拟机的运行，本质是VMM调用vCPU运行.

## Qemu 源码结构

Qemu 软件虚拟化实现的思路是采用二进制指令翻译技术，主要是提取 guest 代码，然后将其翻译成 TCG 中间代码，最后再将中间代码翻译成 host 指定架构的代码.

所以，从宏观上看，源码结构主要包含以下几个部分：
- /vl.c：最主要的模拟循环，虚拟机环境初始化，和 CPU 的执行
- /target-arch/translate.c：将 guest 代码翻译成不同架构的 TCG 操作码
- /tcg/tcg.c：主要的 TCG 代码
- /tcg/arch/tcg-target.c：将 TCG 代码转化生成主机代码
- /cpu-exec.c：主要寻找下一个二进制翻译代码块，如果没有找到就请求得到下一个代码块，并且操作生成的代码块

> 系统镜像可使用[Aliyun Linux 2 LTS(qcow2)](https://www.alibabacloud.com/help/zh/doc-detail/155430.htm).

# kvm
kvm(Kernel-based Virtual Machine), 是基于内核的虚拟化, 是仅支持硬件辅助的虚拟化, 是采用硬件虚拟化(intel vt/amd v)的全虚拟化解决方案. kvm对硬件的最低依赖是cpu硬件虚拟化支持.

![kvm和qemu架构](/misc/img/os/virt/xLEwiKumTSA5HvW.png)


一个kvm vm对应一个linux进程, 每个vCPU是该进程下的一个线程, 还有单独的io处理线程. 因此可通过linux的各种进程调度来实现不同vm的权限限定, 优先级等.

kvm vm看到的硬件是qemu模拟的(不包括VT-d透传的设备), 由qemu截获并转换为对实际设备的驱动操作.

![Xen, KVM, QEMU架构对比](/misc/img/os/virt/c5TkF1QHGyvS3dz.png)

相比于Xen架构，KVM架构有三大的优势：
1. 同等硬件资源环境下，KVM的性能表现更优
1. KVM架构天然的继承Linux内核更新迭代带来的系统优化，几乎不费力气，就完成了一次功能升级, 但对于Xen架构来说，每一次Xen Hypervisor内核或者Linux内核版本升级，Xen架构需要同步优化联调Xen Hypervisor内核和特权域(基于Linux的内核)，才能实现整个虚拟化内核的升级
1. 目前云厂商均使用kvm, 是趋势. 

![VirtIO](/misc/img/os/virt/WqYaSrQ3C8eEKuh.png)

## kvm生态组成
1. kvm内核模块: 主要负责cpu和内存的虚拟化

  由两部分组成:
  - kvm.ko: 处理器架构无关部分
  - kvm_xxx: 处理器相关部分, 比如intel的kvm_intel

  KVM的主要功能是初始化CPU硬件, 打开虚拟化模式, 然后将虚拟客户机运行在虚拟机模式下, 并对虚拟客户机的运行提供一定的支持.

  以intel举例, 在被kvm内核加载的时候, KVM模块会先初始化内部的数据结构; 做好准备之后, KVM模块检测系统当前的CPU, 然后打开CPU控制寄存器CR4中的虚拟化模式开关, 并通过执行VMXON指令将宿主操作系统（包括KVM模块本身） 置于CPU执行模式的虚拟化模式中的根模式； 最后， KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令(by ioctl). 接下来, 虚拟机的创建和运行将是一个用户空间的应用程序（QEMU） 和KVM模块相互配合的过程.

  针对虚拟处理器的最重要的loctl调用就是“执行虚拟处理器”. 通过它， 用户空间准备好的虚拟机在KVM模块的支持下， 被置于虚拟化模式中的非根模式下， 开始执行二进制指令. 在非根模式下， 所有敏感的二进制指令都会被处理器捕捉到， 处理器在保存现场之后自动切换到根模式, 由KVM决定如何进一步处理（要么由KVM模块直接处理， 要么返回用户空间交由用户空间程序处理）.

  除了处理器的虚拟化， 内存虚拟化也是由KVM模块实现的, 比如intel ept.

  处理器对设备的访问主要是通过I/O指令和MMIO, 其中I/O指令会被处理器直接截获， MMIO会通过配置内存虚拟化来捕捉, 但是， 外设的模拟一般不由KVM模块而是qemu负责. 一般来说, 只有对性能要求比较高的虚拟设备才会由KVM内核模块来直接负责， 比如虚拟中断控制器和虚拟时钟， 这样可以大量减少处理器模式切换的开销.

2. qemu用户态工具: 提供设备模拟功能

  每一个虚拟客户机在宿主机中就体现为一个QEMU进程, 而客户机的每一个vCPU就是一个QEMU线程. 虚拟机运行期间, QEMU会通过KVM模块提供的系统调用进入内核, 由KVM模块负责将虚拟机置于处理器的特殊模式下运行. 遇到虚拟机进行I/O操作时， KVM模块会从上次的系统调用出口处返回QEMU, QEMU来负责解析和模拟这些设备.

  QEMU还提供了叫作virtio-blk-data-plane的一种高性能的块设备I/O方式， 它最初在QEMU 1.4版本中被引入. virtio-blk-data-plane与传统virtioblk相比， 它为每个块设备单独分配一个线程用于I/O处理, data-plane线程不需要与原QEMU执行线程同步和竞争锁， 而且它使用ioeventfd/irqfd机制， 同时利用宿主机Linux上的AIO（异步I/O） 来处理客户机的I/O请求， 使得块设备I/O效率进一步提高.

## kvm特性
1. 内存管理

    vm的"物理内存"就是对应宿主机进程的虚拟内存.
    vm自身内存访问落实到真实的宿主机的物理内存的机制叫影子页表(shadow page table), kvm Hypervisor为每个vm准备一份影子页表, 与vm自身页表建立一一对应关系. 因为性能原因, 目前影子页表的映射转换(`GPA->HPA`)已由硬件完成(intel ept/amd npt), 默认开启.

    > - kvm内存映射转换: GVA(guest virtual address, 客户机虚拟地址)->GPA(guest physical address)->HVA(host virtual address, 宿主机虚拟地址)->HPA(host physical address).

    > - vm自身页表描述的是GVA->GPA, 通过CR3寄存器完成; 影子页表描述的是GPA->HPA.
1. kvm image格式

    确切的说是qemu的功能.
    kvm的image格式是qcow2, 支持稀疏文件, 快照(包括多级快照), 压缩和加密.
1. 实时迁移

    在宿主机间迁移vm而不中断服务.
1. 设备驱动程序

    - 支持半虚拟化的驱动程序(virtio标准)来提高io性能.
    - 支持intel VT-d, 通过将宿主机的pci总线上的设备透传(pass-through)给vm.
1. 性能和可伸缩性

    继承了linux的性能和可伸缩性

## kvm调优
- cpu调优

  vm对物理机cpu逻辑核的手工绑定, 需要了解NUMA

  步骤:
  1. `numactl --hardware`, 查看硬件配置
  1. `numastat -c qemu-system-x86_64`, 查看相关进程的numa内存统计
  1. `virsh numatune`修改vm的numa配置

    <vcpu>和<numatune>需保持一致, <numatune>配置的是物理cpu, <vcpu>配置的是逻辑cpu(包括超线程产生的核). <numatune>使用static模式时<nodeset>也必须是.

- 内存调优

  KSM, 即相同内存页合并, 内存气球技术及大页内存的使用

## Libvirt
Libvirt是由Redhat开发的一套开源的软件工具，目标是提供一个通用和稳定的软件库来高效、安全地管理一个节点上的虚拟机，并支持远程操作. libvirt 已经成为使用最为广泛的对各种虚拟机进行管理的工具和应用程序接口（API），而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口.

基于可移植性和高可靠性的考虑，Libvirt采用C语言开发，但是也提供了对其他编程语言的绑定，包括Python、Perl、OCaml、Ruby、Java和PHP. 同时Libvirt支持多种VMM，包括LXC、KVM/QEMU、Xen、VirtualBox等.

为了支持多种VMM，Libvirt采用了基于Driver的架构(中间人)，每种VMM需要提供一个驱动和Libvirt进行通信来操控特定的VMM. 在初始化过程中，所有的驱动被枚举和注册. 每一个驱动都会加载特定的函数为Libvirt API调用.

### Libvirt API
Libvirt定义了各种各样的API，涉及虚拟化的方方面面，主要分为以下几类:
- 虚拟机快照：快照是包括内存、硬盘等信息在内的完整虚拟机状态。这些API就是用于创建、删除和恢复快照的。
- 虚拟机管理：这一类API用于管理虚拟机，也是Libvirt里面使用最频繁的功能。例如：创建、销毁、重启、迁移虚拟机，以及操作虚拟机的磁盘镜像等。
- 事件：事件是Libvirt定义的一套监测特定情况发生的机制，用户可以通过相应的API告诉Libvirt，想要监测什么样的事件，或者事件发生时采取什么样的操作。
- 存储管理

  Libvirt在任何运行了Libvirt daemon的主机端通过管理存储池和卷来为虚拟机提供存储资源，可以支持包括本地文件系统、网络文件系统、iSCSI、LVM等多种后端存储系统.
  虚拟机磁盘格式上支持qcow2、vmdk、raw等格式.

- 宿主机：用于获取宿主机的各种信息，包括机器名、CPU状态等，也用于和特定的VMM建立连接。
- 网络接口：实现网络接口的相应操作，如定义一个新的网络接口

  支持Linux桥、vlan、多网卡绑定管理，比较新的版本还支持open vswitch. libvirt还支持nat和路由方式的网络，可以通过防火墙让虚拟机通过宿主机建立网络通道和外部网络进行通信.
- 错误管理：提供了Libvirt本身的错误管理机制，比如获取最近一次的Libvirt错误。
- 其他设备管理：包括对网络、PCI、USB等设备的管理。

### Libvirt存储池和存储卷
为了提供统一的接口给虚拟机来访问不同的后端存储设备，Libvirt将存储管理分为两个方面：存储卷和存储池. 存储卷可以作为存储设备分配给虚拟机使用，物理上可以是一个虚拟机磁盘文件或一个真实的磁盘分区. 存储池可以被理解为本地目录，或者通过各类分布式存储系统分配过来的目录等，存储卷可以从存储池中生成，Libvirt可以支持多种存储池类型，内容如下:
- 目录池（Directory Pool）：以主机的一个目录作为存储池
- 本地文件系统池（Filesystem Pool）：使用主机已经格式化好的块设备作为存储池，支持的文件系统类型包括ext4、XFS等
- 网络文件系统池（Network Filesystem Pool）：使用远端网络文件系统服务器的导出目录作为存储池
- 逻辑卷池（Logical Volume Pool）：使用已经创建好的LVM卷组，或者基于一系列生成卷组的源设备生成卷组，生成存储池
- 磁盘卷池（Disk Pool）：使用磁盘作为存储池
- iSCSI卷池（iSCSI Pool）：使用iSCSI设备作为存储池
- SCSI卷池（SCSI Pool）：使用SCSI设备作为存储池
- 多路设备池（Multipath Pool）：使用多路设备作为存储池
- RBD Pool：包括一个RADOS池的所有RBD image

Libvirt中的存储管理独立于虚拟机的管理，存储卷从存储池中被划分出来，存储卷分配给虚拟机成为可用的存储设备. 通过virsh工具的pool命令可以查看、创建、激活、注册、删除存储池，因此创建存储资源时，并不需要有虚拟机的存在.

### libvirt cmds
参考:
- [virt-install和virsh详解](https://yq.aliyun.com/articles/529107)

```bash
# apt install libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager
# lsmod | grep -i kvm
# sudo systemctl is-active libvirtd
# usermod -aG libvirt $USER
# usermod -aG kvm $USER
# brctl show # a virtual bridge “virbr0” is created automatically, but this is only used for testing purpose.
# cat << EOF > /etc/netplan/00-installer-config.yaml # To create a network bridge, access the guests from outside the local network
# This is the network config written by 'subiquity'
network:
  ethernets:
    enp0s3:
      dhcp4: no
      dhcp6: no
  version: 2
  bridges:
    br0:
      interfaces: [enp0s3]
      addresses: [172.20.10.9/28]
      gateway4: 172.20.10.1
      nameservers:
        addresses: [4.2.2.2, 8.8.8.8]
EOF
# netplan apply # activate the bride br0
# networkctl status br0 # verify the status of br0 bridge
# ip a s
```

- libvirt-daemon-system : configuration files to run the libvirt daemon as a system service.
- libvirt-clients : software for managing virtualization platforms.
- bridge-utils : a set of command-line tools for configuring ethernet bridges.
- virtinst : a set of command-line tools for creating virtual machines.
- virt-manager : an easy-to-use GUI interface and supporting command-line utilities for managing virtual machines through libvirt.

#### virt-install
通过命令行创建kvm vm的工具, 最终生成一个xml格式的vm配置文件.

```bash
# virt-install --name=testvm --rame=2048 --vCPUs=4 --os-type=linux --hvm --cdrom=/xxx.iso --file=/xxx.img --file-size=10 --bridge=br0 --vnc --vncport=5920
```

解析:
- --bridge=xxx: 连接到名为xxx的网桥上
- --cdrom : 使用cdrom iso安装系统
- --file : vm硬盘文件路径
- --file-size : vm硬盘文件大小, 单位GB
- -p/--paravirt : 指定使用半虚拟化
- -v/--hvm : 指定使用全虚拟化
- --virt-type：使用的hypervisor，如kvm、qemu、xen等；所有可用值可以使用`virsh capabilities`命令获取
- --vnc : 开启vnc支持
- --vncport : 配置vnc端口

## FAQ
### 虚拟机XML格式
```xml
<domain type='kvm' id='29'>
//domain 是一个所有虚拟机都需要的根元素，它有两个属性，
//type定义使用哪个虚拟机管理程序，值可以是：xen、kvm、qemu、lxc、kqemu，
//第二个参数是id，它唯一的标示一个运行的虚拟机，不活跃的客户端没有id。
  <name>i-000039</name>
//name参数为虚拟机定义了一个简短的名字，必须唯一

  <uuid>d59b03ce-2e78-4d35-b731-09d9ca9653af</uuid>
//uid为虚拟机定义了一个全球唯一的标示符，uuid的格式必须遵循RFC 4122指定的格式，当创建虚拟机没有指定uuid时会随机的生成一个uuid. 自己可用命令行工具 uuidgen生成.

  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
  <memtune>
    <hard_limit unit='KiB'>4194304</hard_limit>
  </memtune>
    //memory 定义客户端启动时可以分配到的最大内存，内存单位由unit定义，单位可以是：K、KiB、M、MiB、G、GiB、T、TiB。默认是KiB。
  
  
  <vcpu placement='static'>1</vcpu>
  //vcpu的内容是为虚拟机最多分配几个cpu，值处于1~maxcpu之间，可选参数：cpuset参数指定虚拟cpu可以映射到那些物理cpu上，物理 cpu用逗号分开，单个数字的标示单个cpu，
  //也可以用range符号标示多个cpu，数字前面的脱字符标示排除这个cpu，current参数指定虚拟 机最少，placement参数指定一个domain的cpu的分配模式，值可以是static、auto。
  
  
  <cputune>
    <shares>1024</shares>
    <period>100000</period>
    <quota>-1</quota>
  </cputune>
  <resource>
    <partition>/machine</partition>
  </resource>
  
  
 //操作系统启动介绍
  <os>
    <type arch='x86_64' machine='pc-i440fx-2.8'>hvm</type>
    //type参数指定了虚拟机操作系统的类型，内容：hvm表明该OS被设计为直接运行在裸金属上面，需要全虚拟化, **常用**.
    //而linux(一个不好的名字)指支持Xen 3管理程序来宾ABI的操作系统，
    //type同样有两个可选参数：arch指定虚拟机的CPU构架，machine指定机器的类型.
    //<boot dev='hd'/>dev属性的值可以是：fd、hd、cdrom、network，它经常被用来指定下一次启动。boot的元素可以被设置多个用来建立一个启动优先规则.
    <bootmenu enable='yes' timeout='0'/>
    <bios useserial='yes'/>
  </os>
  
  Hypervisor的特性
  <features>
    <acpi/>
    <apic/>
    <!-- <pae/> -->
  </features>
  Hypervisors允许特定的CPU/机器特性打开或关闭，所有的特性都在fearures元素中，以下介绍一些在全虚拟化中常用的标记：
pae：扩展物理地址模式，使32位的客户端支持大于4GB的内存
acpi：用于电源管理
hap：Enable use of Hardware Assisted Paging if available in the hardware.
  
  
  
  //cpu分配
  <cpu>
    <topology sockets='1' cores='1' threads='1'/>
    <numa>
      <cell id='0' cpus='0' memory='4194304' unit='KiB'/>
    </numa>
  </cpu>
  
  时间设置
  <clock offset='variable' adjustment='0' basis='utc'>
    <timer name='rtc' track='guest'/>
  </clock>
  客户端的时间初始化来自宿主机的时间，大多数操作系统期望硬件时钟保持UTC格式，UTC也是默认格式，然而Windows机器却期望它是’localtime’
clock的offset属性支持四种格式的时间：UTC localtime timezone variable
UTC：当引导时客户端时钟同步到UTC时钟
localtime：当引导时客户端时钟同步到主机时钟所在的时区
timezone：The guest clock will be synchronized to the requested timezone using the timezone attribute.

  
  //控制周期：
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>preserve</on_crash>
  <on_lockfailure>poweroff</on_lockfailure>
  //当一个客户端的OS触发lifecycle时，它将采取新动作覆盖默认操作，具体状态参数如下：
 //on_poweroff：当客户端请求poweroff时执行特定的动作
  //on_reboot：当客户端请求reboot时执行特定的动作
  // on_crash：当客户端崩溃时执行的动作
   //每种状态下可以允许指定如下四种行为：
    //destory：domain将会被完全终止，domain的所有资源会被释放
    //restart：domain会被终止，然后以相同的配置重新启动
    //preserver：domain会被终止，它的资源会被保留用来分析
    //rename-restart：domain会被终止，然后以一个新名字被重新启动
  
  
  <devices>
  //所有的设备都是一个名为devices元素的子设备(All devices occur as children of the main devices element.)，以下是一个简单的配置：
//<emulator>/usr/bin/kvm</emulator>
//emulator元素指定模拟设备二进制文件的全路径
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='cdrom'>
      <backingStore/>
      <target dev='hdd' bus='ide'/>
      <readonly/>
      <boot order='2'/>
      <alias name='ide0-1-1'/>
      <address type='drive' controller='0' bus='1' target='0' unit='1'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/ae386362-eed6-43a8-b5a8-11a42fabc0ed'/>
      <backingStore/>
      <target dev='vda' bus='virtio'/>
      <boot order='1'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'/>
    </disk>
    <controller type='usb' index='0' model='ich9-ehci1'>
      <alias name='usb'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='pci-ohci'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </controller>
    <controller type='ide' index='0'>
      <alias name='ide'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'>
      <alias name='pci.0'/>
    </controller>
    <controller type='pci' index='1' model='pci-bridge'>
      <model name='pci-bridge'/>
      <target chassisNr='1'/>
      <alias name='pci.1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <lease>
      <lockspace>6ee684f1-8b25-4f0a-9721-fe96540c1870</lockspace>
      <key>ae386362-eed6-43a8-b5a8-11a42fabc0ed</key>
      <target path='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/.6ee684f1-8b25-4f0a-9721-fe96540c1870/.leases' offset='4194304'/>
    </lease>
    
    
    网络接口：
有好几种网络接口访问客户端:Virtual network、Bridge to LAN、Userspace SLIRP stack、Generic ethernet connection、Direct attachment to physical interface。
Virtual network：这种推荐配置一般是对使用动态/无线网络环境访问客户端的情况。
Bridge to LAN：这种推荐配置一般是使用静态有限网络连接客户端的情况。
    <interface type='bridge'>
      <mac address='00:16:3e:bd:8e:f3'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='1fb6d3e0-c0c0-4fdd-9edb-c64b1327763f'/>
      </virtualport>
      <target dev='vnd59b03ce0'/>
      <model type='virtio'/>
      <boot order='3'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x10' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:59:09:2d'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f8a225e1-028f-4396-8107-879f5b77152c'/>
      </virtualport>
      <target dev='vnd59b03ce1'/>
      <model type='rtl8139'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x11' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:97:5b:c0'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f688fbc6-c4e3-4348-9408-12dc903dab06'/>
      </virtualport>
      <target dev='vnd59b03ce2'/>
      <model type='virtio'/>
      <alias name='net2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x12' function='0x0'/>
    </interface>
    
    //串行端口
    <serial type='pty'>
      <source path='/dev/pts/14'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    在每组指令中，最顶层的指令(parallel, serial, console, channel)描述设备怎样出现在客户端中，客户端接口通过target配置。
The interface presented to the host is given in the type attribute of the top-level element. The host interface is configured by the source element


    <console type='pty' tty='/dev/pts/14'>
      <source path='/dev/pts/14'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.vmtools'/>
      <target type='virtio' name='com.inspur.ics.vmtools' state='disconnected'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0' state='disconnected'/>
      <alias name='channel1'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    
    
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'>
      <alias name='input1'/>
    </input>
    <input type='keyboard' bus='ps2'>
      <alias name='input2'/>
    </input>
    输入设备：
输入设备允许使用图形化界面和虚拟机交互，当有图形化framebuffer的时候，输入设备会被自动提供的。
<input type='mouse' bus='ps2'/>
input元素：input元素含有一个强制的属性，type属性的值可以是mouse活tablet，前者使用想对运动，后者使用绝对运动。bus属性指定一个明确的设备类型，值可以是：xen、ps2、usb。

    
    <graphics type='vnc' port='5905' autoport='yes' listen='0.0.0.0' keymap='en-us' sharePolicy='force-shared'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    graphics元素：graphics含有一个强制的属性type，type的值可以是：sdl、vnc、rdp、desktop。vnc则启动vnc 服务，port属性指定tcp端口，如果是-1，则表示自动分配，vnc的端口自动分配的话是从5900向上递增。listen属性提供一个IP地址给服 务器监听，可以单独在listen元素中设置。passwd属性提供一个vnc的密码。keymap属性提供一个keymap使用。
Rather than putting the address information used to set up the listening socket for graphics types vnc and spice in the <graphics> listen attribute, a separate subelement of <graphics>, called <listen> can be specified (see the examples above)since 0.9.4. <listen> accepts the following attributes:
listen元素：listen元素专门针对vnc和spice设置监听端口等。它包含以下属性：type、address、network。type的 值可以是address或network。如果设置了type=address，那么address属性设置一个ip地址或者主机名来监听。如果 type=network，则network属性设置一个网络名称在libvirt‘s的网络配置文件中。

字符设备提供同虚拟机进行交互的接口，Paravirtualized consoles, serial ports, parallel ports and channels 都是字符设备，它们使用相同的语法。
    
    
    <video>
      <model type='cirrus' vram='16384' heads='1' primary='yes'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    video元素：是描述声音设备的容器，为了向后完全兼容，如果没有设置video但是有graphics在xml配置文件中，这时libvirt会按照 客户端类型增加一个默认的video，。model元素有一个强制的type属性，它的值可以是：vga、cirrus、vmvga、xen、vbox、 qxl。例如一个客户端类型为kvm，那么默认的type值是cirrus。
    
    
    
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </memballoon>
    <panic model='isa'>
      <address type='isa' iobase='0x505'/>
    </panic>
  </devices>
  
  
  <seclabel type='none' model='none'/>
  <seclabel type='dynamic' model='dac' relabel='yes'>
    <label>+0:+0</label>
    <imagelabel>+0:+0</imagelabel>
  </seclabel>
</domain>
```

为所有虚拟机所需的根元素命名是domain. 它有两个属性，在 type指定用于运行域管理程序. 允许的值是特定于驱动程序的，但包括"xen"，"kvm"即使用kvm，"qemu"即完全虚拟化，"lxc"和"kqemu". 第二属性是id它是正在运行的guest机的唯一整数标识符, 但非活动计算机没有id值.

想在该xml中使用`qemu:commandline`, 需要设置namespace(即`<domain type='qemu' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>`) , 没有这个 XML 的 namespace， 这个参数 libvirt 不认.

`virsh domxml-from-native`提供一个方法将已存在的一组QEMU参数转成可以被libvirt使用Domain XML文件.
`virsh domain-to-natice`可以将libvirt的Domian XML文件转化成一组QEMU参数.

使用该xml:
```bash
# virsh define demo.xml # 将xml文件导入到libvirt虚拟机管理软件中
virsh start qemu-ubuntu # 启动虚拟机. 其中qemu-ubuntu为xml中定义的name(虚拟机别名)
```

### 如何快速大量创建vm
[镜像差量/磁盘差量技术](https://cloud.tencent.com/developer/article/1452081)

### `apt install qemu qemu-kvm qemu-system-x86`中的三者区别
在早先版本中有单独的qemu-kvm模块存在，结合qemu一起做虚拟机工作. qemu 1.3版本后QEMU和QEMU-KVM合二为一了, 因此当需要使用kvm特性时候，只需要增加参数`--enable-kvm`参数使能即可.

qemu-system-x86 是新版，qemu 是旧版本.