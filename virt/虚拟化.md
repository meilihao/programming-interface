# 虚拟化
参考:
- [虚拟化的发展历程和实现方式](https://blog.csdn.net/jmilk/article/details/51031118)

虚拟化是对资源的逻辑抽象、隔离、再分配、管理的一个过程，通常对虚拟化的理解有广义狭义之分。广义包括平台虚拟化、应用程序虚拟化、存储虚拟化、网络虚拟化、设备虚拟化等等。狭义的虚拟化专门指计算机上模拟运行多个操作系统平台.

虚拟化是一种**资源管理技术**, 趋势是KVM(kernel-based virtual machine)借助硬件辅助的虚拟化技术主要负责比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化，两者合作各自发挥自身的优势，相得益彰.

提供虚拟化的软件层称为 Hypervisor或vmm(virtual machine monitor, 虚拟机监视器). VMM对物理资源的虚拟归纳为三个部分:CPU虚拟化、内存虚拟化、IO虚拟化.

分类方式:
- 软硬件实现Hypervisor

    - 软件虚拟化: bochs/qemu, 基于二进制翻译, 性能较弱, 理论上可模拟任何硬件

      在硬件虚拟化未出现之前，软件虚拟化提供了两种技术方案：“二进制代码动态翻译技术”和“修改客户及OS”，分别对应全虚拟化和半虚拟化.这两种技术方案的本质都是为了“捕获并重定向”
    - 硬件虚拟化(Hardware-Assisted Virtualization): Intel 的 VT-x 和 AMD 的 AMD-V, 及基于VT-x/AMD-V的kvm

        对于 Intel，可以查看 grep 'vmx' /proc/cpuinfo；对于 AMD，可以查看 grep 'svm' /proc/cpuinfo. 该选项一般在bois配置.

        > 也可使用`apt install cpu-checker`的kvm-ok命令查看

        当确认开始了标志位(表示当前是在虚拟机状态下，还是在真正的物理机内核下)之后，通过 KVM，GuestOS 的 CPU 指令不用经过 Qemu 转译，直接运行，大大提高了速度.

        [英特尔VT具体包括分别针对处理器、芯片组、网络的VT-X、VT-D和VT-C技术](https://github.com/yifengyou/learn-kvm/blob/master/docs/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/Intel%E7%A1%AC%E4%BB%B6%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF.md):
        - 处理器：英特尔虚拟化技术（英特尔VT-x），包括英特尔虚拟化灵活迁移技术（Intel VT FlexMigration）、英特尔VT FlexPriority、英特尔VT 扩展页表（Extended Page Tables）
        - 芯片组：通过intel IOMMU支持直接 I/O 访问的 VT虚拟化技术（英特尔VT-d）
        - I/O设备：英特尔网络虚拟化技术(英特尔VT-c）, 包括虚拟机设备队列（VMDq）、 虚拟机直接互连（VMDc, 基于SR-IOV生成VF设备直接分配给guest）
- 是否需要修改客户机内核

    - 半虚拟化 : 客户机需要更改系统内核/驱动才得以实现虚拟化, 比如virtio,xen
    - 全虚拟化(Full virtualization) : 不需要修改客户机内核, 虚拟化软件会模拟假的CPU、内存、网络、硬盘等整套设备, 任何操作都经过虚拟化软件, 慢.

    完全虚拟化技术又叫硬件辅助虚拟化技术.

    > 半虚拟化的思想就是，修改操作系统内核，替换掉不能虚拟化的指令，通过超级调用（hypercall）直接和底层的虚拟化层hypervisor来通讯，hypervisor 同时也提供了超级调用接口来满足其他关键内核操作，比如内存管理、中断和时间保持.
- 虚拟层是直接位于硬件之上还是hostos上
    
    1. 主机级虚拟化
        1. Type 1类型 : 裸机架构虚拟化, 它是运行在服务器硬件之上, 比如阿里云的神龙架构(X-Dragon), 微软的Hyper-v、VMware vSphere的ESXi和Citrix的XenServer

          VMM被看做是一个完备的操作系统，同时还具备虚拟化功能，VMM直接管理所有的物理资源，包括处理器，内存和I/O设备等.
        1. Type 2类型 : Hypervisor是运行在HostOS之上,  比如KVM, VMware Workstion, Oracle VM VirtualBox

          宿主机操作系统是传统的操作系统，如Linux，Windows等，宿主机操作系统不提供虚拟化能力，提供虚拟化能力的VMM作为系统的一个驱动或者软件运行在宿主操作系统上，VMM通过调用host OS的服务获得资源，实现处理器，内存和I/O设备的模拟

        > 其实kvm是融合了Type1和Type2的.
    1. 容器级虚拟化, 比如docker
      基于cgroups, namespace的共享kernel的进程隔离解决方案

> xen已被主流的云厂商放弃, 均切换到了kvm.

kvm因为其独特的设计是横跨在Type 1和Type 2之间的, 是基于硬件虚拟化支持的全虚拟化实现, 它将kernel变成了hypervisor.

Linux查看机器是否虚拟机:
```sh
$ dmesg |grep -i virtual
[    0.029424] Booting paravirtualized kernel on KVM # 物理机是`Booting paravirtualized kernel on bare hardware`
[    0.660281] systemd[1]: Detected virtualization kvm.
# virt-what
# systemd-detect-virt
# cat /sys/class/dmi/id/sys_vendor
# dmidecode -t 1
```

虚拟机形式:
1. 进程
1. 模拟器: qemu, bochs
1. 高级语言虚拟机: jvm, python解析器

## 特权指令和敏感指令
特权指令是OS中直接管理关键系统资源的指令.
敏感指令是虚拟机中操作特权资源的指令.

特权指令都是敏感指令，但是敏感指令不一定是特权指令.

“陷入再模拟”其实就是VMM捕获所有敏感指令，VMM模拟执行引起异常的敏感指令.

判断一个系统是否可虚拟化，核心就在于系统对敏感指令的支持. 如果所有的敏感指令都是特权指令，则可虚拟化.

cpu虚拟化是虚拟化关键，内存和IO虚拟化都依赖与处理器虚拟化. 访问内存和IO指令本身就是敏感指令.

x86有4种特权模式:
- ring 0 : os
- ring 1/2 : driver
- ring 3 : 应用
x86虚拟化的一个难点是如何将vm越级的指令使用进行隔离.

> Intel ME是在芯片组里的, 有高于ring 0的优先级

# vmware
VMware vSphere 是 VMware 的虚拟化平台，可将数据中心转换为包括 CPU、存储和网络资源的聚合计算基础架构. vSphere 将这些基础架构作为一个统一的运行环境进行管理，并提供工具来管理加入该环境的数据中心.

![](/misc/img/virt/GUID-5EB66614-1EE8-4F39-8C8B-1E97EEE76791-high.png)
![](/misc/img/virt/2020-07-30_10-23-38.png)

### [VMware ESXi的体系结构](https://warmgrid.github.io/2019/04/09/the-architecture-of-vmware-esxi.html)
VMware ESXi体系结构包括底层操作系统（称为VMkernel）以及在其上运行的进程. VMkernel提供了运行系统上所有进程的方式，包括管理应用程序和代理以及虚拟机.

#### vmkernel
VMkernel是由VMware开发的类似POSIX的操作系统，提供与其他操作系统类似的某些功能，例如进程创建和控制，信号，文件系统和进程线. 它专门用于支持运行多个虚拟机，并提供以下核心功能：
- 资源调度
- I/O堆栈
- 设备驱动程序

# SR-IOV
ref:
-[什么是SR-IOV？先用起来再说！](https://www.wupeng28.top/?p=164)

**[Intel Scalable IOV已计划取代SR-IOV - Intel Scalable IOV介绍及应用实例](https://mp.weixin.qq.com/s/u76IZhPSFdVtGt1ar5DR5g)**

SR-IOV是PCIE的功能, 可让os将一个设备识别为多个虚拟设备的功能. 在SR-IOV中, 将以往以物理方式存在的设备成为Physical Function(PF), 而通过SR-IOV添加的虚拟设备成为Virtual Function(VF).

查看host os是否存在支持SR-IOV的硬件: `lspci -vvvv|grep "SR-IOV"`, 比如intel 82599es网卡.
限制vf的带宽, 在host os执行: `ip link set etch0 vf <num> rate 100`, 100表示带宽是100Mbps.

SR-IOV无法支持热迁移, 因为SR-IOV是借助了硬件虚拟化技术，即Intel的VT等直接将硬件设备透传给VM, vmm无法感知设备状态.

> 热迁移（Live Migration），又叫动态迁移、实时迁移，即虚拟机保存/恢复，通常是将整个虚拟机的运行状态完整保存下来，同时可以快速的恢复到原有硬件平台甚至是不同硬件平台上。 恢复以后，虚拟机仍旧平滑运行，用户不会察觉到任何差异，中断时间可到ms级，是目前云厂商提供的一项重要服务.

# qemu
参考:
- [一文读懂 Qemu 模拟器](https://www.jianshu.com/p/db8c20aa6a69)
- [Qemu KVM(Kernel Virtual Machine)学习笔记](https://github.com/yifengyou/learn-kvm)
- [QEMU-KVM](https://abelsu7.top/2019/07/07/qemu-kvm-live-migration/)
- [QEMU KVM学习笔记](https://yifengyou.gitbooks.io/learn-kvm/content/)
- [qemu支持的platforms](https://wiki.qemu.org/Documentation/Platforms)
- [USING KVM VIRTUALIZATION ON ARM SYSTEMS](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/appe-kvm_on_arm)

qemu是纯软件实现的虚拟化模拟器，几乎可以模拟任何硬件设备, 虚拟机认为自己和硬件打交道，但其实是和 Qemu 模拟出来的硬件打交道，Qemu 将这些指令转译给真正的硬件.

因为 Qemu 是纯软件实现的，所有的指令都要经 Qemu 过一手，性能非常低，所以，在生产环境中，大多数的做法都是配合 KVM 来完成虚拟化工作，因为 **KVM 是硬件辅助的虚拟化技术，主要负责 比较繁琐的 CPU 和内存虚拟化，而 Qemu 则负责 I/O 虚拟化**，两者合作各自发挥自身的优势，相得益彰.

从本质上看，虚拟出的每个虚拟机对应 host 上的一个 Qemu 进程，而虚拟机的执行线程（如 CPU 线程、I/O 线程等）对应 Qemu 进程的一个线程. 下面通过一个虚拟机启动过程看看 Qemu 是如何与 KVM 交互的:
```c
// 第一步，获取到 KVM 句柄
kvmfd = open("/dev/kvm", O_RDWR);
// 第二步，创建虚拟机，获取到虚拟机句柄, 针对该文件句柄的loctl调用可以对虚拟机做相应的管理.
vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0);
// 第三步，为虚拟机映射内存，还有其他的 PCI，信号处理的初始化. ioctl(kvmfd, KVM_SET_USER_MEMORY_REGION, &mem);
// 第四步，将虚拟机镜像映射到内存，相当于物理机的 boot 过程，把镜像映射到内存.
// 第五步，创建 vCPU(虚拟CPU)，并为 vCPU 分配内存空间.
ioctl(kvmfd, KVM_CREATE_VCPU, vcpuid);
vcpu->kvm_run_mmap_size = ioctl(kvm->dev_fd, KVM_GET_VCPU_MMAP_SIZE, 0);
// 第五步，创建 vCPU 个数的线程并运行虚拟机.ioctl(kvm->vcpus->vcpu_fd, KVM_RUN, 0);
// 第六步，线程进入循环，并捕获虚拟机退出原因，做相应的处理.
for (;;) {  
  ioctl(KVM_RUN)  
  switch (exit_reason) {      
    case KVM_EXIT_IO:  /* ... */      
    case KVM_EXIT_HLT: /* ... */  
  }
}
// 这里的退出并不一定是虚拟机关机，
// 虚拟机如果遇到 I/O 操作，访问硬件设备，缺页中断等都会退出执行，
// 退出执行可以理解为将 CPU 执行上下文返回到 Qemu.
```

> vCPU本质是一个结构体. vCPU一般分成两个部分：VMCS(给硬件使用)+非VMCS(Virtual Machine Control Structure,给软件使用)虚拟机控制结构.

> VMM创建虚拟机，首先创建vCPU，然后由VMM调度运行. 虚拟机的运行，本质是VMM调用vCPU运行.

## Qemu 源码结构

Qemu 软件虚拟化实现的思路是采用二进制指令翻译技术，主要是提取 guest 代码，然后将其翻译成 TCG 中间代码，最后再将中间代码翻译成 host 指定架构的代码.

所以，从宏观上看，源码结构主要包含以下几个部分：
- /vl.c：最主要的模拟循环，虚拟机环境初始化，和 CPU 的执行
- /target-arch/translate.c：将 guest 代码翻译成不同架构的 TCG 操作码
- /tcg/tcg.c：主要的 TCG 代码
- /tcg/arch/tcg-target.c：将 TCG 代码转化生成主机代码
- /cpu-exec.c：主要寻找下一个二进制翻译代码块，如果没有找到就请求得到下一个代码块，并且操作生成的代码块

> 系统镜像可使用[Aliyun Linux 2 LTS(qcow2)](https://www.alibabacloud.com/help/zh/doc-detail/155430.htm).

# kvm
kvm(Kernel-based Virtual Machine), 是基于内核的虚拟化, 是仅支持硬件辅助的虚拟化, 是采用硬件虚拟化(intel vt/amd v)的全虚拟化解决方案. kvm对硬件的最低依赖是cpu硬件虚拟化支持.

![kvm和qemu架构](/misc/img/os/virt/xLEwiKumTSA5HvW.png)


一个kvm vm对应一个linux进程, 每个vCPU是该进程下的一个线程, 还有单独的io处理线程. 因此可通过linux的各种进程调度来实现不同vm的权限限定, 优先级等.

kvm vm看到的硬件是qemu模拟的(不包括VT-d透传的设备), 由qemu截获并转换为对实际设备的驱动操作.

vm=用户空间的qemu, 模拟各种设备+kvm负责cpu和memory的虚拟化.

![Xen, KVM, QEMU架构对比](/misc/img/os/virt/c5TkF1QHGyvS3dz.png)

相比于Xen架构，KVM架构有三大的优势：
1. 同等硬件资源环境下，KVM的性能表现更优
1. KVM架构天然的继承Linux内核更新迭代带来的系统优化，几乎不费力气，就完成了一次功能升级, 但对于Xen架构来说，每一次Xen Hypervisor内核或者Linux内核版本升级，Xen架构需要同步优化联调Xen Hypervisor内核和特权域(基于Linux的内核)，才能实现整个虚拟化内核的升级
1. 目前云厂商均使用kvm, 是趋势. 

![VirtIO](/misc/img/os/virt/WqYaSrQ3C8eEKuh.png)

## nested
kvm理论支持N层嵌套, 但实际2层后就已很慢.

intel nested:
1. 开启nested

  ```bash
  # rmmod kvm-intel
  # modprobe kvm-intel nested=1 # 或echo "options kvm-intel nested=y" > /etc/modprobe.d/kvm_mod.conf
  # cat /sys/module/kvm_intel/parameters/nested
  Y
  ```
1. 使用cpu host: `<cpu mode="host-passthrough"/>`
1. 参照host, 配置第一层vm即可安装第二层vm

## kvm生态组成
1. kvm内核模块: 主要负责cpu和内存的虚拟化

  由两部分组成:
  - kvm.ko: 处理器架构无关部分
  - kvm_xxx: 处理器相关部分, 比如intel的kvm_intel

  KVM的主要功能是初始化CPU硬件, 打开虚拟化模式, 然后将虚拟客户机运行在虚拟机模式下, 并对虚拟客户机的运行提供一定的支持.

  以intel举例, 在被kvm内核加载的时候, KVM模块会先初始化内部的数据结构; 做好准备之后, KVM模块检测系统当前的CPU, 然后打开CPU控制寄存器CR4中的虚拟化模式开关, 并通过执行VMXON指令将宿主操作系统（包括KVM模块本身） 置于CPU执行模式的虚拟化模式中的根模式； 最后， KVM模块创建特殊设备文件/dev/kvm并等待来自用户空间的命令(by ioctl). 接下来, 虚拟机的创建和运行将是一个用户空间的应用程序（QEMU） 和KVM模块相互配合的过程.

  针对虚拟处理器的最重要的loctl调用就是"执行虚拟处理器". 通过它， 用户空间准备好的虚拟机在KVM模块的支持下， 被置于虚拟化模式中的非根模式下， 开始执行二进制指令. 在非根模式下， 所有敏感的二进制指令都会被处理器捕捉到， 处理器在保存现场之后自动切换到根模式, 由KVM决定如何进一步处理（要么由KVM模块直接处理， 要么返回用户空间交由用户空间程序处理）.

  除了处理器的虚拟化， 内存虚拟化也是由KVM模块实现的, 比如intel ept.

  处理器对设备的访问主要是通过I/O指令和MMIO, 其中I/O指令会被处理器直接截获， MMIO会通过配置内存虚拟化来捕捉, 但是， 外设的模拟一般不由KVM模块而是qemu负责. 一般来说, 只有对性能要求比较高的虚拟设备才会由KVM内核模块来直接负责， 比如虚拟中断控制器和虚拟时钟， 这样可以大量减少处理器模式切换的开销.

2. qemu用户态工具: 提供设备模拟功能

  每一个虚拟客户机在宿主机中就体现为一个QEMU进程, 而客户机的每一个vCPU就是一个QEMU线程. 虚拟机运行期间, QEMU会通过KVM模块提供的系统调用进入内核, 由KVM模块负责将虚拟机置于处理器的特殊模式下运行. 遇到虚拟机进行I/O操作时， KVM模块会从上次的系统调用出口处返回QEMU, QEMU来负责解析和模拟这些设备.

  QEMU还提供了叫作virtio-blk-data-plane的一种高性能的块设备I/O方式， 它最初在QEMU 1.4版本中被引入. virtio-blk-data-plane与传统virtioblk相比， 它为每个块设备单独分配一个线程用于I/O处理, data-plane线程不需要与原QEMU执行线程同步和竞争锁， 而且它使用ioeventfd/irqfd机制， 同时利用宿主机Linux上的AIO（异步I/O） 来处理客户机的I/O请求， 使得块设备I/O效率进一步提高.

## kvm特性
1. 内存管理

    vm的"物理内存"就是对应宿主机进程的虚拟内存.
    vm自身内存访问落实到真实的宿主机的物理内存的机制叫影子页表(shadow page table), kvm Hypervisor为每个vm准备一份影子页表, 与vm自身页表建立一一对应关系. 因为性能原因, 目前影子页表的映射转换(`GPA->HPA`)已由硬件完成(intel ept/amd npt), 默认开启.

    > - kvm内存映射转换: GVA(guest virtual address, 客户机虚拟地址)->GPA(guest physical address)->HVA(host virtual address, 宿主机虚拟地址)->HPA(host physical address).

    > - vm自身页表描述的是GVA->GPA, 通过CR3寄存器完成; 影子页表描述的是GPA->HPA.
1. kvm image格式

    确切的说是qemu的功能.
    kvm的image格式是qcow2, 支持稀疏文件, 快照(包括多级快照), 压缩和加密.
1. 实时迁移

    在宿主机间迁移vm而不中断服务.
1. 设备驱动程序

    - 支持半虚拟化的驱动程序(virtio标准)来提高io性能.
    - 支持intel VT-d, 通过将宿主机的pci总线上的设备透传(pass-through)给vm.
1. 性能和可伸缩性

    继承了linux的性能和可伸缩性

## 核心
### cpu
多cpu主要有3种架构:
- smp
- numa
- mpp

  通常通过软件组合若干smp或若干numa实现

在普通的Linux系统中， 进程一般有两种执行模式： 内核模式和用户模式. 而在KVM环境中， 增加了第3种模式： 客户模式.

vCPU在3种执行模式下的不同分工如下:
1. 用户模式（User Mode）

  主要处理I/O的模拟和管理， 由QEMU的代码实现

1. 内核模式（Kernel Mode）

  主要处理特别需要高性能和安全相关的指令， 如处理客户模式到内核模式的转换， 处理客户模式下的I/O指令或其他特权指令引起的退出（VM-Exit） ， 处理影子内存管理（shadow MMU）

1. 客户模式（Guest Mode）

  主要执行Guest中的大部分指令， I/O和一些特权指令除外（它们会引起VM-Exit， 被Hypervisor截获并模拟）

![vCPU在KVM中的这3种执行模式下的转换](/misc/img/virt/20150510150314072.png)

有两种操作来实现客户机的SMP(symetric multi-processor, 共享内存资源的多处理器系统, 与NUMA相对), 一是将不同的vCPU的进程交换执行（分时调度， 即使物理硬件非SMP， 也可以为客户机模拟出SMP系统环境）; 二是将在物理SMP硬
件系统上同时执行多个vCPU的进程.

在qemu命令行中， `-smp [cpus=]n[,maxcpus=cpus][,cores=cores][,threads=threads][,sockets=sockets]`参数即是配置客户机的SMP系统:
- n用于设置客户机中使用的逻辑CPU数量（默认值是1） 
- maxcpus用于设置客户机中最大可能被使用的CPU数量， 包括启动时处于下线（offline） 状态的CPU数量（可用于热插拔hot-plug加入CPU， 但不能超过maxcpus这个上限）

  当`n<maxcpus`时可模拟cpu热插拔. 比如在qemu monitor执行`cpu-add <id>`(id可与`info cpu`已有序号不连续), 新加的vcpu在guest里id是连续的.
- cores用于设置每个CPU的core数量（默认值是1） 
- threads用于设置每个core上的线程数即超线程选项（默认值是1） 
- sockets用于设置客户机中看到的总的CPU socket数量

> 不加`-smp`即使用默认值1

> 通过qemu monitor的`info cpu`可查看guest的cpu数量及其对应的qemu线程id. 输出带`*`的是BSP(boot strap process, 系统最初启动时在SMP生效前使用的cpu)

#### cpu过载
推荐的做法是对多个单CPU的客户机使用over-commit. 最不推荐的做法是让某一个客户机的vCPU数量超过物理系统
上存在的CPU数量.

> 不过， 在并非100%满负载的情况下， 一个（或多个） 有4个vCPU的客户机运行在拥有4个逻辑CPU的宿主机中并不会带来明显的性能损失

总结: **KVM允许CPU的过载使用， 但是并不推荐在实际的生产环境（特别是负载较重的环境） 中过载使用CPU**

#### cpu model
查询当前qemu支持的所有cpu model: `qemu-system-x86_64 -cpu ?`, 源码在`https://github.com/qemu/qemu/blob/master/target/i386/cpu.c的builtin_x86_defs`里.

在x86_64下, qemu不添加`-cpu xxx`启动时, 默认使用qemu64.

#### 进程的处理器亲和性和vCPU的绑定
进程的处理器亲和性（Processor Affinity） 即CPU的绑定设置, 是指将进程绑定到特定的一个或多个CPU上去执行， 而不允许将进程调度到其他的CPU上。 Linux内核对进程的调度算法也是遵守进程的处理器亲和性设置的. 设置进程的处理器亲和性带来的好处是可以减少进程在多个CPU之间交换运行带来的缓存命中失效（cache missing） ， 从该进程运行的角度来看， 可能带来一定程度上的性能提升。 换个角度来看， 对进程亲和性的设置也可能带来一定的问题， 如破坏了原有SMP系统中各个CPU的负载均衡（load balance） ，
这可能会导致整个系统的进程调度变得低效。 特别是在使用多处理器、 多核、 多线程技术的情况下， 在NUMA结构的系统中， 如果不能对系统的CPU、 内存等有深入的了解， 对进程的处理器亲和性进行设置可能导致系统的整体性能的下降而非提升.

每个vCPU都是宿主机中一个普通的QEMU线程， 可以使用taskset工具对其设置处理器亲和性， 使其绑定到某一个或几个固定的CPU上去调度.

需求: Iaas为客户提供一个有两个逻辑CPU计算能力的一个客户机, 要求CPU资源独立被占用, 不受宿主机中其他客户机的负载水平的影响.
解决方案:
1. host启动kernel时添加`isolcpus=`以实现cpu的隔离

  通过`ps -eLo psr | grep -e "^[[:blank:]]*<cpu_id>$" | wc -l`查看该cpu上的线程, 隔离后的cpu上跑的线程很少(5~6, 这些线程通常都是内核对各个CPU的守护进程), 可通过`ps -eLo ruser,pid,ppid,lwp,psr,args | awk '{if($5==<cpu_id>) print $0}'`查看具体线程, 这些线程有:
  - migration线程（用于进程在不同CPU间迁移）
  - 两个kworker线程（用于处理workqueues）
  - ksoftirqd线程（用于调度CPU软中断的进程）
  - watchdog线程
1. 启动qemu后找到该进程, 在使用`taskset -pc <cpu_list> <pid>`绑定到隔离出的cpu上.

总结: 在KVM环境中， 一般并不推荐手动设置QEMU进程的处理器亲和性来绑定vCPU， 但是， 在非常了解系统硬件架构的基础上， 根据实际应用的需求， 可以将其绑定到特定的CPU上， 从而提高客户机中的CPU执行效率或实现CPU资源独享的隔离性.

cpu绑定是libvirt通过cgroup实现, 应用场景:
- 系统的cpu压力比较大
- 多核cpu压力不平衡

### 内存
qemu不设置`-m`默认就是128M, 见`hw/core/machine.c的machine_class_init`. `-m`默认单位是M, 可加上`M/G`后缀来作为内存分配单位

KVM中客户机是一个QEMU进程， 宿主机系统没有特殊对待它而分配特定的内存给QEMU， 只是把它当作一个普通Linux进程. 所以是在客户机操作系统请求更多内存时， KVM才向其分配更多的内存.

#### 内存过载
有如下3种方式来实现内存的过载使用:
1. 内存交换（swapping） ： 用交换空间（swap space） 来弥补内存的不足

  内存交换的方式是最成熟的, 但相比KSM和ballooning的方式效率较低一些.
1. 气球（ballooning） ： 通过virio_balloon驱动来实现宿主机Hypervisor和客户机之间的协作

  开启方法:
  - linux

    `CONFIG_VIRTIO_BALLOON`, centos7已默认开启, 在vm中会看到一个Virtio memory balloon的pci设备
  - windows

    1. 安装virt balloon驱动, 可看到名为virtio balloon driver的pci设备
    2. 安装balloon服务: `BLNSVR.EXE -i` 

  vm xml开启:
  ```xml
  <memballoon model="virtio">
    <alias name="balloon0">
  </memballoon>
  ```

  balloon操作:
  - 膨胀 : 虚拟机的内存被挪给host
  - 压缩 : host内存给vm

  查看当前vm mem大小: `virsh qemu-monitor-command ct7 --hmp --cmd info balloon`
  修改vm mem大小到2G: `virsh qemu-monitor-command ct7 --hmp --cmd balloon 2048`
1. 页共享（page sharing） ： 通过KSM（Kernel Samepage Merging, 在centos 6,7上默认开启） 合并多个客户机进程使用的相同内存页(支持numa)

  关闭跨numa的ksm: `echo 0 > /sys/kernel/mm/ksm/merge_across_nodes`
  关闭特定vm的ksm用:
  ```xml
  <memoryBacking>
    <nosharepages/>
  </memoryBacking>
  ```

  慎用ksm, 当业务压力大, vm内存变化频繁时会导致:
  1. 因消耗一定的cpu资源用于内存扫描, 加重其负载
  1. 当内存不足时, 频繁使用swap, 导致vm性能严重下降

  使用场景: 测试环境, 桌面虚拟化.

KVM允许内存过载使用, 但不建议过多地过载使用内存.

ksm的两个服务:
- ksm
- ksmtuned

关闭上诉两个服务即可关闭ksm. 当host内存不足是, 临时开启ksm也是一种应急方案.

`/sys/kernel/mm/ksm`可查看ksm运行状态:
- pages_shared : 记录stable树的节点数, 即有多少共享内存页正在被使用
- pages_sharing: 记录被共享的物理页数, 通过此可计算节省的物理内存
- pages_unshared: 内存被合并时有多少内存页独特但是反复被检查. 记录不稳定树的节点树，即未共享的物理页数
- pages_volatile: 多少内存页改变太快被放置
- full_scans: 已经执行的全区域扫描次数

较高的 pages_sharing/pages_shared 比率表明高效的页面共享, 反之则表明资源浪费.

### 存储配置
一个vm最多支持4个ide; 一个virtio设备占用一个vm的pci sort; virtio-scsi走scsi通道, 理论上一个scsi通道可挂载256块盘.

windows 2000之前的系统仅支持ide.

1. 基本配置参数:

  - -hd<N> file

    将file镜像文件作为客户机中的第1个IDE设备（序号0）, 在客户机中表现为/dev/hda设备（若客户机中使用PIIX_IDE驱动）或/dev/sda设备（若客户机中使用ata_piix驱动）.
  - -fd<N> file

    将file作为客户机中的第1个软盘设备（序号0）, 在客户机中表现为/dev/fd0设备. 也可以将宿主机中的软驱（/dev/fd0） 作为-fda的file来使用
  - -cdrom file

    将file作为客户机中的光盘CD-ROM， 在客户机中通常表现为/dev/cdrom设备. 也可以将宿主机中的光驱（/dev/cdrom） 作为-cdrom的file来使用

    **-cdrom参数不能和-hdc参数同时使用， 因为`-cdrom`就是客户机中的第3个IDE设备**
  - -mtdblock file

    使用file文件作为客户机自带的一个Flash存储器（通常说的闪存）
  - -sd file

    使用file文件作为客户机中的SD卡（Secure Digital Card）
  - -pflash file

    使用file文件作为客户机的并行Flash存储器（Parallel Flash Memory）
1. 支持详细配置的`-drive option[,option[,option[,...]]]`参数, 它定义一个存储驱动器

  - file=file

    使用file文件作为镜像文件加载到客户机的驱动器中
  - if=interface

    指定驱动器使用的接口类型， 可用的类型有： ide、 scsi、 sd、 mtd、 floopy、 pflash、virtio等等.
  - bus=bus， unit=unit

    设置驱动器在客户机中的总线编号和单元编号
  - index=index

    设置在同一种接口的驱动器中的索引编号
  - media=media

    设置驱动器中媒介的类型， 其值为`disk`或`cdrom`
  - snapshot=snapshot

    设置是否启用"-snapshot"选项， 其可选值为"on"或"off". 当snapshot启用时， QEMU不会将磁盘数据的更改写回镜像文件中， 而是写到临时文件中。 当然可以在QEMU monitor中使用"commit"命令强制将磁盘数据的更改保存回镜像文件中.

  - cache=cache


    设置宿主机对块设备数据（包括文件或一个磁盘） 访问中的cache情况， 可以设置为"none"（或"off"） "writeback""writethrough"等. 其默认值是"writethrough".

    - writethrough

      即"直写模式", 在调用write写入数据的同时将数据写入磁盘缓存（disk cache） 和后端块设备（block device） 中， 其优点是操作简单， 其缺点是写入数据速度较慢.

      打开disk image或块设备使用O_DSYNC. vm的driver告知vm没有cache, 因此vm不需要发出刷盘命令以保证数据一致性.

      场景: 断电或host故障不丢数据
    - writeback

      即"回写模式", 在调用write写入数据时只将数据写入磁盘缓存中即返回， 只有在数据被换出缓存时才将修改的数据写到后端存储中。 其优点是写入数据速度较快， 其缺点是一旦更新数据在写入后端存储之前遇到系统掉电， 数据会无法恢复.

      打开disk image或块设备既不是O_DSYNC, 也不是O_DIRECT, 会使用host页面缓存, 数据到达host页面缓存即可返回写成功. vm 磁盘控制器被告知可使用回写缓存, 因此vm需要发出刷盘命令以保证数据一致性.
    - none

      关闭缓存的方式, 所以其读写数据都是绕过缓存直接从块设备中读写的.

      打开disk image或块设备使用O_DIRECT, 绕过host页面缓存, io直接对接qemu的用户空间缓存和host存储设备. 数据在被放入写入队列时就返回写成功, vm 磁盘控制器被告知可使用回写缓存, 因此vm需要发出刷盘命令以保证数据一致性.

      场景: 在线迁移
    
    writethrough和writeback在读取数据时都尽量使用缓存. 一些块设备文件（比如qcow2格式文件） 在"writethrough"模式下性能表现很差， 如果这时对性能要求比正确性更高, 建议使用"writeback"模式.

  - aio=aio

    选择异步IO（Asynchronous IO） 的方式， 有"threads"和"native"两个值可选. 其默认值为"threads"， 即让一个线程池去处理异步IO。 而"native"只适用于"cache=none"的情况，就是使用Linux原生的AIO.

  - format=format

    指定使用的磁盘格式， 在默认情况下QEMU自动检测磁盘格式

    镜像加密只用于镜像传输, 使用时, 必须通过convert对镜像镜像转换, 以到达解密.
    raw不支持snap, 只有qcow2支持.
  - serial=serial

    指定分配给设备的序列号\
  - addr=addr

    分配给驱动器控制器的PCI地址， 该选项只有在使用virtio接口时才适用。
  - id=name

    设置该驱动器的ID， 这个ID可以在QEMU monitor中用"info block"看到
  - readonly=on|off

    设置该驱动器是否只读

1. 配置客户机启动顺序的参数: `-boot [order=drives][,once=drives][,menu=on|off] [,splash=splashfile] [,splash-time=sp-time]`

  在QEMU模拟的x86 PC平台中, 用"a""b"分别表示第1个和第2个软驱, 用"c"表示第1个硬盘， 用"d"表示CD-ROM光驱， 用"n"表示从网络启动. 其中， 默认从硬盘启动， 要从光盘启动可以设置"-boot order=d". "once"表示设置第1次启动的启动顺序, 在系统重启（reboot） 后该设置即无效, 如"-boot once=d"设置表示本次从光盘启动， 但系统重启后从默认的硬盘启动. "memu=on|off"用于设置交互式的启动菜单选项（前提是使用的客户机BIOS支持） ， 它的默认值是"menu=off"， 表示不开启交互式的启动菜单选择. "splash=splashfile"和"splash-time=sp-time"选项都是在"menu=on"时才有效， 将名为splashfile的图片作为logo传递给BIOS来显示； 而sp-time是BIOS显示splash图片的时间， 其单位是毫秒（ms） 。 比如在使用"-boot order=dc， menu=on"设置后， 需在客户机启动窗口中按F12键进入的启动菜单.


缓存对在线迁移的影响:
1. 只有raw和qcow2支持
1. 如果使用集群文件系统, 那么所有镜像格式支持在线迁移, 否则仅none模式支持

### 后备镜像差量
后备镜像差量指多台vm共用一个后备镜像, 每个vm根据需要, 如果是读操作, 读取镜像. 如果是写操作, 写自己的镜像文件. 后备镜像支持raw和qcow2, 差量镜像仅支持qcow2.

差量方式的优势:
1. 快速生成vm image
1. 节省磁盘空间

风险:
1. 多台vm启动时, i/o压力大, 尤其是第一次启动
1. 对后备镜像安全性要求非常高

  通常将后备镜像和差量镜像分散到不同的硬盘上, 保证镜像的安全性

```bash
qemu-img create -f qcow2 -b w2k3-test w2k3-test-01 # 创建差量镜像
qemu-img convert -f qcow2 -O qcow2 w2k3-test-01 w2k3-test-01-10 # 将差量镜像转为普通镜像
qemu-img rebase -u -b w2k3-test-01-10 w2k3-test-01 # 更换后备镜像. -u非安全方式, 只更换镜像; 没有-u则会比较旧后备镜像和新的后备镜像的差异, 以新的镜像为准
qemu-img commit [-f format] filename # 将文件改变的内容合并到后备镜像中， 主要用于需要更新后备镜像的场景
```

qemu-img的resize只修改镜像大小, 不能修改更新, 但guestfish的virt-resize可同时修改它俩.

qcow2不支持缩小, 但raw支持, 前提是先将分区和fs缩小, 否则会损坏数据.

裸设备可直接分配给vm, 但推荐使用lvm/zfs, 它们支持动态调整和snap.

使用lvm时建议将`/etc/lvm/lvm.conf`的write_cache_state设为`0`(即关闭lvm cache), 避免断电丢数据.
不建议在vm运行时, 做snap, 可能出现数据不一致情况.

磁盘格式建议:
1. cpu消耗型vm使用qcow2

  性能不如裸设备, 但支持特性多
1. 磁盘i/o消耗型使用lvm/zfs zvol这样的裸设备

  读写性能好, 支持动态扩展, 以及snap

> windows server 2008开始默认使用1MB块对齐, linux也跟进了.

为了发挥ssd性能, vm disk应使用裸设备映射, 而不是qcow2.

### 网络
kvm网络优化方案: 总的思路是让vm访问物理网卡的层数更少, 直至对物理网卡独占.

在kvm中, 默认网络设备是由qemu在用户空间模拟并提供给vm, 这提供了最大的灵活性. 但由于网络i/o的过程需要虚拟化引擎参与有大量的vm exit/entry, 效率低下, 因此产生了全虚拟化网卡和半虚拟化网卡:
- 全虚拟化网卡 : 由虚拟化层完全模拟出来的网卡
- 半虚拟化网卡 : 通过驱动对os做改造, 比如网络半虚拟化标准virtio, 它让vm可直接与虚拟化层通信, 从而大大提高了vm性能.

  `qemu-system-x86_64 -netdev type=tap,script=/etc/kvm/qemu-ifup,id=net0 -device virtio-net-pci,netdev=net0 ...`
  vm xml:
  ```xml
  <interface type="bridge">
    <mac address="fa:16:3e:fc:e0:c0"/>
    <source bridge="br0"/>
    <model type="virtio"/> # 如果是全虚拟化网卡, 比如e1000是intel的千兆网卡
    <address type="pic" domain="0x0000" bus="0x00" slot="0x03" function="0x0"/>
  </interface>
  ```

vm到host 网络网卡的完整链路: mv->虚拟网卡->虚拟化层->内核网桥->物理网卡

MacVTap是虚拟化层跳过kernel的网桥, 直接和host 物理网卡通信, 是用来代替tun/tap和bridge内核模块的. macVTap是基于MacVlan(可给一个物理网卡配置多个mac), 提供tun/tap中tap设备使用的接口, 让使用macvtap以太网口的vm能通过tap设备接口, 直接将数据传递给内核中对应的macvtap以太网口. 它与传统的网桥方案(tap+briage)相比, 支持新的虚拟化网络技术, 可以将host cpu的一些工作交给网络设备.

> 传统的网桥方案(tap+briage): 将vm连接到虚拟的tap网卡, 在将tap加入bridge, 此时bridge相当于软交换机, 本质是用host cpu通过软件模拟网络, 缺点:
> 1. 每台host都存在bridge, 会使网络拓扑复杂, 等价于增加了交换机的级联层数
> 1. 同一台host上vm间的流量直接在bridge完成交换, 使流量监控, 监管困难
> 1. bridge是软件实现的二层交换技术, 会加大host负担

> 当前macvtap对windows vm不佳, 不推荐使用它.

macvtap支持3种工作模式:
1. vepa(virtual Ethernet port aggregator)

  同一个物理网卡下的macvtap设备之间的流量也要发送到外部交换机再由外部交换机发回host, 前提是交换机必须支持hairpin模式
1. brigde

  类似传统linux bridge, 同一个物理网卡下的macvtap设备之间的流量之间交换, 不需要外部交换机介入 
1. private

  同一个物理网卡下的macvtap设备之间不互通, 无论外部交换机是否支持hairpin模式

创建macvtap:
```bash
ip link add link eth1 name MacVTap0 type MacVTap
ip link set MacVTap0 address 12:34:56:78:9a:bc up
ip link show MacVTap0
```

使用MacVTap0:
```xml
<interface type="direct">
  <mac address="12:34:56:78:9a:bc"/>   
  <source dev="eth1" mode="bridge"/> 
  <model type="virtio"/> 
  <driver name="vhost"/> 
</interface>
```

vhost_net是使vm绕过用户空间的虚拟化层, 直接和host kernel通信, 从而提高网络性能, 它必须使用virtio的半虚拟化网卡. 

vhost-net是对virtio的优化, virtio是虚拟化层的前端优化方案, 减少硬件虚拟化下根模式和非根模式的切换, 而vhost-net是虚拟层后端优化方案. 不使用vhost-net, 进入cpu根模式后需要进入用户态将数据发送到tap设备后, 再次切入内核态; 使用它后, 进入内核态后不需要再进行内核态用户态的切换, 进一步减少特权级切换的开销.

使用vhost_net:
```xml
<interface type="direct">
  <mac address="12:34:56:78:9a:bc"/>   
  <source dev="eth1" mode="bridge"/> 
  <model type="virtio"/> 
  <driver name="vhost"/> # 关键
</interface>
```

网卡多队列本质是网卡的数据请求通过多个cpu处理, 即为每个队列创建单独的中断线程将每个队列产生的中断分布到多个cpu, 实现负载均衡以避免单个cpu占用过高且其他核空闲的情况, 可在`/sys/class/net/eth0/queues`看到对应的多个发送和接收队列. 判断kernel是否支持多队列: `grep IFF_MULTI_QUEUE /usr/include/linux/if_tun.h`. vm多队列配置: `<driver name="vhost" queues="N" />`, vm开启多队列使用`ethtool -L eth0 combined M`, M<=N.

centos通过irqbalance服务优化中断分配, 它会自动收集系统数据以分析使用模式, 并已经系统负载将工作状态设置为:
- performance : irqbalance会将中断尽可能均匀地分布到各个cpu core, 以充分利用cpu多核
- power-save : irqbalance会将中断集中分配到一个cpu， 保证空闲cpu的睡眠时间, 降低能耗

irqbalance在大压力下, 尤其是万兆网卡上, 有中断漂移及分配不均匀的问题, 因此次数可手动调整系统的网卡中断, 可参考gist.github.cm/syuu1228/4352382: 关闭irqbalance以防止中断cpu亲和性被它改写.

rss需要网卡硬件支持. 网卡软中断可用RPS(receive packet steering)和RFS(receive flow steering)
- RPS : 没考虑app在哪个cpu运行

  rps可把一个rx队列的软中断分发到多个cpu, 从而达到负载均衡的目的.
- RFS : 考虑了应用在哪个cpu执行, 会在其上执行数据包处理

  rfs是rps的扩展

借助rps/rfs可实现vm 软中断分发.

通过QEMU的支持， 常见的可以实现以下4种网络形式：
1. 基于网桥（bridge） 的虚拟网络

  可以让客户机和宿主机共享一个物理网络设备连接网络, 客户机有自己的独立IP地址, 可以直接连接与宿主机一模一样的网络, 客户机可以访问外部网络,  外部网络也可以直接访问客户机（就像访问普通物理主机一样）. 即使宿主机只有一个网卡设备， 使用bridge模式也可让多个客户机与宿主机共享网络设备.

  bridge模式的网络（后端） 参数配置`-netdev tap,id=id[,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper]`或`-net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile][,helper=helper]`. 这两个配置方法效果等价， 但用于不同场合：`-netdev tap， id=id[， ...]与-device参数配套使用. `-net tap[， ...]`与`-net nic`参数配套使用, 第2种方法可以认为是第一种的简化版， 因为免去了后端设备id的指定, 具体参数:
  - tap参数， 表明使用TAP设备

    TAP是虚拟网络设备， 它仿真了一个数据链路层设备（ISO七层网络结构的第2层）, 它像一个网桥一样处理第2层数据报. 而TUN与TAP类似，也是一种虚拟网络设备， 它是对网络层设备的仿真. TAP用于创建一个网桥， 而TUN则与路由相关（工作在IP层）.

    > 用tap模式时候, helper方式很不好用， 因为没法在参数里指定bridge名字， 而它只能使用名为br0的bridge连接tap设备.

  - vlan=n， 设置该设备连入VLAN n， 默认值为0（即没有VLAN）
  - name=name， 设置名称， 在QEMU monior中可能用到， 一般由系统自动分配即可
  - fd=h， 连接到现在已经打开着的TAP接口的文件描述符。 一般不要设置该选项， 而是让QEMU自动创建一个TAP接口. 在使用了fd=h选项后， ifname、 script、 downscript、helper、 vnet_hdr等选项都不可用了（不能与fd选项同时出现在命令行中）.
  - ifname=name， 设置在宿主机中添加的TAP虚拟设备的名称（如tap1、 tap5等） 。 当不设置这个参数时， QEMU会根据系统中目前的情况，产生一个TAP接口的名称.
  - script=file， 设置宿主机在启动客户机时自动执行的网络配置脚本. 如果不指定， 其默认值为"/etc/qemu-ifup"这个脚本. 可指定自己的脚本路径以取代默认值； 如果不需要执行脚本， 则设置为"script=no"
  - downscript=dfile， 设置宿主机在客户机关闭时自动执行的网络配置脚本.

    如果不设置， 其默认值为"/etc/qemu-ifdown"； 若客户机关闭时宿主机不需要执行脚本， 则设置为"downscript=no"

    由于QEMU在客户机关闭时会解除TAP设备的bridge绑定， 也会自动删除已不再使用的TAP设备， 所以qemu-ifdown这个脚本不是必需的， 最好设置为"downscript=no".
  - helper=helper， 设置启动客户机时在宿主机中运行的辅助程序， 包括建立一个TAP虚拟设备， 默认值为/usr/local/libexec/qemu-bridge-helper.  此处一般不用定义.

  使用bridge步骤:
  ```bash
  # dnf install bridge-utils tunctl
  # modprobe tun bridge
  # ll /dev/net/tun # 确保当前用户对该文件有可读写权限
  # brctl addbr virbr0 # 创建名为virbr0的bridge, 配置在/etc/sysconfig/network-scripts/ifcfg-virbr0
  # brctl addif virbr0 eno2 # 将将eno2绑定到virbr0上
  # ifup virbr0
  ```

  新版qemu由更直接的网桥配置选项: `-netdev bridge,id=id[,br=bridge][,helper=helper]`或`-net bridge[,vlan=n][,name=name][,br=bridge][,helper=helper]`. 它们比上面的方式省掉了关于tap name、 script等的参数指定, 而只需要指定网桥就可以了. 其他的都封装在helper程序里面自动帮忙做掉了， 包括自动命名和创建tap设备、 自动启动tap设备（即原来的script脚本要完成的工作）、绑定网桥等. 这些本来在多数应用场景中就不需要特别指定的.

2. 基于NAT（Network Addresss Translation） 的虚拟网络

  在QEMU/KVM中， 默认使用IP伪装的方式实现NAT， 而不是使用SNAT（SourceNAT） 或DNAT（Destination-NAT） 的方式.

  在KVM中配置客户机的NAT网络方式， 需要在宿主机中运行一个DHCP服务器给宿主机分配NAT内网的IP地址, 可以使用dnsmasq工具来实现.

  > libvirt的3种虚拟网络的实现（NAT、 Routed、 Isolated）, 在Hypervisor是QEMU/KVM的情况下， 就是通过QEMU的bridge网络模式来实现的.
3. QEMU内置的用户模式网络（user mode networking）

  提供了一种由qemu自身实现的用户模式（user-mode） 的网络模拟, 不依赖于其他的工具（如前面提到的bridge-utils、 dnsmasq、 iptables等） ， 而且不需要root用户权限. QEMU使用Slirp在源码的`slirp`目录实现了一整套TCP/IP协议栈， 并且使用这个协议栈实现了一套虚拟的NAT网络.

  用户模式网络有以下3个缺点：
  1. 由于其在QEMU内部实现所有网络协议栈， 因此其性能较差。
  1. 不支持部分网络功能（如ICMP） ， 所以不能在客户机中使用ping命令测试外网连通性
  1. 不能从宿主机或外部网络直接访问客户机

  使用方法: `-netdev user,id=id[,option][,option][,...]`或`-net user[,option][,option][,...]`, 常用选项有:
  - vlan=n : 将用户模式网络栈连接到编号为n的VLAN中（默认值为0）
  - name=name : 分配一个在QEMU monitor中会用到的名字（如在monitor的"info network"命令中可看到这个网卡的name）
  - net=addr[/mask] : 设置客户机可以看到的网络地址（客户机所在子网） ， 其默认值是10.0.2.0/24. 其中， 子网掩码（mask） 有两种形式可选，一种是类似于255.255.255.0这样的地址， 另一种是32位IP地址中前面被置位为1的位数（如10.0.2.0/24）.
  - host=addr， 指定客户机可见宿主机的地址， 默认值为客户机所在网络的第2个IP地址（如10.0.2.2）
  - ipv6-net=addr[/int]， 设置客户机看到的IPv6网络地址， 默认是`fec0::/64`
  - ipv6-host=addr : 设置客户机的IPv6地址， 默认是第2个IP地址`xxxx::2`
  - restrict=y|yes|n|no， 如果将此选项打开（ 为y或yes） ， 则客户机将会被隔离， 客户机不能与宿主机通信，其IP数据包也不能通过宿主机而路由到外部网络中。 这个选项不会影响"hostfwd"显式地指定的转发规则， 即hostfwd"选项始终会生效。 默认值为n或no， 不会隔离客户机。
  - hostname=name: 设置在内置的DHCP服务器中保存的客户机主机名
  - dhcpstart=addr: 设置能够分配给客户机的第1个IP， 在QEMU内嵌的DHCP服务器有16个IP地址可供分配。 在客户机中IP地址范围的默认值是子网中的第15～30个IP地址（ 如10.0.2.15～10.0.2.30）
  - dns=addr : 指定虚拟DNS的地址， 这个地址必须与宿主机地址（ 在"host=addr"中指定的） 不相同， 其默认值是网络中的第3个IP地址（ 如10.0.2.3）
  - tftp=dir : 激活QEMU内嵌的TFTP服务器， 目录dir是TFTP服务的根目录. 在客户机使用TFTP客户端连接TFTP服务后需要使用binary模式来操作。
  - bootfile=file， 与tftp=dir配合使用， 可以实现虚拟的PXE boot. 比如"-boot n-net user， tftp=/path/to/tftp/files，bootfile=/pxelinux.0"就指定了客户机PXE启动文件， 它位于QEMU虚拟的tftp服务器路径下.
  - smb=dir[， smbserver=addr] : QEMU可以激活（ 模拟） 一个内置的Samba服务器，作为连接host和Windows客户机的文件传输的纽带。"dir"指示的就是host上存放共享文件的文件夹， "smbserver"（ 可选） 指定对客户机而言的Samba server的IP地址， 默认是net的第4个IP， 即x.x.x.4. 注意， Windows客户机需要把这个Samba server的IP地址静态解析（ "10.0.2.4 smbserver"这样一行） 写入它的LMHOSTS文件中， 比如Windows 10系统的C：\Windows\System32\drivers\etc\lmhosts.sam文件. 这样， 在客户机中， 就可以通过`\\smbserver\qemu`访问宿主机的"dir"文件夹了。 注意, 宿主机要求安装好Samba服务并启动.
  - ipv6-dns=addr: 指定虚拟的IPv6 DNS的地址， 这个地址必须与宿主机地址（ 在"ipv6-host=addr"中指定的） 不相同，其默认值是网络中的第3个IP地址（ 如xxxx： ： 3）.
  - dnssearch=domain : 内置的DHCP server在分配IP给客户机的时候， 会附带DNS域的信息. 这个参数就是指定域列表（ 可以是多个）
  - hostfwd=`[tcp|udp]:[hostaddr]:hostport-[guestaddr]:guestport`， 将访问宿主机的hostpot端口的TCP/UDP连接重定向到客户机（ IP为guestaddr） 的guestport端口上. 如果没有设置guestaddr， 那么默认使用x.x.x.15（ DHCP服务器可分配的第1个IP地址）.如果指定了hostaddr的值， 则可以根据宿主机上的一个特定网络接口的IP端口来重定向. 如果没有设置连接类型为TCP或UDP， 则默认使用TCP连接。 "hostfwd=…"这个选项在一个命令行中可以多次重复使用.
  - guestfwd=`[tcp]:server:port-dev`及`guestfwd=[tcp]:server:port-cmd:command`, 将客户机中访问IP地址为server的port端口的连接转发到宿主机的dev这个字符设备上. 或者每次访问这个server:port就执行一次命令（cmd即具体命令）. "guestfwd=…"这个选项也可以在一个命令行中多次重复使用

  其他不常用网络选项:
  - `-net socket[,vlan=n][,name=name][,fd=h][,listen=[host]:port][,connect=host:port]`: 使用TCP socket连接客户机的VLAN

    使用TCP socket连接将n号VLAN连接到一个远程的QEMU虚拟机的VLAN。 如果有"listen=…"参数， 那么QEMU会等待对port端口的连接， 而host参数是可选的（默认值为本机回路IP地址127.0.0.1） 。 如果有"connect=…"参数， 则表示连接远端的已经使用"listen"参数的QEMU实例。 可使用"fd=h"（文件描述符h） 指定一个已经存在的TCP socket
  - `-net socket[,vlan=n][,name=name][,fd=h][,mcast=maddr:port]`: 使用UDP的多播socket建立客户机间的连接

    建立n号VLAN， 使用UDP多播socket连接使其与另一个QEMU虚拟机共享， 用相同的多播地址（maddr） 和端口（port） 为每个QEMU虚拟机建立同一个总线. 多播的支持还与[用户模式Linux（User-Mode Linux）](http://user-mode-linux.sourceforge.net/)兼容
  - `-net vde[,vlan=n][,name=name][,sock=socketpath][,port=n][,group=groupname][,mode= octalmode]`:使用VDE swith的网络连接

    连接n号VLAN到一个VDE switch的n号端口， 这个VDE switch运行在宿主机上并且监听着在socketpath上进来的连接. 它使用groupname和octalmode（八进制模式的权限设置） 去更改通信端口的拥有组和权限。 这个选项只有在QEMU编译时有了对VDE的支持后才可用。
  - `-net dump[,vlan=n][,file=file][,len=len]` : 转存（dump） 出VLAN中的网络数据

    将编号为n的VLAN中的网络流量转存（dump） 出来保存到file文件中（默认为当前目录中的qemu-vlan0.pcap文件）. 最多截取并保存每个数据包中的前len（默认值为64） 个字节的内容. 保存的文件格式为libcap， 故可以使用tcpdump、 Wireshark等工具来分析转存出来的文件.
  - `-net none` : 不分配任何网络设备

    单独使用它时, 表示不给客户机配置任何网络设备，可用于覆盖默认的`-net nic-net user`

4. 直接分配网络设备从而直接接入物理网络（包括VT-d和SR-IOV）

> 当前sr-iov不支持vm在线迁移, 在单机虚拟化场景可用它解决性能以及vm间资源平衡分配的问题.

qemu使用`-net`进行网络配置, 默认使用`-net nic-net user`即使用完全基于QEMU内部实现的用户模式下的网络协议栈. **在新的QEMU中, 推荐用-device+-netdev组合的方式**. 因为QEMU正逐渐规范统一的设备模型的参数使用, -device囊括了所有QEMU模拟的前端设备(qemu模拟出来的在guest中可见的设备, 它的实体是要靠一个对应的后端设备来实现的)的参数指定， 也就是客户机
里看到的设备（包括本章的网卡设备）; -netdev指定的是网卡模拟的后端方式， 也就是本节后面要讲的各种QEMU实现网络的方式.

> `qemu-system-x86_64 -device help`显示支持哪些device

> 通过`qemu-system-x86_64 -net nic,model=?`查看当前qemu支持模拟的网卡设备. `e1000`是qemu默认提供的intel e1000系列的网卡模拟. 在qemu monitor可用`info qtree/network`查看模拟网卡的详细信息

`-net nic[,vlan=n][,macaddr=mac][,model=type][,name=name][,addr=addr][,vectors=v]`选项:
- -net nic : 必需的参数, 表明这是一个网卡的配置
- vlan=n : 表示将网卡连入VLAN n， 默认为0（即没有VLAN）
- macaddr=mac， 设置网卡的MAC地址， 默认根据宿主机中网卡的地址来分配. 若局域网中客户机太多, 建议自己设置MAC地址, 以防止MAC地址冲突. 在实际使用
中建议配置自己的MAC地址, 否则QEMU会默认分配一个相同的MAC地址（`52:54:00:12:34:56`）, 此时几个虚拟机同时运行则可能会通过DHCP分配到相同的IP, 从而引发IP地址冲突.
- model=type : 设置模拟的网卡的类型, 默认为e1000
- name=name : 为网卡设置一个易读的名称， 该名称仅在QEMU monitor中可能用到
- addr=addr : 设置网卡在客户机中的PCI设备地址为addr
- vectors=v， 设置该网卡设备的MSI-X向量的数量为v， 该选项仅对使用virtio驱动的网卡有效. `vectors=0`是关闭virtio网卡的MSI-X中断方式

直接分配 by xml:
1. `virsh nodedev-list --tree/pci`查看pci设备, 获取该设备xml: `virsh nodedev-dumpxml pci_0000_07_00_0`
2. 编辑vm xml

  **因为是直接分配物理设备, vm中应有该设备的driver**

  ```bash
  <hostdev mode='subsystem' type='pci' managed='yes'>
   <source>
      <address domain='0' bus='0' slot='25' function='0'/> # 来自nodedev-dumpxml
   </source>
 </hostdev>
  ```

### 图形显示
在QEMU模拟器中的图形显示默认使用的就是SDL, 需要在编译QEMU时配置对SDL的支持. 它也有局限性, 那就是在创建客户机并以SDL方式显示时会直接弹出一个窗口， 所以SDL方式只能在图形界面中使用.

> SDL（Simple DirectMedia Layer） 是一个用C语言编写的、 跨平台的、 免费和开源的多媒体程序库， 它提供了一个简单的接口用于操作硬件平台的图形显示、 声音、 输入设备等.

> QEMU默认使用Ctrl+Alt组合键来实现鼠标在客户机与宿主机中的切换

> QEMU命令行提供了"-no-quit"参数来去掉SDL窗口的直接关闭功能, 避免误操作而关闭guest. 在加了"-no-quit"参数后， SDL窗口中的"关闭"按钮的功能将会失效， 而最小化、
最大化（或还原） 等功能正常

#### vnc
VNC（Virtual Network Computing） 是图形化的桌面分享系统， 它使用RFB（Remote FrameBuffer） 协议来远程控制另外一台计算机系统. 尽管QEMU仍然采用SDL作为默认的图形显示方式， 但VNC的管理方式在虚拟化环境中使用得更加广泛.

在qemu命令行中， 添加"-display vnc=displayport"参数就能让VGA显示输出到VNC会话中而不是SDL中. 如果在进行QEMU编译时没有SDL的支持， 有VNC的支持， 则qemu命令行在启动客户机时不需要"-vnc"参数也会自动使用VNC而不是SDL.

displayport参数取值:
- host： N

  表示仅允许从host主机的N号显示窗口来建立TCP连接到客户机。 在通常情况下，QEMU会根据数字N建立对应的TCP端口， 其端口号为5900+N。 而host值在这里是一个主机名或一个IP地址， 是可选的， 如果host值为空， 则表示QEMU建立的Server端接受来自任何主机的连接。 增加host参数值， 可以阻止来自其他主机的VNC连接请求， 从而在一定程度上提高了使用QEMU的VNC服务的安全性。
- to=L

  QEMU在上面指定的端口（5900+N） 已被被其他应用程序占用的情况下， 会依次向后递增尝试。 这里to=L， 就表示递增到5900+L端口号为止， 不再继续往后尝试。 默认为0， 即不尝试.
- unix： path

  表示允许通过Unix domain socket连接到客户机， 而其中的path参数是一个处于监听状态的socket的位置路径。 这种方式不常用.
- none

  表示VNC已经被初始化， 但是并不在开始时启动。 而在需要真正使用VNC之时， 可以在QEMU monitor中用change命令启动VNC连接

作为可选参数的option则有如下几个可选值， 每个option标志用逗号隔开:
1. reverse

  表示"反向"连接到一个处于监听中的VNC客户端， 这个客户端是由前面的display参数（host： N） 来指定的。 需要注意的是， 在反向连接这种情况下， display中的端口号N是对端（客户端） 处于监听中的TCP端口， 而不是现实窗口编号， 即如果客户端（IP地址为IP_Demo） 已经监听的命令为"vncviewer-listen： 2"， 则这里的VNC反向连接的参数为"-vnc IP_Demo： 5902， reverse"， 而不是用2这个编号
1. password

  表示在客户端连接时需要采取基于密码的认证机制， 但这里只是声明它使用密码验证， 其具体的密码值必须在QEMU monitor中用change命令设置
1. "tls""x509=/path/to/certificate/dir""x509verify=/path/to/certificate/dir""sasl"和"acl"

  这5个选项都是与VNC的验证、 安全相关的选项

> QEMU monitor中的"change vnc xxx"命令可在guest设置VNC参数并启动后动态地修改VNC的参数.

> VNC显示中的鼠标偏移: 窗口显示2个鼠标且通常不重合. 解决方法: 将"-usb"和"-usbdevice tablet"这两个USB选项一起使用. "tablet"类型的设备是一个使
用绝对坐标定位的指针设备， 就像在触摸屏中那样定位， 这样可以让QEMU能够在客户机不抢占鼠标的情况下获得鼠标的定位信息. 在最新的QEMU中， 与"-usb-usbdevice tablet"参数功能相同， 也可以用"-device piix3-usb-uhci-device usb-tablet"参数.

在qemu命令行中， 添加"-nographic"参数可以完全关闭QEMU的图形界面输出， 从而让QEMU在该模式下成为简单的命令行工具. 此时需要修改客户机的grub配置(by kernel boot args:`console=ttyS`)， 使其在kernel行中加上将console输出重定向到串口ttyS0.

其他图形选项:
-curses

  让QEMU将VGA显示输出到使用curses/ncurses接口支持的文本模式界面， 而不是使用SDL来显示客户机. 与"-nographic"模式相比， 它的好处在于， 由于它是接收客户机VGA的正常输出而不是串口的输出信息， 因此不需要额外更改客户机配置将控制台重定向到串口. 为了使用"-curses"选项，在宿主机中必须有"curses或ncurses"这样的软件包提供显示接口的支持并编译QEMU时启用了它.
- -vga type

  选择为客户机模拟的VGA卡的类别， 可选类型有如下6种:
  1. cirrus

    为客户机模拟出"Cirrus Logic GD5446"显卡， 在客户机启动后， 可以在客户机中看到VGA卡的型号， 如在Linux中可以用"lspci"查看到VGA卡的信息。 这个选项对图形显示的体验并不是很好， 它的彩色是16位的， 分辨率也不高， 仅支持2D显示， 不支持3D。 不过绝大多数的系统（包括Windows 95） 都支持这个系列的显卡
  1. std

    模拟标准的VGA卡， 带有Bochs VBE扩展。 当客户机支持VBE 2.0及以上标准时（ 目前流行的操作系统多数都支持） ， 如果需要支持更高的分辨率和彩色显示深度， 就会使用这个选项.
  1. VMware

    提供对"VMware SVGA-II"兼容显卡的支持
  1. virtio

    半虚拟化的VGA显卡模拟
  1. qxl

    它也是一种半虚拟化的模拟显卡， 与VGA兼容。 当使用spice display的时候， 推荐选择这种显卡.
  1. none

    关闭VGA卡， 使SDL或VNC窗口中无任何显示。 一般不使用这个参数.
- -no-frame

  使SDL显示时没有边框
- -full-screen

  在启动客户机时， 自动使用全屏显示
- -alt-grab

  使用"Ctrl+Alt+Shift"组合键去抢占和释放鼠标， 从而使"Ctrl+Alt+Shift"组合键成为QEMU中的一个特殊功能键。 在QEMU中默认使用"Ctrl+Alt"组合键， 所以本书常提到在SDL或VNC中用"Ctrl+Alt+2"组合键切换到QEMU monitor的窗口， 而使用了"-alt-grab"选项后， 应该相应改用"Ctrl+Alt+Shift+2"组合键切换到QEMU monitor窗口。
- -ctrl-grab

  使用右"Ctrl"键去抢占和释放鼠标， 使其成为QEMU中的特殊功能键。 这与前面的"-alt-grab"的功能类似

## vt-d
在QEMU/KVM中， 客户机可以使用的设备大致可分为如下3种类型:
- Emulated device： QEMU纯软件模拟的设备， 比如-device rt8139等
- virtio device： 实现VIRTIO API的半虚拟化驱动的设备， 比如-device virtio-net-pci等
- PCI device assignment： PCI设备直接分配

> 较新的x86架构的主要硬件平台（包括服务器级、 桌面级） 都已经支持设备直接分配， 其中Intel定义的I/O虚拟化技术规范为“Intel(R)Virtualization Technology for
Directed I/O”（VT-d） ， 而AMD的I/O虚拟化技术规范为“AMD-Vi”（也叫作IOMMU）.

为了设备分配的安全性， 还需要中断重映射（interruptremapping） 的支持, qemu进行设备分配时不直接检查是否开启了中断重映射功能, 但libvirt需默认开启才能使用VT-d分配设备供客户机使用.

> 对于使用VT-d直接分配了设备的客户机， 其动态迁移功能将会受限， 不过也可以用热插拔或libvirt工具等方式来缓解这个问题.

BIOS中VT-d设置选项一般为“Intel(R)VT for Directed I/O”或“IntelVT-d”等, host需开启它.

VT-d相关kernel配置:
```conf
CONFIG_GART_IOMMU=y #AMD平台相关
# CONFIG_CALGARY_IOMMU is not set #IBM平台相关
CONFIG_IOMMU_HELPER=y
CONFIG_VFIO_IOMMU_TYPE1=m
CONFIG_VFIO_NOIOMMU=y
CONFIG_IOMMU_API=y
CONFIG_IOMMU_SUPPORT=y
CONFIG_IOMMU_IOVA=y
CONFIG_AMD_IOMMU=y #AMD平台的IOMMU设置
CONFIG_AMD_IOMMU_STATS=y
CONFIG_AMD_IOMMU_V2=m
CONFIG_INTEL_IOMMU=y #Intel平台的VT-d设置
# CONFIG_INTEL_IOMMU_DEFAULT_ON is not set#Intel平台的VT-d是否默认打开。 这里没有选上， 需要在kernel boot parameter中加上“
CONFIG_INTEL_IOMMU_FLOPPY_WA=y
# CONFIG_IOMMU_DEBUG is not set
# CONFIG_IOMMU_STRESS is not set
```

为了启用vfio还需配置vfio-pci(在RHEL 7默认内核中， 都将CONFIG_KVM_VFIO配置为y（直接编译到内核） ， 其他的功能配置为模块（m）):
```conf
CONFIG_VFIO_IOMMU_TYPE1=m
CONFIG_VFIO=m
CONFIG_VFIO_NOIOMMU=y #支持用户空间的VFIO框架
CONFIG_VFIO_PCI=m
# CONFIG_VFIO_PCI_VGA is not set #这个是for显卡的VT-d
CONFIG_VFIO_PCI_MMAP=y
CONFIG_VFIO_PCI_INTX=y
CONFIG_KVM_VFIO=y
```

host执行`dmesg | grep -i dmar`, 看到`DMAR： IOMMU enabled`和`DMAR-IR: Enabled IRQ remapping in xxx mode`(xxx可能是x2apic或xapic), 因为VT-d启用后kernel会在boot时做DMA Remapping. 其他方法:
```bash
# --- [Check if VT-D / IOMMU has been enabled in the BIOS/UEFI](https://stackoverflow.com/questions/51261999/check-if-vt-d-iommu-has-been-enabled-in-the-bios-uefi)
# --- 1
if systool -m kvm_amd -v &> /dev/null || systool -m kvm_intel -v &> /dev/null ; then
    echo "AMD-V / VT-X is enabled in the BIOS/UEFI."
else
    echo "AMD-V / VT-X is not enabled in the BIOS/UEFI"
fi
# --- 2
if compgen -G "/sys/kernel/iommu_groups/*/devices/*" > /dev/null; then
    echo "AMD's IOMMU / Intel's VT-D is enabled in the BIOS/UEFI."
else
    echo "AMD's IOMMU / Intel's VT-D is not enabled in the BIOS/UEFI"
fi
```

### vfio
从Kernel 3.10发布， VFIO正式被引入， 取代了原来的pci-stub的VT-d方式.

与Legacy KVM Device Assignment（使用pci-stub driver） 相比， [VFIO（VirtualFunction IO）](http://www.linux-kvm.org/images/b/b4/2012-forum-VFIO.pdf) 最大的改进就是隔离了设备之间的DMA和中断， 以及对IOMMU Group的支持， 从而有了更好的安全性. IOMMU Group可以认为是对PCI设备的分组， 每个group里面的设备被视作IOMMU可以操作的最小整体； 换句话说， 同一个IOMMU Group里的设备(可在/sys/kernel/iommu_groups/目录下查看有哪些IOMMU Group以及所属的设备)不可以分配给不同的客户机. 在以前的Legacy KVM Device Assignment中， 并不会检查这一点， 而后面的操作却注定是失败的, 新的VFIO会检查并及时报错.

另外, 新的VFIO架构也做到了平台无关, 有更好的可移植性.

在host需要使用vfio_pci这个内核模块来对需要分配给客户机的设备进行隐藏， 从而让宿主机和未被分配该设备的客户机都无法使用该设备， 达到隔离和安全使用的目的.

隐藏设备:
```bash
# modprobe vfio_pci
# ls /sys/bus/pci/drivers/vfio-pci/
# lspci -s 03:10.3 -Dn # 查看设备
0000:03:10.3 0200: 8086:1520 (rev 01)
# lspci -s 03:10.3 -k # 查看设备驱动, 如不是vfio-pci， 则解绑当前驱动， 然后绑定到vfio-pci上. 在客户机不需要使用该设备后， 让宿主机使用该设备， 则需要将其恢复到使用原本的驱动
03:10.3 Ethernet controller: Intel Corporation I350 Ethernet Controller Virtual Function (rev 01)
Subsystem: Intel Corporation Device 35c4
Kernel driver in use: igbvf
Kernel modules: igbvf
# echo 0000:03:10.3 > /sys/bus/pci/drivers/igbvf/unbind
# echo -n "8086 1520" > /sys/bus/pci/drivers/vfio-pci/new_id
# lspci -s 03:10.3 -k
03:10.3 Ethernet controller: Intel Corporation I350 Ethernet Controller Virtual Function (rev 01)
Subsystem: Intel Corporation Device 35c4
Kernel driver in use: vfio-pci
Kernel modules: igbvf
```

> 如果vfio_pci已被编译到内核而不是作为module， 则仅需最后一个命令来检查/sys/bus/pci/drivers/vfio-pci/目录是否存在即可.

> 隐藏和恢复pcie设备的script可见<<KVM实战>>的6.2.3"VT-d环境配置"

qemu分配vfio-pci设备`-device driver[,prop[=value][,...]]`:
- driver是设备使用的驱动

  有很多种类， 如vfio-pci表示VFIO方式的PCI设备直接分配， virtio-balloon-pci（又为virtio-balloon） 表示ballooning设备（等同于"-balloon virtio"）
- prop[=value]是设置驱动的各个属性值

  `-device vfio-pci,host=03:10.3,addr=08`的属性中， host属性指定分配的PCI设备在宿主机中的地址（BDF号）, addr属性表示设备在客户机中的PCI的slot编号（即BDF中的D-device的值）

> `qemu-system-x86_64 -device help`可查看可用的驱动, `qemu-system-x86_64 -device vfio-pci,help`可查看其prop

由于设备直接分配是客户机独占该设备， 因此一旦将一个设备分配给客户机使用， 就不能再将其分配给另外的客户机使用了. QEUM/KVM还支持设备的热插拔（hot-plug） ， 在客户机运行时添加所需的直接分配设备, 这需要在QEMU monitor中运行相应的命令.

> qemu monitor可通过`info pci`查看分配给guest的pci设备的情况

> qemu使用`-net none`表示不使用其他的网络设备但直接分配的网卡除外, 否则guest中会出现一个直接分配的网卡和一个emulated的网卡.

将SATA或SAS设备作为PCI设备直接分配时, 建议将其控制器也作为一个整体分配到客户机中.

USB直接分配也是指对整个USB Host Controller的直接分配， 而并不一定仅分配一个USB设备. 另外, 也有其他的命令行参数（-usbdevice） 来支持USB设备的分配. 不同于对USB Host Controller的直接分配， -usbdevice参数用于分配单个USB设备: 先用lsusb找到设备, 再使用`-usbdevice host:xx`来分配

### SR-IOV
> 主板集成的网卡需bios开启SR-IOV, 一般默认是关闭的.

PCI-SIG组织发布了SR-IOV（Single Root I/OVirtualization and Sharing） 规范, 该规范定义个了一个标准化的机制， 用以原生地支持实现多个共享的设备（不一定是网卡设备）. 不过， 目前SR-IOV（单根I/O虚拟化） 最广泛的应用还是在以太网卡设备的虚拟化方面.

> 一个设备可支持多个VF， PCI-SIG的SR-IOV规范指出每个PF最多能拥有256个VF, 而实际支持的VF数量是由设备的硬件设计及其驱动程序共同决定的. 比如使用“igb”驱动的82576、 I350等千兆（1G） 以太网卡的每个PF支持最多7个VF，而使用“ixgbe”驱动的82599、 X540等万兆（10G） 以太网卡的每个PF支持最多63个VF.

SR-IOV中引入的两个新的功能（function） 类型
1. Physical Function（PF， 物理功能） ： 拥有包含SR-IOV扩展能力在内的所有完整的PCI-e功能， 其中SR-IOV能力使PF可以**配置和管理SR-IOV功能**. 简言之， PF就是一个
普通的PCI-e设备（带有SR-IOV功能） ， 可以放在宿主机中配置和管理其他VF， 它本身也可以作为一个完整独立的功能使用.
2. Virtual Function（VF， 虚拟功能） ： 由PF衍生而来的“轻量级”的PCI-e功能， 包含数据传送所必需的资源， 但是仅谨慎地拥有最小化的配置资源。 简言之, VF通过PF的配
置之后， 可以分配到客户机中作为独立功能使用.

SR-IOV为客户机中使用的VF提供了独立的内存空间、 中断、 DMA流， 从而不需要Hypervisor介入数据的传送过程。 SR-IOV架构设计的目的是允许一个设备支持多个VF, 同时也尽量减小每个VF的硬件成本.

一个具有SR-IOV功能的设备能够被配置为在PCI配置空间（configuration space） 中呈现出多个Function（包括一个PF和多个VF）, 每个VF都有自己独立的配置空间和完整的
BAR（Base Address Register， 基址寄存器）. Hypervisor通过将VF实际的配置空间映射到客户机看到的配置空间的方式， 实现将一个或多个VF分配给一个客户机. 通过Intel VT-x和VT-d等硬件辅助虚拟化技术提供的内存转换技术， 允许直接的DMA传输去往或来自一个客户机， 从而绕过了Hypervisor中的软件交换机（software switch）. 每个VF在同一个时刻只能被分配到一个客户机中, 因为VF需要真正的硬件资源（不同于emulated类型的设备）. 在客户机中的VF表现给客户机操作系统的就是一个完整的普通的设备.

在KVM中, 可以将一个或多个VF分配给一个客户机, 客户机通过自身的VF驱动程序直接操作设备的VF而不需要Hypervisor（即KVM） 的参与.

为了让SR-IOV工作起来， 需要硬件平台支持Intel VT-x和VT-d（或AMD的SVM和IOMMU） 硬件辅助虚拟化特性， 还需要有支持SR-IOV规范的设备， 当然也需要
QEMU/KVM的支持。 支持SR-IOV的设备较多， 其中Intel有很多中高端网卡支持SR-IOV特性， 如Intel 82576网卡（代号“Kawella”， 使用igb驱动） 、 I350网卡（igb驱动） 、 82599网卡（代号“Niantic”， 使用ixgbe驱动） 、 X540（使用ixgbe驱动） 、 X710（使用i40e驱动） 等.


通过`lspci -s 03:00.0 -v |grep Capabilities|grep SR-IOV`确定设备是否支持SR-IOV.
通过`cat /sys/bus/pci/devices/0000\:03\:00.0/sriov_totalvfs`查看支持多少VF; 通过`cat /sys/bus/pci/devices/0000\:03\:00.0/sriov_numvfs`查看当前有多少VF; 通过`ls -l /sys/bus/pci/devices/0000\:03\:00.3/virtfn*`可查看PV和VF的所属关系.
可通过sysfs动态生成及增减VF, 比如`echo 5 > /sys/bus/pci/devices/0000\:03\:00.3/sriov_numvfs`即创建5个VF.

> 通过PF驱动（如igb， ixgbe） 加载时也可指定max_vfs参数, 比如`modprobe -r igb; modprobe igb max_vfs=7`, **不推荐**. 同理可配置`vim /etc/modprobe.d/igb.conf`为`option igb max_vfs=7`让系统加载驱动时候自动带上max_vfs参数.

vm xml:
```xml
<interface type='hostdev' managed="yes">
      <driver name='vfio'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
      </source>
</interface>
```

vm需要安装网卡驱动才能使用vf, 高版本linux可自动识别vf; windows 2008 server及以上版本+intel网卡驱动才支持vf.


使用SR-IOV主要有如下3个优势：
1. 真正实现了设备的共享（多个客户机共享一个SR-IOV设备的物理端口）
2. 接近于原生系统的高性能（比纯软件模拟和virtio设备的性能都要好）, vm绕过虚拟化层和系统, 直接访问硬件, 没有虚拟化层软件模拟的开销
3. 相比于VT-d， SR-IOV可以用更少的设备支持更多的客户机， 可以提高数据中心的空间利用率. 降低成本, 减少设备数量, 比如网卡的SR-IOV减少了网卡数量, 交换机端口, 网线

而SR-IOV的不足之处有如下两点：
1. 对设备有依赖， 只有部分PCI-E设备支持SR-IOV
2. 使用SR-IOV时， 不方便动态迁移客户机

SR-IOV使用问题解析:
- VF在客户机中MAC地址全为零

  host是kernel 3.9且使用igb或ixgbe驱动的网卡做VF时可能碰到, 解决方法:
  1. 在分配VF给客户机之前， 在宿主机中用ip命令来设置需要使用的VF的MAC地址, 比如`ip link set eno2 vf 0 mac 52:54:00:56:78:9a`, 其中eno2是PF, `vf 0`表示使用编号为0的VF.
  1. 升级客户机系统的内核或VF驱动程序

    比如可以将Linux客户机升级到使用Linux 3.9之后的内核及其对应的igbvf驱动程序. 最新的igbvf驱动程序可以处理VF的MAC地址为零的情况
- Windows客户机中关于VF的驱动程序

  有少数的最新Windows系统（如Windows 8、 Windows 2012 Server等） 默认带有这些网卡驱动， 而多数的Windows系统（如Windows 7、 Windows 2008 Server等）都没有默认带有相关的驱动，需要自行下载和安装.
- 少数网卡的VF在少数Windows客户机中不工作
  
  qemu默认的CPU模型是qemu64, 它不支持MSI-X这种中断方式, 而32位的Windows 2008 Server版本中的82576、 82599网卡的VF只能用MSI-X中断方式来工作. 可指定cpu model来避免该问题, 用“-cpu SandyBridge”“-cpu Westmere”等参数来指定CPU类型， 也可以用“-cpu host”参数来尽可能多地将物理CPU信息暴露给客户机， 还可以用“-cpu qemu64， model=13”来改变qemu64类型CPU的默认模型.

  [个别版本的VF（如： Intel I350） 在个别版本的Windows（如：Windows 2008 Server） 中不工作.](https://bugzilla.kernel.org/show_bug.cgi?id=56981)


### openswitch
基本概念:
- bridge : 代表一个以太网交换机(switch), 一个主机中可以创建一个或者多个bridge设备
- port: 相当于物理交换机的端口, 每个port隶属于一个bridge
- interface: 连接到port的网络接口设备. 在通常情况下, port和interface是一对一的关系. 只有在配置port为bond模式后, port和interface才是一对多的关系
- controller: openflow控制器. ovs可以同时接受一个或多个openflow控制器的管理
- datapath: 在ovs中, datapath负责执行数据交换, 也就是把从接收端口收到的数据包在流表中进行匹配, 并执行匹配到的动作
- flow table: 每个datapath都和一个flow table关联, 当datapath收到数据之后, ovs会在flow table中查找匹配的flow, 执行对应的操作, 比如转发数据到另外端口.

源码安装:
```
yum install gcc kernel-devel
wget http://kvm.org/releases/kvm-2.3.0.tar.gz
tar -xf kvm-2.3.0.tar.gz && cd kvm-2.3.0
./configure --with-linux=/lib/modules/`uname -r`/build
make && make install

mkdir -p /usr/local/etc/kvm
ovsdb-tool create /usr/local/etc/kvm/conf.db # 创建db
ovsdb-server --remote=punix:/usr/local/etc/kvm/db.sock --pidfile --detach # 启动db
ovs-vsctl --no-wait init # init db
ovs-vswitchd --pidfile --detach # 启动ovs-vswitchd
ps -ef |grep ovs # 检查ovsdb-server和ovs-vswitchd 是否已启动
```

**推荐**使用yum安装, 并可使用fedora openstack-grizzly的源, 执行`yum install kvm`

配置:
```bash
ovs-vsctl add-br br0
ovs-vsctl add-port br0 eth1 # 把eth1加入br0
vim vm.xml
<interface type='bridge'>
 <mac address='52:54:00:71:b1:b6'/>
 <source bridge='br0'/>
 <virtualport type='openvswitch'/>
 <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>
# - 启动vm后, 查看ovs配置
ovs-vsctl show
```

在openswitch中, port默认是trunk模式, 除非指定其vlan tag. `ovs-vsctl set port tap1 tag=3`可在线修改vm的vlan

openswitch bond配置:
```bash
ovs-vsctl add-br br0
ovs-vsctl add-port br0 bond0 eth2 eth3 lacp=active
ovs-vsctl set port bond0 bond_mode=balance-slb
ovs-appctl bond/show bond0
```
openswitch bond模式:
1. active-backup: 主备模式
1. balance-slb: 将流量根据源mac和输出vlan id在物理网卡之间进行负载均衡
1. balance-tcp: 与balance-slb类似, 处理根据l2的信息外, 还可根据l3, l4的信息进行负载均衡, 比如ip地址, tcp端口. 此时上游交换机支持802.3ad且lacp协商成功, 否则回退到balance-slb.

lacp支持:
- active

  交换机对应端口配置了port channel, 此时必须是active.
- passive
- off

连接两个openvswitch的bridge:
- veth(virtual ethernet)

  ```bash
  ovs-vsctl add-br br0
  ovs-vsctl add-br br1
  ip link add name veth0 type veth peer name veth1
  ovs-vsctl add-port br0 veth0
  ovs-vsctl add-port br1 veth1
  ```
- openvswitch的patch类型端口

  ```bash
  ovs-vsctl add-br br0
  ovs-vsctl add-br br1
  ovs-vsctl add-port br0 patch-to-br0
  ovs-vsctl set interface patch-to-br0 type=patch
  ovs-vsctl set interface patch-to-br0 options:patch-to-br1
  ovs-vsctl add-port br0 patch-to-br1
  ovs-vsctl set interface patch-to-br1 type=patch
  ovs-vsctl set interface patch-to-br1 options:patch-to-br0
  ```

## 热插拔
热插拔（hot plugging） 即“带电插拔”， 指可以在计算机运行时（不关闭电源） 插上或拔除硬件. 热插拔最早出现在服务器领域, 目的是提高服务器扩展性、 灵活性和对灾难的
及时恢复能力. 实现热插拔需要有几方面支持: 总线电气特性、 主板BIOS、 操作系统和设备驱动.

在KVM虚拟化环境中， 在不关闭客户机的情况下， 也可以对客户机的设备进行热插拔. 目前， 主要支持PCI设备、 CPU(仅支持插入)、 内存的热插拔.

### pci设备热插拔
QEMU/KVM支持动态添加和移除各种PCI设备， 包括QEMU模拟的virtio类别的以及VT-d直接分配的.

PCI设备的热插拔主要需要如下几个方面的支持:
1. BIOS

  QEMU/KVM默认使用SeaBIOS作为客户机的BIOS， 该BIOS文件路径一般为/usr/local/share/qemu/bios.bin. 目前默认的BIOS已经可以支持PCI设备的热插拔.
1. PCI总线

  （对于VT-d传入的设备） 物理硬件中必须有VT-d的支持， 而且现在的PCI、 PCIe总线都支持设备的热插拔
1. 客户机操作系统

  多数流行的Linux和Windows操作系统都支持设备的热插拔

  kernel配置:
  ```conf
  CONFIG_HOTPLUG_PCI_PCIE=y
  CONFIG_HOTPLUG_PCI=y
  CONFIG_HOTPLUG_PCI_ACPI=y
  CONFIG_HOTPLUG_PCI_ACPI_IBM=m
  CONFIG_HOTPLUG_PCI_SHPC=m
  ```
1. 客户机中的驱动程序

  一些网卡驱动（如Intel的e1000e、 igb、 ixgbe、 igbvf、 ixgbevf等） 、 SATA或SAS磁盘驱动、 USB2.0、 USB3.0驱动都支持设备的热插拔. 较新内核的Linux系统（如RHEL 6以后、 Fedora 17以后等） 默认启动的系统就支持设备热插拔.

热插拔功能只需要在QEMU monitor中使用两个命令即可完成:
```bash
device_add vfio-pci,host=02:00.0,id=mydevice # 将一个BDF为02： 00.0的PCI设备动态添加到客户机中（设置id为mydevice）. mydevice是在添加设备时设置的唯一标识， 可以通过“info pci”命令在QEMU monitor中查看到当前的客户机中的PCI设备及其id值
device_del mydevice # 将一个设备（id为mydevice） 从客户机中动态移除
```

### usb
对于USB设备， 使用两个专门的命令（usb_add和usb_del） 对单个USB设备进行热插拔操作。 当然， 还可以用“device_add”和“device_del”将USB根控制器（它是一个PCI设
备） 连带它上面的所有USB设备一并热插拔.

```bash
usb_add host:003.003 # 003.003取自host的lsusb
usb_add host:03f0:8607 # devname是对该USB设备的唯一标识， usb_add支持两种devname的格式： 一种是USB hub中的Bus和Device号码的组合, 一种是USB的vendor ID和device ID的组合
info usb
usb_del 0.2 # 0.2取自info usb
```

> QEMU的usb_add/del热插拔， 包括启动时（-usbdevice host） 指定， 都依赖于`libusbx-devel(但2014年以后, libusbx已经并回libusb开发分支)`+qemu编译参数`--enable-libusb`.

> QEMU默认没有向客户机提供USB总线， 需要在启动客户机的qemu命令行中添加“-usb”参数（或“-device piix4-usb-uhci”参数）, 来提供客户机中的USB总线

> **对于使用device_add命令动态添加的USB设备， 则应使用如下device_del命令将其移除**

### cpu
CPU和内存的热插拔是RAS（Reliability、 Availability和Serviceability） 的一个重要特性， 在非虚拟化环境中， 只有较少的x86服务器硬件支持CPU和内存的热插拔.

在操作系统方面， 拥有较新内核的Linux系统（如RHEL 7）,windows(2008数据中心版, 2012标准版/数据中心版) 等已经支持CPU和内存的热插拔， 在其内核配置文件与CPU热插拔有关的项:
```conf
CONFIG_HOTPLUG_CPU=y
CONFIG_BOOTPARAM_HOTPLUG_CPU0=y
# CONFIG_DEBUG_HOTPLUG_CPU0 is not set
CONFIG_ACPI_HOTPLUG_CPU=y
```

qemu使用`-smp <n>,maxvcpus=<N>`即可开启cpu热插拔即通过maxvcpus预留cpu. 在guest中可用`ls /sys/devices/system/cpu/cpu<N>/online`查看cpu状态.

使用方法:
1. virsh

  ```bash
  # --- 假设原先vcpu=4
  # virsh setvcpus centos7 5 --live # on host
  # echo 1 > /sys/devices/system/cpu/cpu4/online # 激活cpu, on guest
  # echo 0 > /sys/devices/system/cpu/cpu4/online # 关闭cpu, 但不支持在host上减少cpu个数即`virsh setvcpus centos7 4 --live`会报错
  ```

  > windows热添加cpu后无需特殊配置
2. qemu monitor 

  在qemu monitor可用`cpu-add <cpu_id>`来添加cpu. 需注意: 1. qemu没有cpu-del即只能插入; 2. cpu-add cpu_id, 必须顺序加入, 不能乱序, 否则会影响动态迁移.

### 内存
virtio_balloon可以认为是早期的间接实现内存热插拔的功能. 但其实并没有热插拔, 而是动态增减内存大小, 并且依赖于virtio_balloon的驱动, 对客户机来说并没有硬件上的增减. 真正的热插拔是指内存设备（DIMM） 的插拔.

内核社区将内存的热插拔分为两步骤： 物理内存热插拔（Physical Memory Hotplug）和逻辑内存热插拔（Logical Memory Hotplug）. 前者指对物理的内存条插拔的支持， 后
者指物理内存作为内核内存管理系统可以使用的资源， 被动态地加入或踢出的支持。 内存热插拔的过程是： 物理内存热插入→逻辑内存热添加→逻辑内存热删除→物理内存热拔出. 目前， Linux kernel对于这4个步骤都已支持（除了逻辑内存热删除有一点局限性).

物理内存热插拔的支持， 主要依赖于ACPI的功能. 逻辑内存热插拔的支持, 需要对原来的内存管理子系统的功能进行增补:
1. 新增了ZONE_MOVABLE, 与原来ZONE_NORMAL、 ZONE_DMA、ZONE_HIGH-MEM并列. ZONE_MOVABLE就专门管理movable（可以动态移除的页）
1. kernel的启动参数， 新增了kernelcore和movablecore， 以及movable_node这3个参数. kernelcore指定系统boot起来时， 分配多少内存作为kernel page， 剩下的都作为movable page. movablecore就是反过来， 指定多少作为movable page， 剩下的都是kernel page. movable_node是指定是否需要这样一个memory node专门放movable zone.
1. /sys/device/system/memory下面的内存设备管理的接口， 如新增valid_zone等接口
1. 其他内核相关变更

> kernel page和movable page是两个相对的概念. 内核对逻辑内存热插拔的支持， 是将要物理拔出的内存条上承载的逻辑页先迁移（migate） 到别的内存条所承载的逻辑页上， 然后将前面的目标逻辑页free出来， 才能安全地物理拔出。 所以， 要对逻辑内存（页） 进行归类， 哪些是可以被移除的， 称为movablepage， 通常是匿名页和page cache； 那些是不能移除的， 就称为kernel page.

在/sys/device/system/memory下面可以看到很多memoryN这样的子目录， 这是因为kernel是以memory block为单位管理物理内存的. 每个block的大小(/sys/devices/system/memory/block_size_bytes, 输出是16进制)根据平台可能会有所不同, 在x86_64环境中， 通常是128MB.

guest kernel内存热插拔配置:
```bash
CONFIG_MEMORY_HOTPLUG=y
CONFIG_SPARSEMEM=y
CONFIG_ACPI_HOTPLUG_MEMORY=y
CONFIG_MEMORY_HOTPLUG_SPARSE=y
CONFIG_MEMORY_HOTREMOVE=y
# CONFIG_MEMORY_HOTPLUG_DEFAULT_ONLINE is not set
CONFIG_ARCH_ENABLE_MEMORY_HOTPLUG=y
CONFIG_ARCH_ENABLE_MEMORY_HOTREMOVE=y
CONFIG_MIGRATION=y
CONFIG_ARCH_ENABLE_HUGEPAGE_MIGRATION=y
```

QEMU/KVM中对内存热插拔的支持主要是通过对dimm设备的热插拔的支持来实现的， 对客户机的内核来说， 就相当于物理地插入和拔出内存条一样. dimm设备的热插拔与PCI设备一样， 通过“device_add”来完成. 在device_add之前, 要通过object_add来定义这个后端设备， 它的名字叫“memory-backend-ram”

> 除了“memory-backend-ram”这个object类型以外， 还有一个类似的“memorybackend-file”， 其实这个更早被支持， 就是用宿主机里的一个文件（可以是普通文件， 也
可以是hugetlbfs） 作为前端dimm设备的后端.

qemu使用`-m <N>G, slots=x,maxmem=yy`来开启内存热插拔, 其中表示当前Ng内存大小, 可供热插拔的内存插槽一共有多少， 最大可以增加到多大内存.

qemu monitor操作:
```bash
(qemu) object_add memory-backend-ram,id=mem1,size=1G # 添加后端设备
(qemu) device_add pc-dimm,id=dimm1,memdev=mem1 # 添加前端设备
(qemu) info memory-devices # 查看内存设备
# --- 反向操作删除内存设备, 此时guest dmesg会提示若干`Offlined Pages xxx`表示逻辑内存拔出的步骤
(qemu) device_del dimm1
(qemu) object_del mem1
```

上述操作在guest `/sys/devices/system/memory/memory*`会变多, 且多出的`/sys/devices/system/memory/memory<N>/state`是online, 且`cat /sys/devices/system/memory/memory<N>/removable`是1表示可移除, 通过`cat /sys/devices/system/memory/memory<N>/valid_zones`可见新加入的内存页都默认归入了NORMAL
zone.

### 磁盘的热插拔
在guest中的磁盘一般在宿主机中表现为raw/qcow2等格式的一个文件.

qemu monitor操作:
```bash
(qemu) drive_add 0 file=/root/hotplug-10G.img,format=qcow2,id=drive-disk1,if=none
OK #这个OK是命令执行成功后的输出信息
(qemu) device_add virtio-blk-pci,drive=drive-disk1,id=disk1 # 将磁盘驱动器以virtio-blk-pci设备的形式添加到客户机中
(qemu) device_del disk1
```

> 较新的主流Linux发行版中的内核一般都将磁盘 hotplug的支持编译到内核中了, 配置为CONFIG_HOTPLUG_PCI_ACPI=y

libvirt和virsh即使用virsh attach-device、 virsh detach-device（或者attach-disk、 detach-disk） 这两个命令来实现磁盘的热插拔

### 网卡
qemu monitor操作:
```bash
(qemu) netdev_add user,id=net1 # 在host模拟一个网卡设备. 这里设备类型选择了最简单的user模式的网卡， 还有其他tap、 bridge、 vhost-user等可供选择
(qemu) device_add virtio-net-pci,netdev=net1,id=nic1,mac=52:54:00:12:34:56
(qemu) device_del nic1
```

libvirt和virsh使用virsh attach-device、 virsh detach-device（或者attach-interface、 detach-interface） 这两个命令来实现网卡的热插拔.

## kvm内存管理进阶
### 大页
x86（包括x86-32和x86-64） 架构的CPU默认使用4KB大小的内存页面, 但是它们也支持较大的内存页, 如x86-64系统就支持2MB及1GB大小的大页（Huge Page）.

Linux2.6及以上的内核都支持Huge Page. 启用Huge Page后内存页的数量会减少， 从而需要更少的页表（Page Table）, 节约了页表所占用的内存数量， 并且所需的地
址转换也减少了， TLB缓存失效的次数就减少了， 从而提高了内存访问的性能。 另外， 由于地址转换所需的信息一般保存在CPU的缓存中， Huge Page的使用让地址转换信息减
少， 从而减少了CPU缓存的使用， 减轻了CPU缓存的压力， 让CPU缓存能更多地用于应用程序的数据缓存， 也能够在整体上提升系统的性能.

> /proc/cpuinfo中cpuflag`pae pse`（ Phsycial Address Extension， Page Size Extension） 表示CPU硬件对2MB内存页的支持

> 较新的Intel CPU都支持1GB大页， 可以查看`/proc/cpuinfo`中cpuflag是否有pdpe1gb

巨页可提升内存分配效率, 提升系统性能; 坏处时必须手工配置, vm的数量, 可用内存, 启动, 关闭, 迁移都需要重新配置, 且不能使用swap.
透明巨页好处:
- 可以使用swap, 内存页默认大小是2M, 需要使用swap时, 内存被分割为4K
- 对用户透明, 不需要用户做特殊配置
- 不需要root
- 不需要依赖某种库文件

> centos 6支持透明巨页, 默认开启巨页, 并且自动配置. host和vm需一起开启THP.

transparent_hugepage配置:
- never : 关闭

  ```
  echo never > /sys/kernel/mm/transparent_hugepage/enabled
  ```
- alway : 尽量使用透明巨页, 扫描内存, 有512个4K页面可整和, 就整合成一个2M页面
- madvise : 避免改变内存占用

/sys/kernel/mm/transparent_hugepage/khugepaged:
- pages_to_scan(默认4096=16M) : 一个扫描周期被扫描的内存页数
- scan_sleep_millisecs(默认10000=10s) : 多少时间扫描一次
- alloc_sleep_millisecs(默认60000=60s) : 多长时间整理一次碎片

vm设置巨页数量:
```
<memoryBacking><hugepages/></memoryBacking>
```

huge page的kernel配置:
```bash
CONFIG_HUGETLBFS=y
CONFIG_HUGETLB_PAGE=y
```

内核启动时候的参数中， 与大页相关的如下：
- default_hugepagesz : 默认的大页的大小， 可以是2MB或者1GB
- hugepages : 内核启动后给系统准备的大页的数量
- hugepagesz : 内核启动后给系统准备的大页的大小， 可以是2MB或者1GB

hugepages和hugepagesz可以组合交替出现， 表示不同大小的大页分别准备多少.

大页kernel描述:
- /proc/meminfo : **只体现了`default_hugepagesz`的huge page信息**
- `/proc/sys/vm/`

  ```bash
  # ls -l /proc/sys/vm/*huge*
  -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugepages_treat_as_movable
  -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/hugetlb_shm_group
  -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages
  -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_hugepages_mempolicy
  -rw-r--r-- 1 root root 0 Jan 27 11:27 /proc/sys/vm/nr_overcommit_hugepages
  ```

  - hugepages_treat_as_movable

    这个参数设置为非0时， 表示允许hugepage从ZONE_MOVABLE里面分配. 但这有利有弊： 虽然这扩大了hugepage的来源，但它也增加了内存热拔出时的失败几率， 如果当时这个hugepage正在被用， 哪怕只用到了一小块， 它也不能当时被换出， 这样整个内存条也就不能拔出。 更极端的情况： 系统中没有其他的大页可以供这个大页的内容迁移， 则内存热拔出一直会失败。 所以， 这个参数默认是0.

    > ZONE_MOVABLE的创建依赖于内核启动参数kernelcore=xx, 如果没有ZONE_MOVABLE，则无论hugepages_treat_as_movable设置成什么， 也是没有意义的.
  - hugetlb_shm_group： 

    SysV共享内存段（ shared memory segment） 的id， 该内存段将使用大页. 常用的POSIX共享内存的方法用不到这个.
  - nr_hugepages_mempolicy： 通常与NUMA mempolicy（ 内存策略） 相关
  - nr_hugepages： 就是大页的数目

    ```
    echo 25000 > /proc/sys/vm/nr_hugepages # 修改巨页数量, 或使用`sysctl vm.nr_hugepages=N`
    mount -t hugetlbfs hugetlbfs /dev/hugepages
    systemctl start libvirtd
    virsh start c7

    # 关闭巨页
    sysctl vm.nr_hugepages=0
    umount hugetlbfs
    ```
  - nr_overcommit_hugepages

    在nr_hugepages数目以外， 在nr_hugepages数目不够的时候， 还可以动态补充多少大页.

    大页资源放在一个池（ huge page pool） 中， nr_hugepages指定的就是恒定的大页数目（ persistent hugepage） ， 它们是不会被拆分成小页的， 即使系统需要小页的时候。 而在persistent hugepage不够的时候， 系统可以向大页池中补充大页， 这些补充的大页由系统中空闲且物理连续的小页拼凑而来， 最多允许拼凑nr_overcommit_hugepages个大页. 当这些额外拼凑来的大页后来被释放出来的时候， 它们又会被解散成小页， 释放回小页池.
- /sys/kernel/mm/hugepages/目录

  **现在的kernel已经逐渐废弃通过/proc文件系统去配置大页, 而转向/sys文件系统**

  可在`/sys/kernel/mm/hugepages`查看各种启用的大页, `hugepages-<N>kB`下各内容:
  - nr_hugepages :  该种大页的数量

    ```bash
    echo 128 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
    ```
  - free_hugepages: 大页池中有多少空闲的大页
  - resv_hugepages: 被保留的暂时未分配出去的大页
  - surplus_hugepages: 具体的额外分配来的大页（ nr_overcommit_hugepages指示的是这个数目的上限）

  其中nr_hugepages、 nr_hugepages_mempolicy、 nr_overcommit_hugepages是可读可写的. 即可通过它们动态地调整大页池的大小. 在系统启动后可能因存在内存碎片而导致分配大页失败.


操作系统之上的应用程序（包括QEMU创建的客户机） 要利用上大页, 无非下面3种途径之一:
1. mmap系统调用使用MAP_HUGETLB flag创建一段内存映射
1. shmget系统调用使用SHM_HUGETLB flag创建一段共享内存区
1. 使用hugetlbfs创建一个文件挂载点， 这个挂载目录之内的文件就都是使用大页的了

> 有时也会见到第1种和第3种方法的叠加， 即： 创建hugetlbfs挂载点， 然后mmap它. 这个方法中， mmap时候就不需要MAP_HUGETLB flag了。 另外， 要注意munmap取消映
射的时候， 长度（length） 参数要与大页的大小对齐， 否则会失败.

#### KVM虚拟化对大页的利用
在KVM虚拟化环境中, 就是使用第3种方法： 创建hugetlbfs的挂载点， 通过“-mempath FILE”选项， 让客户机使用宿主机的huge page挂载点作为它内存的backend. 另外， 还
有一个参数“-mem-prealloc”是让宿主机在启动客户机时就全部分配好客户机的内存， 而不是在客户机实际用到更多内存时才按需分配. -mem-prealloc必须在有“-mem-path”参数时
才能使用。 提前分配好内存的好处是客户机的内存访问速度更快， 缺点是客户机启动时就得到了所有的内存， 从而使得宿主机的内存很快减少（而不是根据客户机的需求而动态地调整内存分配）.

KVM使用大页:
1. 检查大页支持: `cat /proc/cpuinfo | grep flags | uniq | grep pdpe1gb | grep pae | grep pse`. 目前， 几乎所有的Intel处理器都有上述对大页的硬件支持。 如果没有硬件的支持， 内核将无法使用大页.
1. 检查系统是否支持hugetlbfs(即大页的kernel配置项): `cat /proc/filesystems | grep hugetlbfs`
1. 检查并确保大页池（huge page pool） 中有足够大页. `cat /sys/kernel/mm/hugepages/hugepages-*kB/nr_hugepages`和`cat /sys/kernel/mm/hugepages/hugepages-*kB/free_hugepages`
1. 创建hugetlbfs的挂载点: `mount -t hugetlbfs -o pagesize=1G,size=8G,min_size=4G nodev /mnt/1G-hugepage/`
1. 启动客户机让其使用hugepage的内存， 使用`-mem-path /mnt/1G-hugepage -mem-prealloc`参数

对于内存访问密集型的应用， 在KVM客户机中使用Huge Page是可以较明显地提高客户机性能的. 不过, 它也有一个缺点, 使用Huge Page的内存不能被swap out,  也不能使用ballooning方式自动增长.

### 透明大页
大页有如下几个缺点
1. 大页必须在使用前就准备好
2. 应用程序代码必须显式地使用大页（一般是调用mmap、 shmget系统调用， 或者使用libhugetlbfs库对它们的封装）
3. 大页必须常驻物理内存中， 不能交换到交换分区中

  透明大页是可交换的（swapable） ， 当需要交换到交换空间时， 透明大页被打碎为常规的4KB大小的内存页。 在使用透明大页时， 如果因为内存碎片导致大页内存分配失败， 这时系统可以优雅地使用常规的4KB页替换， 而且不会发生任何错误、 故障或用户态的通知。而当系统内存较为充裕、 有很多的大页可用时， 常规的页分配的物理内存可以通过khugepaged内核线程自动迁往透明大页内存.
4. 需要超级用户权限来挂载hugetlbfs文件系统， 尽管挂载之后可以指定挂载点的uid、 gid、 mode等使用权限供普通用户使用
5. 如果预留了大页内存但没实际使用， 就会造成物理内存的浪费

透明大页（Transparent Hugepage） 既发挥了大页的一些优点， 又能避免了上述缺点.

在使用透明大页时， 普通的使用hugetlbfs大页依然可以正常使用， 而在没有普通的大页可供使用时， 才使用透明大页.

> 内核线程khugepaged的作用是， 扫描正在运行的进程， 然后试图将使用的常规内存页转换到使用大页. 目前， 透明大页仅仅支持匿名内存（anonymous memory） 的映射， [对磁盘缓存（page cache） 和共享内存（shared memory）的支持还处于开发之中](http://lxr.linux.no/#linux+v4.10/Documentation/vm/transhuge.txt).

透明大页的kernel配置:
```conf
ONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE=y
CONFIG_TRANSPARENT_HUGEPAGE=y
CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS=y
# CONFIG_TRANSPARENT_HUGEPAGE_MADVISE is not set
```
上述默认对所有应用程序的内存分配都尽可能地使用透明大页。 当然， 还可以在系统启动时修改Linux内核启动参数“transparent_hugepage(/sys/kernel/mm/transparent_hugepage)”来调整这个默认值:
- always : 尽可能地在内存分配中使用透明大页
- madvise : 表示仅在“MADV_HUGEPAGE”标识的内存区域使用透明大页

  在嵌入式Linux系统中内存资源比较珍贵, 为了避免使用透明大页可能带来的内存浪费， 如申请一个2MB内存页但只写入了1 Byte的数据， 可选择“madvise”方式使用透明大页
- never

  关闭透明内存大页的功能

将该值设置为“never”则表示关闭透明内存大页的功能。 当设置为“always”或“madvice”时， 系统会自动启动“khugepaged”这个内核进程去执行透明大页的功能； 当设置为“never”时， 系统会停止“khugepaged”进程的运行.

“transparent_hugepage/defrag”这个接口是表示系统在发生页故障（page fault） 时同步地做内存碎片的整理工作， 其运行的频率较高（在某些情况下可能会带来额外的负担） ；而“transparent_hugepage/khugepaged/defrag”接口表示在khugepaged进程运行时进行内存碎片的整理工作， 它运行的频率较低

透明大页通常在host上开启, 但还可以在KVM客户机中使用透明大页， 这样在宿主机和客户机同时使用的情况下(**推荐**)， 更容易提高内存使用的性能.

> 通过“/proc/meminfo”的“AnonHugePages”可看系统内存中透明大页的大小; 通过`/proc/<pid>/smaps”中“AnonHugePages`的大小可查看进程中透明大页的大小

### KSM
在现代操作系统中， 共享内存被很普遍地应用。 如在Linux系统中， 当使用fork函数创建一个进程时， 子进程与其父进程共享全部的内存， 而当子进程或父进程试图修改它们
的共享内存区域之时， 内核会分配一块新的内存区域， 并试图将修改的共享内存区域复制到新的内存区域上， 然后让进程去修改复制的内存。这就是著名的“写时复制”（copy-onwrite， COW） 技术.

KSM是“Kernel SamePage Merging”的缩写， 中文可称为“内核同页合并”。 KSM允许内核在两个或多个进程（包括虚拟客户机） 之间共享完全相同的内存页。 KSM让内核扫描检查正在运行中的程序并比较它们的内存， 如果发现它们有完全相同的内存区域或内存页， 就将多个相同的内存合并为一个单一的内存页， 并将其标识为“写时复制”。 这样可以起到节省系统内存使用量的作用。 之后， 如果有进程试图去修改被标识为“写时复制”的合并内存页， 就为该进程复制出一个新的内存页供其使用.

> 如果在同一宿主机上的多个客户机运行的是相同的操作系统或应用程序， 则客户机之间的相同内存页的数量就可能比较大， 这种情况下KSM的作用就更加显著.

在KVM虚拟化环境中， KSM能够提高内存的速度和使用效率。 具体可以从以下两个方面来理解:
1. 在KSM的帮助下， 相同的内存页被合并了， 减少了客户机的内存使用量。 一方面， 内存中的内容更容易被保存到CPU的缓存中， 另一方面， 有更多的内存可用于缓存一
些磁盘中的数据。 因此， 不管是内存的缓存命中率（CPU缓存命中率） ， 还是磁盘数据的缓存命中率（在内存中命中磁盘数据缓存的命中率） 都会提高， 从而提高了KVM客户机
中操作系统或应用程序的运行速度
1. KSM是内存过载使用的一种较好的方式。 KSM通过减少每个客户机实际占用的内存数量， 可以让多个客户机分配的内存数量之和大于物理上的内存数量

ksm内核配置:`CONFIG_KSM=y`, 由内核进程ksmd负责扫描后合并进程的相同内存页(只合并Private anonymous Page（匿名页） ， 而不合并Page Cache)， 从而实现KSM功能.

通过`/sys/kernel/mm/ksm/`目录下的文件来配置和监控ksmd这个守护进程, KSM只会去扫描和试图合并那些应用程序建议为可合并的内存页， 应用程序（如
QEMU） 通过如下的madvice系统调用来告诉内核哪些页可合并.

另外， 在使用KSM实现内存过载使用时， 最好保证系统的交换空间（swap space） 足够大. 因为KSM将不同客户机的相同内存页合并而减少了内存使用量， 但是客户机可能由于需要修改被KSM合并的内存页， 从而使这些被修改的内存被重新复制出来占用内存空间， 因此可能会导致系统内存的不足， 这时就需要有足够的交换空间来保证系统的正常运行.

`/sys/kernel/mm/ksm/`说明:
- full_scans： 记录已经对所有可合并的内存区域扫描过的次数。
- merge_across_nodes： 在NUMA（见7.4节） 架构的平台上， 是否允许跨节点（node）合并内存页。
- pages_shared： 记录正在使用中的共享内存页的数量。
- pages_sharing： 记录有多少数量的内存页正在使用被合并的共享页， 不包括合并的内存页本身。 这就是实际节省的内存页数量。

  pages_sharing的值越大， 说明KSM节省的内存越多， KSM效果越好.
  节省内存(MB)=pages_sharing： * $(getconf PAGESIZE) / 1024 / 1024

  pages_sharing除以pages_shared得到的值越大， 说明相同内存页重复的次数越多，KSM效率就越高.
- pages_unshared： 记录了守护进程去检查并试图合并， 却发现了因没有重复内容而不能被合并的内存页数量

  pages_unshared除以pages_sharing得到的值越大， 说明ksmd扫描不能合并的内存页越多， KSM的效率越低
- pages_volatile： 记录了因为其内容很容易变化而不被合并的内存页。
- pages_to_scan： 在ksmd进程休眠之前扫描的内存页数量即每次扫描的内存页数量
- sleep_millisecs： ksmd进程休眠的时间（单位： 毫秒） ， ksmd的两次运行之间的间隔。
- run： 控制ksmd进程是否运行的参数， 默认值为0， 要激活KSM必须要设置其值为1（除非内核关闭了sysfs的功能）

  - 设置为0， 表示停止运行ksmd但保持它已经合并的内存页
  - 设置为1， 表示马上运行ksmd进程
  - 设置为2表示停止运行ksmd， 并且分离已经合并的所有内存页， 但是保持已经注册为可合并的内存区域给下一次运行使用

> 通过前面查看这些sysfs中的ksm相关的文件可以看出， 只有pages_to_scan、sleep_millisecs、 run这3个文件对root用户是可读可写的， 其余6个文件都是只读的

> 调整ksm行为的后台服务: ksm服务的类型是一次性的; ksmtuned服务一直保持循环执行, **推荐**.

> QEMU通过-machine（或者-M缩写） 参数的mem-merge=on/off来指定的开启ksm. 默认是on， 也就是允许内存合并

### numa(Non-Uniform Memory Access， 非统一内存访问架构)

## 资源限制
ref:
- `<<深度实践KVM>>的第6章 KVM虚拟机的资源限制`

kvm的资源限制主要通过cgroups解决, libvirt在cgroups上封装了一层, 也可通过修改xml实现.

在libvirtd服务启动时, 会自动在cgroup各子系统下创建libvirt群组, 主要是因为libvirt会自动检查cgroup的位置并自动挂载, 这在/etc/libvirt/qemu.conf中有说明.

cgroup对网络的限制没有限制cpu, memory那样方便, 因此一般不使用它, 而是用:
- tc
- libvirt

  libvirt实际也是使用tc来限制, 因此`<inbound>`内容无效.

  ```xml
  <bandwidth>
    <inbound average="100" peak="50" burst="1024"/> 
    <outbound average="100" peak="50" burst="1024"/> # 平均带宽不超过100KB/s, 最高不超过50KB/s
  </bandwidth>
  ```
- iptables

  默认经过桥接的vm网络流量不受host iptables限制, 需开启`echo 1 > /proc/sys/net/bridge/bridge-nf-call-iptables`

  不能精确控制网速, 只能控制包的个数, 具体可用`mtu*包数`来计算. 但它支持控制流入和流出.

## kvm调优
- cpu调优

  vm对物理机cpu逻辑核的手工绑定, 需要了解NUMA

  步骤:
  1. `numactl --hardware`, 查看硬件配置
  1. `numastat -c qemu-system-x86_64`, 查看相关进程的numa内存统计
  1. `virsh numatune`修改vm的numa配置

    <vcpu>和<numatune>需保持一致, <numatune>配置的是物理cpu, <vcpu>配置的是逻辑cpu(包括超线程产生的核). <numatune>使用static模式时<nodeset>也必须是.

- 内存调优

  KSM, 即相同内存页合并, 内存气球技术及大页内存的使用

## p2v
ref:
- `<<深度实践KVM>>的第7章 物理机转虚拟机实践`

两种方案:
1. 静态方案

  将物理机关机, 使用克隆工具克隆, 再在vm上还原. 比如virt-p2v和vmware esx 4.0之前的版本

  优点:
  1. 关机操作, 对现有系统不改造, 即使实施失败, 也不影响现有系统

  缺点:
  1. 物理机系统里没有vm的磁盘驱动, 比如virtio, 要将驱动导入系统中, 否则克隆的系统不能正常启动, 比如windows会蓝屏, linux会报无法失败磁盘等错误.

  可安装必要驱动再实施静态方案.

  > Ubuntu 20.04已自带virtio驱动.
1. 动态方案

  物理机处于运行状态, 使用专门的agent将物理机文件在线复制到vm. vmware esx 4.0开始采用这种方案.

  优点:
  1. 开机情况下完成物理机到虚拟机的转换, 转换时不需要加载驱动

  缺点:
  1. 要安装agent, 有一定几率转换不成功.

再升龙可克隆磁盘.

guestfish的virt-v2v支持将vmware vm转为kvm vm.

## 桌面虚拟化
ref:
- `<<深度实践KVM>>的第8章 KVM桌面虚拟化实践`

虚拟化技术和远程访问协议是桌面虚拟化(Virtual Desktop Infrastructure, VDI)的核心技术.

基于kvm的vdi一般使用spice协议.

spice架构:
1. client: 负责发起连接, 运行在用户终端, 比如virt-viewer
1. server: 处理client的连接, 运行在host
1. agent: 可选, 运行在vm中, 用于增强性能, 提升用户体验

此时sound和video设备是必选. 如果video model type使用qxl, 则必选安装qxl驱动, 否则vm可能启动失败.

spice配置:
```xml
<graphics type="spice" port="10022" autoport="no" listen="0.0.0.0" keymap="en-us">
  <listen type="address" address="0.0.0.0">
</graphics>
```

配置spice windows7模板:
1. 安装spice-quest-tools, 它集成了RHEV spice agent和qxl驱动, 且需要RHEV spice agent服务开机自启.

> 推荐对是ssd的磁盘使用: `echo "noop" > /sys/block/<sdb>/queue/scheduler`

## 分布式fs
ref:
- `<<深度实践KVM>>的第9章 几种常见开源文件系统在KVM中的应用`

glusterfs建议在生产环境使用分布条带复制卷, 并且复制3份, 这样在性能, 稳定性和安全性达到较好的平衡.
sheepdog是为kvm定制的分布式fs.

vm以qcow2格式在cephfs上不稳定, windows容易蓝屏. 而直接在ceph rdb上运行vm是正常的, 因此cephfs适合作为镜像的备份, ceph rdb适合做云盘.

## Libvirt
Libvirt是由Redhat开发的一套开源的软件工具，目标是提供一个通用和稳定的软件库来高效、安全地管理一个节点上的虚拟机，并支持远程操作. libvirt 已经成为使用最为广泛的对各种虚拟机进行管理的工具和应用程序接口（API），而且一些常用的虚拟机管理工具（如virsh、virt-install、virt-manager等）和云计算框架平台（如OpenStack、OpenNebula、Eucalyptus等）都在底层使用libvirt的应用程序接口.

基于可移植性和高可靠性的考虑，Libvirt采用C语言开发，但是也提供了对其他编程语言的绑定，包括Python、Perl、OCaml、Ruby、Java和PHP. 同时Libvirt支持多种VMM，包括LXC、KVM/QEMU、Xen、VirtualBox等.

为了支持多种VMM，Libvirt采用了基于Driver的架构(中间人)，每种VMM需要提供一个驱动和Libvirt进行通信来操控特定的VMM. 在初始化过程中，所有的驱动被枚举和注册. 每一个驱动都会加载特定的函数为Libvirt API调用.

## 管理平台
ref:
- `<<深度实践KVM>>的第11~13章 KVM虚拟化管理平台`

- openstack
- oVirt
- cloudstack
- WebVirtCloud

## 硬件选型
- 网卡

  - 千兆: intel i350, 支持vf=6
  - 万兆: intel x520/x540, 支持vf=64

  均支持SRIOV.

## image制作
ref:
- `<<深度实践KVM>>的第16章 虚拟机镜像制作配置与测试`

1. 不建议先制作小镜像再通过virt-resize扩展, 因为windows扩展后需要做fs检查, 时间可能很长


> 大部分云产商通过复制镜像的方法进行部署vm.

> 2014年年底发布的[windows virtio](https://www.linux-kvm.org/page/WindowsGuestDrivers/Download_Drivers)驱动修复了网卡闪断的问题, win virtio驱动在[这里](https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/archive-virtio/), 说明文档在[这里](https://github.com/virtio-win/virtio-win-pkg-scripts/blob/master/README.md).

## kvm集群
使用raw, 因为使用的商业存储都支持精简模式(truenas scale), qcow2使用fs, 性能会低很多.

### 在线迁移
在目标端`tail -f /var/log/libvirt/qemu/xxx.log`可查看迁移进度

## libvirt

### Libvirt API
Libvirt定义了各种各样的API，涉及虚拟化的方方面面，主要分为以下几类:
- 虚拟机快照：快照是包括内存、硬盘等信息在内的完整虚拟机状态。这些API就是用于创建、删除和恢复快照的。
- 虚拟机管理：这一类API用于管理虚拟机，也是Libvirt里面使用最频繁的功能。例如：创建、销毁、重启、迁移虚拟机，以及操作虚拟机的磁盘镜像等。
- 事件：事件是Libvirt定义的一套监测特定情况发生的机制，用户可以通过相应的API告诉Libvirt，想要监测什么样的事件，或者事件发生时采取什么样的操作。
- 存储管理

  Libvirt在任何运行了Libvirt daemon的主机端通过管理存储池和卷来为虚拟机提供存储资源，可以支持包括本地文件系统、网络文件系统、iSCSI、LVM等多种后端存储系统.
  虚拟机磁盘格式上支持qcow2、vmdk、raw等格式.

- 宿主机：用于获取宿主机的各种信息，包括机器名、CPU状态等，也用于和特定的VMM建立连接。
- 网络接口：实现网络接口的相应操作，如定义一个新的网络接口

  支持Linux桥、vlan、多网卡绑定管理，比较新的版本还支持open vswitch. libvirt还支持nat和路由方式的网络，可以通过防火墙让虚拟机通过宿主机建立网络通道和外部网络进行通信.
- 错误管理：提供了Libvirt本身的错误管理机制，比如获取最近一次的Libvirt错误。
- 其他设备管理：包括对网络、PCI、USB等设备的管理。

### Libvirt存储池和存储卷
为了提供统一的接口给虚拟机来访问不同的后端存储设备，Libvirt将存储管理分为两个方面：存储卷和存储池. 存储卷可以作为存储设备分配给虚拟机使用，物理上可以是一个虚拟机磁盘文件或一个真实的磁盘分区. 存储池可以被理解为本地目录，或者通过各类分布式存储系统分配过来的目录等，存储卷可以从存储池中生成，Libvirt可以支持多种存储池类型，内容如下:
- 目录池（Directory Pool）：以主机的一个目录作为存储池
- 本地文件系统池（Filesystem Pool）：使用主机已经格式化好的块设备作为存储池，支持的文件系统类型包括ext4、XFS等
- 网络文件系统池（Network Filesystem Pool）：使用远端网络文件系统服务器的导出目录作为存储池
- 逻辑卷池（Logical Volume Pool）：使用已经创建好的LVM卷组，或者基于一系列生成卷组的源设备生成卷组，生成存储池
- 磁盘卷池（Disk Pool）：使用磁盘作为存储池
- iSCSI卷池（iSCSI Pool）：使用iSCSI设备作为存储池
- SCSI卷池（SCSI Pool）：使用SCSI设备作为存储池
- 多路设备池（Multipath Pool）：使用多路设备作为存储池
- RBD Pool：包括一个RADOS池的所有RBD image

Libvirt中的存储管理独立于虚拟机的管理，存储卷从存储池中被划分出来，存储卷分配给虚拟机成为可用的存储设备. 通过virsh工具的pool命令可以查看、创建、激活、注册、删除存储池，因此创建存储资源时，并不需要有虚拟机的存在.

### libvirt cmds
参考:
- [virt-install和virsh详解](https://yq.aliyun.com/articles/529107)

```bash
# apt install libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager
# lsmod | grep -i kvm
# sudo systemctl is-active libvirtd
# usermod -aG libvirt $USER
# usermod -aG kvm $USER
# brctl show # a virtual bridge "virbr0" is created automatically, but this is only used for testing purpose.
# cat << EOF > /etc/netplan/00-installer-config.yaml # To create a network bridge, access the guests from outside the local network
# This is the network config written by 'subiquity'
network:
  ethernets:
    enp0s3:
      dhcp4: no
      dhcp6: no
  version: 2
  bridges:
    br0:
      interfaces: [enp0s3]
      addresses: [172.20.10.9/28]
      gateway4: 172.20.10.1
      nameservers:
        addresses: [4.2.2.2, 8.8.8.8]
EOF
# netplan apply # activate the bride br0
# networkctl status br0 # verify the status of br0 bridge
# ip a s
```

- libvirt-daemon-system : configuration files to run the libvirt daemon as a system service.
- libvirt-clients : software for managing virtualization platforms.
- bridge-utils : a set of command-line tools for configuring ethernet bridges.
- virtinst : a set of command-line tools for creating virtual machines.
- virt-manager : an easy-to-use GUI interface and supporting command-line utilities for managing virtual machines through libvirt.

## FAQ
### 虚拟机XML格式
ref:
- [<<KVM实战>>]
- [虚拟机配置](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html)
- [Libvirt XML文件详解（一）](https://blog.csdn.net/u012324798/article/details/113615576)
- [Domain XML format](https://libvirt.org/formatdomain.html)

```xml
<domain type='kvm' id='29'>
//domain 是一个所有虚拟机都需要的根元素，它有两个属性: 1. type定义使用哪个虚拟机管理程序，值可以是：xen、kvm、qemu、lxc、kqemu, vmvare中的一个;2. id，它唯一的标示一个运行的虚拟机. 如果不设置id属性， libvirt会按顺序分配一个最小的可用ID.
  // [虚拟机名称必须由数字、字母、“_”、“－”、“:”组成，但不支持全数字的字符串，且虚拟机名称不超过64个字符](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html)
  <name>i-000039</name>
//name参数为虚拟机定义了一个简短的名字，在同一host上必须唯一

  <uuid>d59b03ce-2e78-4d35-b731-09d9ca9653af</uuid>
//uid为虚拟机定义了一个全球唯一的标示符，uuid的格式必须遵循RFC 4122指定的格式，当创建虚拟机没有指定uuid时会随机的生成一个uuid. 自己可用命令行工具 uuidgen生成.

  // [虚拟CPU和虚拟内存](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html)
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
  <memtune>
    <hard_limit unit='KiB'>4194304</hard_limit>
  </memtune>
    //memory 定义客户端启动时可以分配到的最大内存，内存单位由unit定义，单位可以是：K、KiB、M、MiB、G、GiB、T、TiB。默认是KiB
    // currentMemory标签中的内存表示启动时即分配给客户机使用的内存. 在使用QEMU/KVM时， 一般将二者设置为相同的值
  
  
  <vcpu placement='static' cpuset="1-4,^3,6" current="1">2</vcpu>
  //vcpu的内容是为虚拟机最多分配几个cpu，值处于1~maxcpu之间，可选参数：cpuset参数指定虚拟cpu可以映射到那些物理cpu上，物理 cpu用逗号分开，单个数字的标示单个cpu，
  //也可以用range符号标示多个cpu，数字前面的脱字符标示排除这个cpu，current参数指定虚拟 机最少，placement参数指定一个domain的cpu的分配模式，值可以是static、auto. 比如`1-4,^3,6`表示客户机的两个vCPU被允许调度到
1、 2、 4、 6号物理CPU上执行（^3表示排除3号）
  // current表示启动客户机时只给1个vCPU， 最多可以增加到使用2个vCPU
  
  // 对CPU的分配进行更多调节
  // vcpupin标签表示将虚拟CPU绑定到某一个或多个物理CPU上， 如"<vcpupin vcpu="2"cpuset="4"/>"表示客户机2号虚拟CPU被绑定到4号物理
CPU上； "<emulatorpin cpuset="1-3"/>"表示将QEMU emulator绑定到1~3号物理CPU上。 在不设置任何vcpupin和cpuset的情况下, 客户机的虚拟CPU可能会被调度到任何一个物理CPU上去运行
  // shares表示客户机占用CPU时间的加权配置. 一个配置为2048的域获得的CPU执行时间是配置为1024的域的两倍. 如果不设置shares值， 就会使
用宿主机系统提供的默认值
  <cputune>
    <vcpupin vcpu="0" cpuset="1"/>
    <vcpupin vcpu="1" cpuset="2,3"/>
    <vcpupin vcpu="2" cpuset="4"/>
    <vcpupin vcpu="3" cpuset="5"/>
    <emulatorpin cpuset="1-3"/>
    <shares>1024</shares>
    <period>100000</period>
    <quota>-1</quota>
  </cputune>
  <resource>
    <partition>/machine</partition>
  </resource>
  
  
 //操作系统启动介绍
 // [手动导入secure boot证书](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E7%AE%A1%E7%90%86%E8%99%9A%E6%8B%9F%E6%9C%BA.html)
  <os firmware="efi"> // firmware可选值：bios、efi, 此属性允许管理应用程序自动填充 <loader/> 和 <nvram/> 二级元素，并可能启用所选固件所需的一些特性. libvirt 将扫描下列路径中描述了 installed firmware images 的文件，并使用能满足 domain 要求的最具体的文件。扫描路径（优先顺序：从一般到最具体）：/usr/share/qemu/firmware、/etc/qemu/firmware、$XDG_CONFIG_HOME/qemu/firmware. x64如果不配置loader，则使用默认启动方式BIOS
    <type arch='x86_64' machine='pc-i440fx-2.8'>hvm</type>
    //type参数指定了虚拟机操作系统的类型，内容：hvm(hardware virtual machine)表明表示在硬件辅助虚拟化技术（Intel VT或AMD-V等）的支持下不需要修改客户机操作系统就可以启动客户机. 因为KVM一定要依赖于硬件虚拟化技术的支持, 所以在KVM中, 客户机类型应该总是hvm, **常用**.
    //type同样有两个可选参数：arch指定虚拟机的CPU构架，machine指定机器的类型.
    // hvm 表示操作系统是为在裸机上运行而设计的，因此需要完全虚拟化
    //<boot dev='hd'/>dev属性的值可以是：fd、hd、cdrom、network，它经常被用来指定下一次启动. boot的元素可以被设置多个用来建立一个启动优先规则, **推荐用disk/cd/nic设备的`boot order(从1开始)`代替`boot dev`**
    // bootmenu:指定是否在 guest 启动时显示 boot menu 交互界面
    <boot dev='hd'/>
    <boot dev='cdrom'/>
    <bootmenu enable='yes' timeout='0'/>
    <bios useserial='yes'/>
  </os>
  
  Hypervisor的特性
  <features>
    <acpi/>
    <apic/>
    <!-- <pae/> -->
  </features>
  Hypervisors允许特定的CPU/机器特性打开或关闭，所有的特性都在fearures元素中，以下介绍一些在全虚拟化中常用的标记：
pae：扩展物理地址模式，使32位的客户端支持大于4GB的内存
acpi：用于电源管理
hap：Enable use of Hardware Assisted Paging if available in the hardware.
  
  
  
  //cpu分配
  // CPU模型： Haswell-noTSX， 可以在文件/usr/share/libvirt/cpu_map.xml(为了保证vm在不同host间迁移的兼容性, libvirt对cpu提炼出标准的几种类型)中查看详细描述
  // custom模式：基于某个基础的CPU模型， 再做个性化的设置
  // host-model模式： 根据物理CPU的特性， 选择一个与之最接近的标准CPU型号， 如
果没有指定CPU模式， 默认也是使用这种模式。 xml配置文件为： <cpu mode='hostmodel'/>
  // host-passthrough模式： 直接将物理CPU特性暴露给虚拟机使用, 在虚拟机上看到
的完全就是物理CPU的型号. 应用场景: 嵌套虚拟化或公有云vm看到的cpu(用户体验好). 缺点: 不同型号cpu的host间不能迁移vm。 xml配置文件为： <cpu mode='host-passthrough'/>
   // 三种mode的性能排序是：host-passthrough > host-model > custom
   // 三种mode的热迁移通用性是： custom > host-model > host-passthrough
  <cpu mode='custom' match='exact'>
    <model fallback='allow'>Haswell-noTSX</model>
    <topology sockets='1' cores='1' threads='1'/>
    <numa>
      <cell id='0' cpus='0' memory='4194304' unit='KiB'/>
    </numa>
  </cpu>
  
  时间设置
  <clock offset='variable' adjustment='0' basis='utc'>
    <timer name='rtc' track='guest'/>
  </clock>
  客户端的时间初始化来自宿主机的时间，大多数操作系统期望硬件时钟保持UTC格式，UTC也是默认格式，然而Windows机器却期望它是’localtime’
clock的offset属性支持四种格式的时间：UTC localtime timezone variable
UTC：当引导时客户端时钟同步到UTC时钟
localtime：当引导时客户端时钟同步到主机时钟所在的时区
timezone：The guest clock will be synchronized to the requested timezone using the timezone attribute.

  
  //控制周期：
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>preserve</on_crash>
  <on_lockfailure>poweroff</on_lockfailure>
  //当一个客户端的OS触发lifecycle时，它将采取新动作覆盖默认操作，具体状态参数如下：
 //on_poweroff：当客户端请求poweroff时执行特定的动作
  //on_reboot：当客户端请求reboot时执行特定的动作
  // on_crash：当客户端崩溃时执行的动作
   //每种状态下可以允许指定如下四种行为：
    //destory：domain将会被完全终止，domain的所有资源会被释放
    //restart：domain会被终止，然后以相同的配置重新启动
    //preserver：domain会被终止，它的资源会被保留用来分析
    //rename-restart：domain会被终止，然后以一个新名字被重新启动
  
  // [配置虚拟设备](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html)
  <devices>
  //所有的设备都是一个名为devices元素的子设备(All devices occur as children of the main devices element.)，以下是一个简单的配置：
//<emulator>/usr/bin/kvm</emulator>
//emulator元素指定模拟设备二进制文件的全路径
    <emulator>/usr/libexec/qemu-kvm</emulator>
    // <disk>标签是客户机磁盘配置的主标签, 其中包含它的属性和一些子标签. 它的type属性表示磁盘使用哪种类型作为磁盘的来源， 其取值为file、 block、 dir或network中的一个， 分别表示使用文件、 块设备、 目录或网络作为客户机磁盘的来源. 它的device属性表示让客户机如何来使用该磁盘设备， 其取值为floppy、 disk、 cdrom或lun中的一个， 分别表示软盘、 硬盘、 光盘和LUN（逻辑单元号）, 默认值为disk（硬盘）.
    // <disk>的<driver>子标签用于定义Hypervisor如何为该磁盘提供驱动， 它的name属性用于指定宿主机中使用的后端驱动名称， QEMU/KVM仅支持name='qemu'， 但是它支持的类型type可以是多种， 包括raw、 qcow2、 qed、 bochs等. 而这里的cache属性表示在宿主机中打开该磁盘时使用的缓存方式， 可以配置为default、 none、 writethrough、 writeback、directsync和unsafe等多种模式.
    // <disk>的<source>子标签表示磁盘的来源， 当<disk>标签的type属性为file时， 应该配置为<source file='/var/lib/libvirt/images/centos7u2-1.img'/>这样的模式， 而当type属性为block时， 应该配置为<source dev='/dev/sda'/>这样的模式
    // <disk>的<target>子标签表示将磁盘暴露给客户机时的总线类型和设备名称. 其dev属性表示在客户机中该磁盘设备的逻辑设备名称， 而bus属性表示该磁盘设备被模拟挂载的总线类型， bus属性的值可以为ide、 scsi、 virtio、 xen、 usb、 sata等. 如果省略了bus属性， libvirt
则会根据dev属性中的名称来"推测"bus属性的值， 例如， sda会被推测是scsi， 而vda被推测是virtio, fda是fdc
  // <disk>的<address>子标签表示该磁盘设备在客户机中的PCI总线地址, 这个标签在前面网络配置中也是多次出现的, 如果该标签不存在, libvirt会自动分配一个地址
    <disk type='file' device='cdrom'>
      <backingStore/>
      <target dev='hdd' bus='ide'/>
      <readonly/>
      <boot order='2'/>
      <alias name='ide0-1-1'/>
      <address type='drive' controller='0' bus='1' target='0' unit='1'/>
    </disk>
    // 使用qcow2格式的镜像文件作为客户机的磁盘, 其在客户机中使用virtio总线（使用virtio-blk驱动）, 设备名称为/dev/vda, 其PCI地址为0000:01:01.0
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/ae386362-eed6-43a8-b5a8-11a42fabc0ed'/>
      <backingStore/>
      <target dev='vda' bus='virtio'/>
      <boot order='1'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x01' slot='0x01' function='0x0'/>
    </disk>
    // [总线配置](https://docs.openeuler.org/zh/docs/22.03_LTS/docs/Virtualization/%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%85%8D%E7%BD%AE.html)
    // 根据客户机架构的不同， libvirt默认会为客户机模拟一些必要的PCI控制器（而不需要在XML配置文件中指定, 由libvirt自动指定）, 而一些PCI控制器需要显式地在XML配置文件中配置.
    // libvirt默认还会为客户机分配一些必要的PCI设备， 如PCI主桥（Host bridge）, ISA桥等
    // controller：控制器元素，表示一个总线
    // 虚拟机内部的网卡、磁盘控制器、PCIe直通设备都需要挂接到PCIe Root Port下面，每个Root Port对应一个PCIe插槽。Root Port的下挂设备支持热插拔，但是Root Port本身不支持热插拔
    // [qemu usb源码](https://github.com/qemu/qemu/tree/master/hw/usb), usb分类:
    // UHCI（Universal Host Controller Interface）：通用主机控制器接口，也称为USB 1.1主机控制器规范。
    // EHCI（Enhanced Host Controller）：增强主机控制器接口，也称为USB 2.0主机控制器规范。
    // xHCI（eXtensible Host Controller Interface）：可扩展主机控制器接口，也称为USB 3.0主机控制器规范
    <controller type='usb' index='0' model='ich9-ehci1'>
      <alias name='usb'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='pci-ohci'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </controller>
    <controller type='ide' index='0'>
      <alias name='ide'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </controller>
    <controller type='pci' index='0' model='pci-root'>
      <alias name='pci.0'/>
    </controller>
    <controller type='pci' index='1' model='pci-bridge'>
      <model name='pci-bridge'/>
      <target chassisNr='1'/>
      <alias name='pci.1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <lease>
      <lockspace>6ee684f1-8b25-4f0a-9721-fe96540c1870</lockspace>
      <key>ae386362-eed6-43a8-b5a8-11a42fabc0ed</key>
      <target path='/datastore/6ee684f1-8b25-4f0a-9721-fe96540c1870/.6ee684f1-8b25-4f0a-9721-fe96540c1870/.leases' offset='4194304'/>
    </lease>
    
    
    网络接口：
有好几种网络接口访问客户端:Virtual network、Bridge to LAN、Userspace SLIRP stack、Generic ethernet connection、Direct attachment to physical interface。
Virtual network：这种推荐配置一般是对使用动态/无线网络环境访问客户端的情况。
Bridge to LAN：这种推荐配置一般是使用静态有限网络连接客户端的情况。
    // type='bridge'表示使用桥接方式使客户机获得网络, 可用`brctl show`查看host bridge. address用于配置客户机中网卡的MAC地址. <source bridge='xlansw-000003'/>表示使用宿主机中的xlansw-000003网络接口来建立网桥， <model type='virtio'/>表示在客户机中使用virtio-net驱动的网卡设备, 也配置了该网卡在客户机中的PCI设备编号为`0000:00:10.0`
    // type='network'和<source network='default'/>表示使用NAT的方式， 并使用默认的网络配置， 客户机将会分配到192.168.122.0/24网段中的一个IP地址. 使用NAT必须保证宿主机中运行着DHCP和DNS服务器, 一般默认使用dnsmasq软件查询. 由于配置使用了默认的NAT网络配置(<source network='default'/>), 可以在libvirt相关的网络配置中看到一个default.xml文件（/etc/libvirt/qemu/networks/default.xml）, 它具体配置了默认的连接方式.
    // type='user'表示该客户机的网络接口是用户模式网络， 是完全由QEMU软件模拟的一个网络协议栈.
    // type='hostdev'(仅支持libvirt 0.9.11以上的版本其仅支持SR-IOV特性中的VF的直接配置; 旧版本直接使用<hostdev>标签)表示使用网卡设备直接分配（VT-d）即将PCI/PCI-e网卡将设备直接分配给客户机使用. <driver name='vfio'/>指定使用哪一种分配方式（默认是VFIO， 如果使用较旧的传统的device assignment方式, 这个值可配为'kvm'）, 用<source>标签来指示将宿主机中的哪个VF分配给宿主机使用, 还可使用<mac address='52:54:00:6d:90:02'>来指定在客户机中看到的该网卡设备的MAC地址. 示例配置如下所示， 它表示将宿主机的`0000:08:10.0`这个VF网卡直接分配给客户机使用， 并规定该网卡在客户机中的MAC地址为"52:54:00:6d:90:02"
    // <hostdev>标签是libvirt 0.9.11版本之前对设备直接分配的唯一使用方式, 而且对设备的支持较为广泛, 既支持有SR-IOV功能的高级网卡的VF的直接分配， 也支持无SR-IOV功能的普通PCI或PCI-e网卡的直接分配. 这种方式并**不支持对直接分配的网卡在客户机中的MAC地址的设置**, 在客户机中网卡的MAC地址与宿主机中看到的完全相同. 示例如下所示, 它表示将宿主机中的PCI 0000:08:00.0设备直接分配给客户机使用
    <interface type='bridge'>
      <mac address='00:16:3e:bd:8e:f3'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='1fb6d3e0-c0c0-4fdd-9edb-c64b1327763f'/>
      </virtualport>
      <target dev='vnd59b03ce0'/>
      <model type='virtio'/>
      <boot order='3'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x10' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:59:09:2d'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f8a225e1-028f-4396-8107-879f5b77152c'/>
      </virtualport>
      <target dev='vnd59b03ce1'/>
      <model type='rtl8139'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x11' function='0x0'/>
    </interface>
    <interface type='bridge'>
      <mac address='00:16:3e:97:5b:c0'/>
      <source bridge='vxlansw-000003'/>
      <virtualport type='openvswitch'>
        <parameters interfaceid='f688fbc6-c4e3-4348-9408-12dc903dab06'/>
      </virtualport>
      <target dev='vnd59b03ce2'/>
      <model type='virtio'/>
      <alias name='net2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x12' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:32:7d:f6'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='user'>
      <mac address="00:11:22:33:44:55"/>
    </interface>
    <interface type='hostdev'>
      <driver name='vfio'/>
      <source>
      <address type='pci' domain='0x0000' bus='0x08' slot='0x10' function= '0x0'/>
      </source>
      <mac address='52:54:00:6d:90:02'>
    </interface>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
      <address domain='0x0000' bus='0x08' slot='0x00' function='0x0'/>
      </source>
  </hostdev>
    
    // 串口和控制台是非常有用的设备， 特别是在调试客户机的内核或遇到客户机宕机的情况下， 一般都可以在串口或控制台中查看到一些利于系统管理员分析问题的日志信息
    //<serial>设置了客户机的编号为0的串口（即/dev/ttyS0）, 使用宿主机中的伪终端（pty）. 没有<source>时libvirt会自己选择一个空闲的虚拟终
端（可能为/dev/pts/下的任意一个）. 当然也可以加上<source path='/dev/pts/1'/>配置来明确指定使用宿主机中的哪一个虚拟终端
    // 在通常情况下, 控制台（console） 配置在客户机中的类型为'serial'时， 如果没有配置串口（serial） ， 则会将控制台的配置复制到串
口配置中， 如果已经配置了串口（本例即是如此） ， 则libvirt会忽略控制台的配置项.
    // 当然为了让控制台有输出信息并且能够与客户机交互， 也需在客户机中配置将信息输出到串口， 如在Linux客户机内核的启动行中添加"console=ttyS0"这样的配置
    <serial type='pty'>
      <source path='/dev/pts/14'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty'>
    <target type='serial' port='0'/>
    </console>
    在每组指令中，最顶层的指令(parallel, serial, console, channel)描述设备怎样出现在客户端中，客户端接口通过target配置。
The interface presented to the host is given in the type attribute of the top-level element. The host interface is configured by the source element


    <console type='pty' tty='/dev/pts/14'>
      <source path='/dev/pts/14'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.vmtools'/>
      <target type='virtio' name='com.inspur.ics.vmtools' state='disconnected'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/channels/i-000039.com.inspur.ics.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0' state='disconnected'/>
      <alias name='channel1'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    
    // <input>进行交互的输入设备的配置
    // 这里的配置会让QEMU模拟PS2接口的鼠标和键盘， 还提供了tablet这种类型的设备，让光标可以在客户机获取绝对位置定位
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
      <address type='usb' bus='0' port='1'/>
    </input>
    <input type='mouse' bus='ps2'>
      <alias name='input1'/>
    </input>
    <input type='keyboard' bus='ps2'>
      <alias name='input2'/>
    </input>
    输入设备：
输入设备允许使用图形化界面和虚拟机交互，当有图形化framebuffer的时候，输入设备会被自动提供的。
<input type='mouse' bus='ps2'/>
input元素：input元素含有一个强制的属性，type属性的值可以是mouse活tablet，前者使用想对运动，后者使用绝对运动。bus属性指定一个明确的设备类型，值可以是：xen、ps2、usb。

    // <graphics>是对连接到客户机的图形显示方式的配置
    <graphics type='sdl' display=':0.0'/>
    <graphics type='rdp' autoport='yes' multiUser='yes' />
    <graphics type='desktop' fullscreen='yes'/>
    <graphics type='spice'>
    <listen type='network' network='rednet'/>
    </graphics>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <graphics type='vnc' port='5905' autoport='yes' listen='0.0.0.0' keymap='en-us' sharePolicy='force-shared'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    graphics元素：graphics含有一个强制的属性type，type的值可以是：sdl、vnc、rdp、desktop。vnc则启动vnc 服务，port属性指定tcp端口，如果是-1，则表示自动分配，vnc的端口自动分配的话是从5900向上递增。listen属性提供一个IP地址给服 务器监听，可以单独在listen元素中设置。passwd属性提供一个vnc的密码。keymap属性提供一个keymap使用。
Rather than putting the address information used to set up the listening socket for graphics types vnc and spice in the <graphics> listen attribute, a separate subelement of <graphics>, called <listen> can be specified (see the examples above)since 0.9.4. <listen> accepts the following attributes:
listen元素：listen元素专门针对vnc和spice设置监听端口等。它包含以下属性：type、address、network。type的 值可以是address或network。如果设置了type=address，那么address属性设置一个ip地址或者主机名来监听。如果 type=network，则network属性设置一个网络名称在libvirt‘s的网络配置文件中。

字符设备提供同虚拟机进行交互的接口，Paravirtualized consoles, serial ports, parallel ports and channels 都是字符设备，它们使用相同的语法。
    
    // Arch架构常用属性值为virtio，x86_64架构通常使用属性值为vga/qxl
    <video>
      <model type='cirrus' vram='16384' heads='1' primary='yes'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <video>
      <model type='qxl' ram='65536' vram='65536' vgamem='16384' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    video元素：是显卡配置，为了向后完全兼容，如果没有设置video但是有graphics在xml配置文件中，这时libvirt会按照 客户端类型增加一个默认的video，。model元素有一个强制的type属性，它的值可以是：vga、cirrus、vmvga、xen、vbox、 qxl. 例如一个客户端类型为kvm，那么默认的type值是cirrus. vram属性表示虚拟显卡的显存容量（单位为KB） ， heads属性表示显示屏幕的序号
    
    // <sound>标签表示的是声卡配置， 其中model属性表示为客户机模拟出来的声卡的类型， 其取值为es1370、 sb16、 ac97和ich6中的一个
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    
    // 内存的ballooning相关的配置包含在devices这个标签的memballoon子标签中, 该标签配置了该客户机的内存气球设备
    // 该配置将为客户机分配一个使用virtio-balloon驱动的设备, 以便实现客户机内存的ballooning调节. 该设备在客户机中的PCI设备编号为`0000:00:07.0`
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </memballoon>
    <panic model='isa'>
      <address type='isa' iobase='0x505'/>
    </panic>
  </devices>
  
  
  <seclabel type='none' model='none'/>
  <seclabel type='dynamic' model='dac' relabel='yes'>
    <label>+0:+0</label>
    <imagelabel>+0:+0</imagelabel>
  </seclabel>
</domain>
```

为所有虚拟机所需的根元素命名是domain. 它有两个属性，在 type指定用于运行域管理程序. 允许的值是特定于驱动程序的，但包括"xen"，"kvm"即使用kvm，"qemu"即完全虚拟化，"lxc"和"kqemu". 第二属性是id它是正在运行的guest机的唯一整数标识符, 但非活动计算机没有id值.

想在该xml中使用`qemu:commandline`, 需要设置namespace(即`<domain type='qemu' xmlns:qemu='http://libvirt.org/schemas/domain/qemu/1.0'>`) , 没有这个 XML 的 namespace， 这个参数 libvirt 不认.

`virsh domxml-from-native`提供一个方法将已存在的一组QEMU参数转成可以被libvirt使用Domain XML文件.
`virsh domain-to-natice`可以将libvirt的Domian XML文件转化成一组QEMU参数.

使用该xml:
```bash
# virsh define demo.xml # 将xml文件导入到libvirt虚拟机管理软件中
virsh start qemu-ubuntu # 启动虚拟机. 其中qemu-ubuntu为xml中定义的name(虚拟机别名)
```

### 如何快速大量创建vm
[镜像差量/磁盘差量技术](https://cloud.tencent.com/developer/article/1452081)

### `apt install qemu qemu-kvm qemu-system-x86`中的三者区别
在早先版本中有单独的qemu-kvm模块存在，结合qemu一起做虚拟机工作. qemu 1.3版本后QEMU和QEMU-KVM合二为一了, 因此当需要使用kvm特性时候，只需要增加参数`--enable-kvm`参数使能即可.

qemu-system-x86 是新版，qemu 是旧版本.