# sdn
openflow作为sdn的主要实现, 它的发展史就是SDN的发展史. 因为基于OpenFlow为网络带来的可编程的特性，斯坦福的Nick McKeown教授和他的团队进一步提出了SDN（Software Defined Network，软件定义网络）的概念.

![OpenFlow控制转发分离架构](/misc/img/net/Image00001_net.jpg)

## openflow
### OpenFlow设计思路
OpenFlow协议的思路是网络设备维护一个FlowTable，并且只通过FlowTable对报文进行处理，FlowTable本身的生成、维护和下发完全由外置的控制器实现. 此外，OpenFlow 交换机把传统网络中完全由交换机或路由器控制的报文转发，转换为由交换机和控制器共同完成，从而实现报文转发与路由控制的分离. 控制器则通过事先规定好的接口操作OpenFlow交换机中的流表，达到数据转发的目的.

在 OpenFlow 交换机中，包含了安全通道、多级流表和组表. 通过安全通道，OpenFlow 交换机可以和控制器建立基于 OpenFlow 协议的连接；而流表则用来匹配OpenFlow交换机收到的报文；组表用来定义流表需要执行的动作.

### FlowTable
OpenFlow 通过用户定义的或预设的规则匹配和处理网络包. 一条 OpenFlow 的规则由匹配域、优先级、处理指令和统计数据等字段组成.

在一条规则中，可以根据网络包在L2、L3或者L4等网络报文头的任意字段进行匹配，比如以太网帧的源MAC地址、IP包的协议类型和IP地址或者TCP/UDP的端口号等.

所有OpenFlow的规则都被组织在不同的FlowTable中，而在同一个FlowTable中，按规则的优先级进行先后匹配。一个 OpenFlow Switch 可以包含一个或者多个FlowTable，从0开始依次编号排列.

![数据包处理流程](/misc/img/net/Image00003_net.jpg)

当网络数据包进入Switch后，必须从table 0开始依次匹配，table可以按从小到大的次序越级跳转，但不能从某一table向前跳转至编号更小的table. 当数据包成功匹配一条规则后，将首先更新该规则对应的统计数据（如成功匹配数据包总数目和总字节数等），然后根据规则中的指令进行相应操作，比如跳转至后续某一table继续处理，修改或立即执行该数据包对应的Action Set等. 当数据包已经处于最后一个table时，其对应的Action Set中的所有Action将被执行，包括转发至某一端口、修改数据包的某一字段、丢弃数据包等. OpenFlow规范对目前所支持的Instructions和Actions进行了完整详细的说明和定义.

### OpenFlow通信通道
OpenFlow 协议主要通过对不同类型消息的处理来实现控制器与交换机之间的路由控制.目前，OpenFlow 主要支持三种消息类型，分别是 Controller-to-Switch、Asynchronous（异步消息）及Symmetric（对称消息）:
- Controller-to-Switch：指由 Controller 发起，Switch 接收并处理的消息，主要包括Features、Configuration、Modify-State、Read-Stats和Barrier等消息. 这些消息主要由Controller对Switch进行状态查询和修改配置等操作.
- Asynchronous：由Switch发送给Controller，用来通知Switch上发生的某些异步事件的消息，主要包括Packet-in、Flow-Removed、Port-Status和Error等. 例如，当某一条规则因为超时而被删除时，Switch 将自动发送一条Flow-Removed消息通知Controller，以方便Controller进行相应的操作，比如重新设置相关规则.
- Symmetric：主要用来建立连接，检测对方是否在线等，都是些双向对称的消息，包括Hello、Echo与厂商自定义消息.

### OpenFlow应用
随着OpenFlow以及SDN的发展和推广，其研究和应用领域也得到了不断拓展，比如网络虚拟化、安全和访问控制、负载均衡、绿色节能，以及与传统网络设备交互和整合等. 下面重点介绍网络虚拟化和负载均衡.

1. 网络虚拟化——FlowVisor

网络虚拟化的本质是对底层网络的物理拓扑进行抽象，在逻辑上对网络资源进行分片或整合，从而满足各种应用对于网络的不同需求.

为了达到这个目的，FlowVisor实现了一个特殊的OpenFlow Controller，可以看作其他不同用户或应用的Controller与网络设备之间的一层代理. 因此，不同用户或应用可以使用自己的Controller来定义不同的网络拓扑，同时FlowVisor又可以保证这些Controller之间能够互相隔离且互不影响.

![](/misc/img/net/Image00005_net.jpg)

1. 负载均衡——Aster*x
传统的负载均衡方案一般需要在服务器集群的入口处，通过一个gateway监测、统计服务器的工作负载，并据此将用户请求动态地分配到负载相对较轻的服务器上. 既然网络中的所有网络设备都可以通过OpenFlow进行集中式的控制和管理，同时服务器的负载又可以及时地反馈给OpenFlow Controller，那么OpenFlow就非常适合做负载均衡的工作.

基于OpenFlow的负载均衡模型Aster*x通过Host Manager和Net Manager来分别监测服务器和网络的工作负载，然后将这些信息反馈给FlowManager，这样 Flow Manager 就可以根据这些实时的负载信息，重新定义网络设备上的OpenFlow规则，从而将用户请求（即网络包）按照服务器的能力进行调整和分发

![](/misc/img/net/Image00006_net.jpg)

## sdn
SDN 将控制功能从交换机中剥离出来，形成了一个统一的、集中式的**控制平面**，而交换机只保留了简单的转发功能，从而形成了**转发平面（数据平面）**. 通过控制平面对数据平面的集中化控制，SDN 为网络提供了开放的编程接口，并实现了灵活的可编程能力，从而使网络能够真正地被软件定义，达到按需定制服务、简化网络运维、灵活管理调度的目标.

在SDN中，网络设备只负责单纯的数据转发，可以采用通用的硬件. 如果将网络中所有的网络设备视为被管理的硬件资源，参考操作系统的设计原理，则可以抽象出一个网络操作系统（Network OS）的概念. 这个网络操作系统一方面抽象了底层网络设备的具体细节，负责与网络硬件进行交互，实现对硬件的编程控制和接口操作，同时还为上层应用访问网络设备提供了统一的管理视图和编程接口. 基于这个网络操作系统，用户可以开发各种网络应用程序，通过软件定义逻辑上的网络拓扑，以满足对网络资源的不同需求，而无须关心底层网络的物理拓扑结构.

![SDN基本架构](/misc/img/net/Image00007_net.jpg)

SDN 采用了上图所示的基本架构，集中式的控制平面和分布式的转发平面相互分离，控制平面利用控制器、转发通信接口对转发平面上的网络设备进行集中式管理.

sdn组件包括:
- 基础设施层（Infrastructure Layer）：主要承担数据转发功能，由各种网络设备构成，如数据中心的网络路由器，支持OpenFlow 的硬件交换机等。
- 控制层（Control Layer）：网络转发的控制管理平面，负责管理网络的基础设施，主要组成部分为SDN控制器。SDN控制器是整个网络的大脑、控制中心，主要功能是按照配置的业务逻辑，产生对应的数据平面的流转发规则，通过下发给网络设备，控制其进行数据转发。
- 应用层（Application Layer）：指商业应用。开发者可以通过SDN控制器提供的北向接口，如 REST 接口实现应用和网络的联动，例如网络拓扑的可视化、监控等。
- 南向接口（Sorthbound Interface）:SDN 控制器对网络的控制主要通过OpenFlow、NetConf等南向接口实现，包括链路发现、拓扑管理、策略制定、表项下发等。其中，链路发现和拓扑管理主要是控制其利用南向接口的上行通道对底层交换设备上报信息进行统一的监控和统计，而策略制定和表项下发则是控制器利用南向接口的下行通道对网络设备进行统一的控制。
- 北向接口（Northbound Interface）：北向接口是通过控制器向上层应用开放的接口，其目标是使得应用能够便利地调用底层的网络资源和能力。因为北向接口是直接为应用服务的，因此其设计需要密切联系应用的业务需求，比如需要从用户、运营商或产品的角度去考量。

在SDN发展初期，控制平面的表现形式更多是以单实例的控制器出现，实现SDN的协议也以 OpenFlow 为主，因此 SDN控制器更多指的是 OpenFlow 控制器. 随着SDN的发展，ONF也在白皮书中提出了SDN的架构标准. 广义的SDN支持丰富的南向协议，包括OpenFlow、NetConf、OVSDB、BGPLS、PCEP及厂商协议等，可实现灵活可编程和灵活部署，支持网络虚拟化、SR路由、智能分析和调度.

与南向接口方面已有OpenFlow等国际标准不同，目前还缺少业界公认的北向接口标准. 因此，北向接口的协议制定成为当前SDN领域竞争的一大焦点，不同的参与者或从各种角度提出了很多方案. 据悉，目前至少有20种控制器，每种控制器都会对外提供北向接口，用于上层应用开发和资源编排. 当然，对于上层的应用开发者来说，RESTful API是比较乐于采用的北向接口形式.

### SDN实现
OpenFlow并非实现SDN的唯一途径, 比如IETF定义的开放SDN架构, Overlay网络技术和基于专用接口.

#### IETF定义的开放SDN架构
IETF定义的开放SDN架构的核心思路是重用当前的技术而不是OpenFlow，比如利用Netconf和已有的设备接口. IETF的Netconf使用XML来配置设备，旨在减少与自动化设备配置有关的编程工作量. 这种架构充分地利用了现有设备，能够更大限度地保护已有的投资.

#### Overlay网络技术
Overlay网络技术是在现行的物理 IP 网络基础上建立叠加逻辑网络（Overlay Logical Network），屏蔽底层物理网络差异，实现网络资源的虚拟化，使得多个逻辑上彼此隔离的网络分区，以及多种异构的虚拟网络可以在同一共享的物理网络基础设施上共存.

![Overlay网络](/misc/img/net/Image00007_net.jpg)

Overlay 网络的主要思想可被归纳为解耦、独立、控制三个方面:
- 解耦是指将网络的控制从网络物理硬件中脱离出来，交给虚拟化的Overlay逻辑网络处理
- 独立是指Overlay网络承载于物理IP网络之上，因此只要IP可达，那么相应的虚拟化网络就可以被部署，而无须对原有物理网络架构（例如原有的网络硬件、原有的服务器虚拟化解决方案、原有的网络管理系统、原有的IP地址等）做出任何改变. Overlay网络可以便捷地在现网上部署和实施，这是它最大的优势.
- 控制是指叠加的逻辑网络将以软件可编程的方式被统一控制，网络资源可以和计算资源、存储资源一起被统一调度和按需交付.

Overlay是在现有的网络架构上叠加的虚拟化技术，本质是L2 Over IP的隧道技术. 细分Overlay的技术路线，主要可分为如下三大技术路线
- VXLAN

    VXLAN是将以太网报文封装在UDP传输层上的一种隧道转发模式，VXLAN通过将原始以太网数据头(MAC、IP、四层端口号等)的HASH值作为UDP的号. 采用24比特标识二层网络分段，称为VNI(VXLAN Network Identifier)，类似于VLAN ID作用.

    VXLAN是阿里云VPC采用的技术路线. 阿里云在硬件网关和自研交换机设备的基础上实现了VPC产品. 每个VPC都有一个独立的隧道号，一个隧道号对应着一个虚拟化网络. 一个VPC内的ECS（Elastic Compute Service）实例之间的传输数据包都会加上隧道封装，带有唯一的隧道ID标识，然后送到物理网络上进行传输. 不同VPC内的ECS实例因为所在的隧道ID不同，本身处于两个不同的路由平面，所以不同VPC内的ECS实例无法进行通信，天然地进行了隔离.
- NVGRE

    NVGRE是将以太网报文封装在GRE内的一种隧道转发模式。采用24比特标识二层网络分段，称为VSI，类似于VLAN ID作用。NVGRE在GRE扩展字段flow ID，这就要求物理网络能够识别到GRE隧道的扩展信息，并以flow ID进行流量分担.
- STT

    STT利用了TCP的数据封装形式，但改造了TCP的传输机制，数据传输不遵循TCP状态机，而是全新定义的无状态机制，将TCP各字段意义重新定义，无需三次握手建立TCP连接，因此称为无状态TCP.

这三种技术大体思路都是将以太网报文承载到某种隧道层面，差异性在于选择和构造隧道的不同，而底层均是IP转发. 总体而言，VXLAN利用了现有通用的UDP传输，成熟度高, 具有更明显的优势，L2-L4层链路HASH能力强，不需要对现有网络改造，对传输层无修改，使用标准的UDP传输流量，业界支持度最好，目前商用网络芯片大部分支持.

现在云服务商用的比较多的是基于主机(x86服务器)的Overlay技术，在服务器的Hypervisor内vSwitch上支持基于IP的二层Overlay技术. 主机的vSwitch支持基于IP的Overlay之后，虚拟机的二层访问直接构建在Overlay上，物理网络不再感知虚拟机的诸多特性，从而实现了和虚拟网络的解耦.

#### 基于专用接口
基于专用接口的方案的实现思路是不改变传统网络的实现机制和工作方式，通过对网络设备的操作系统进行升级改造，在网络设备上开发出专用的 API 接口，管理人员可以通过 API 接口实现网络设备的统一配置管理和下发，改变原先需要一台台设备登录配置的手工操作方式，同时这些接口也可供用户开发网络应用，实现网络设备的可编程. 典型的基于专用接口的SDN实现方案是的思科ONE架构.

### sdn转发平面的可编程性.
现有的SDN解决方案为用户开放的是控制平面的可编程能力，在正常情况下，对于转发设备来说，数据包的解析转发流程是由设备转发芯片固化的，所以设备在协议的支持方面并不具备扩展能力. 并且，厂商扩展转发芯片所支持的协议特性，甚至开发新的转发芯片以支持新的协议，代价非常高，需要将之前的硬件重新设计，这样势必导致更新的成本居高不下、时间周期长等一系列问题.

因此，新一代的SDN解决方案必须让转发平面也具有可编程能力，让软件能够真正定义网络和网络设备. 而P4（Programming Protocol-Independent Packet Processors, 是一门主要用于数据平面的编程语言）正是为用户提供了这种能力，打破了硬件设备对转发平面的限制，让数据包的解析和转发流程也能通过编程去控制，使得网络及设备自上而下地真正向用户开放.

因此P4解决数据平面的可编程问题，OpenFlow是解决控制平面的可编程问题.

P4编译器本质上是将在P4程序中表达的数据平面的逻辑翻译成一个在特定可编程数据包处理硬件上的具体物理配置.

## NFV (Network Function Virtualization)
NFV由运营商联盟提出，为了加速部署新的网络服务，运营商倾向于放弃笨重且昂贵的专用网络设备，转而使用标准的IT虚拟化技术拆分网络功能模块. 通过将硬件与虚拟化技术结合，NFV可以实现所有的网络功能.

NFV是网络功能虚拟化. 传统的网络设备基本都是使用专用的硬件设备实现的，比如熟悉的防火墙，F5这样的硬件设备，又如路由器和交换机. 而NFV则是希望使用**普通的x86服务器来代替专用的网络设备**，使网络功能不再依赖专用设备，而是通过x86服务器和软件来实现. NFV除了可以节约成本外，还通过开放API的方式，使得网络功能更灵活，更有弹性.

![ETSI NFV标准框架](/misc/img/net/Image00012_net.jpg)

其中，NFV infrastructure（NFVI）、MANO和VNF（Virtual Network Function）是顶层的概念实体.

NFVI包含了虚拟化层（Hypervisor或容器管理系统，如Docker）及物理资源，如交换机、存储设备等. NFVI可以跨越若干个物理位置进行部署，为这些物理站点提供数据连接的网络也成为NFVI的一部分.

VNF与NFV虽然是三个同样的字母调换了顺序，但含义截然不同。NFV是一种虚拟化技术或概念，解决了将网络功能部署在通用硬件上的问题；而VNF指的是具体的虚拟网络功能，提供某种网络服务，是一种软件，利用NFVI提供的基础设施部署在虚拟机、容器或物理机中。相对于VNF，传统的基于硬件的网元可以称为PNF。VNF和PNF能够单独或混合组网，形成Service Chain，提供特定场景下所需的E2E（End-to-End）网络服务。

MANO提供了NFV的整体管理和编排，向上接入OSS/BSS（运营支撑系统/业务支撑系统），由NFVO（NFV Orchestrator）、VNFM（VNF Manager）及VIM（Virtualised Infrastructure Manager）虚拟化基础设施管理器三者共同组成。

编排（Orchestration）指以用户需求为目的，将各种网络服务单元进行有序的安排和组织，生成能够满足用户要求的服务。在NFV架构中，凡是带“O”的组件都有一定的编排作用，各个VNF、PNF 及其他各类资源只有在合理编排下，在正确的时间做正确的事情，整个系统才能发挥应有的作用。

VIM 主要负责基础设施层虚拟化资源和硬件资源的管理、监控和故障上报，并面向上层 VNFM 和 NFVO 提供虚拟化资源池，负责虚拟机和虚拟网络的创建和管理，OpenStack和VMware都可以作为VIM。VNFM负责 VNF 的生命周期管理，如上线、下线，状态监控。VNFM基于VNFD（VNF Descriptor，描述一个VNF模块部署与操作行为的配置模板）来管理VNF。NFVO负责NS（Network Service）生命周期的管理和全局资源调度。

## 虚拟交换
1. DPDK

DPDK（Data Plane Development Kit）可提供高性能的数据包处理库和用户空间驱动程序。它不同于Linux系统以通用性设计为目的，而是专注于网络应用中数据包的高性能处理。具体体现在 DPDK 绕过了 Linux 内核协议栈对数据包的处理过程，运行在用户空间上利用自身提供的数据平面库来收发数据包。

在最近的一项研究中，使用DPDK的OVS（Open vSwitch）平均吞吐量提高了75%。该技术被英特尔公司推广，可以在多处理器上使用，并作为 EPA（Enhanced Platform Awareness，旨在加速数据平面）技术的一部分。EPA除DPDK以外的主要技术是大页、NUMA和SR-IOV：大页通过减少页面查找提高VNF的效率；NUMA确保工作负载使用处理器本地的内存；SR-IOV可以使网络流量旁路管理程序，直接转到虚拟机。

2. OVS-DPDK

DVS是一个具有工业级质量的多层虚拟交换机，它支持OpenFlow和OVSDB协议。通过可编程扩展，可以实现大规模网络的自动化（配置、管理和维护）。最初的OVS 版本是通过 Linux 内核进行数据分发的，因而用户能够得到的最终吞吐量受限于Linux网络协议栈的性能。

OVS-DPDK使用DPDK技术对Open VSwitch进行优化。OVS-DPDK是用户态的vSwitch，网络包直接在用户态进行处理。

3. FD.IO

FD.IO（Fast Data Input/Output）是Linux基金会旗下的开源项目，LFN六大创始项目之一.

FD.IO在通用硬件平台上提供了具有灵活性、可扩展、组件化等特点的高性能I/O服务框架，该框架支持高吞吐量、低延迟、高资源利用率的用户空间I/O服务，并可适用于多种硬件架构和部署环境。

FD.IO的关键组件来自Cisco捐赠的商用VPP（Vector Packet Processing，矢量分组处理引擎）库。VPP和FD.IO其他子项目如NSH_ SFC、Honeycomb、ONE等一起用于加速数据平面。

所谓 VPP 向量报文处理是与传统的标量报文处理相对而言的。传统报文处理方式的逻辑是：按照到达先后顺序来处理，第一个报文处理完，处理第二个，依次类推；函数会频繁嵌套调用，并最终返回。相比而言，向量报文处理则是一次并行处理多个报文，相当于一次处理一个报文数组，扩展了整个数据包集合的查找和计算开销，从而提高了效率。

## Linux操作系统网络栈
在Linux中，网络分为两个层次，分别是网络协议栈，以及接收和发送网络协议的设备驱动程序. 网络协议栈是硬件中独立出来的部分，主要用来支持TCP/IP等多种协议，而网络设备驱动程序连接了网络协议栈和网络硬件设备. Linux中与网络有关的实现主要有：
- 网络驱动程序
- Linux VLAN：一种虚拟设备，只有绑定一个真实网卡才能完成实际的数据发送和接收
- Linux Bridge（网桥）：工作于二层的虚拟网络设备，功能类似于物理的交换机. 其他Linux网络设备可以被绑定到Bridge上作为从设备，并被虚拟化为端口. 当一个从设备被绑定到Bridge上时，就相当于真实网络中的交换机端口插入了一个连接有终端的网线.
- Linux TCP/IP协议栈：可以处理IP、ICMP、ARP、TCP/UDP/SCTP等协议。
- Linux Socket函数库：从Berkeley大学开发的BSD UNIX系统中移植而来。网络的Socket数据传输是一种特殊的I/O。
- Linux应用层协议：处理更高层的协议，常用的有DNS、HTTP、SSH、Telnet等。

## 网络操作系统
1. OpenDaylight

ODL（OpenDaylight）是由 Linux 基金会和多家行业巨头如 Cisco、Juniper 和Broadcom等公司一起创立的开源项目，其目的在于推出一个通用的SDN控制平台。

ODL支持OpenFlow、Netconf和OVSDB等多种南向接口，是一个广义的SDN控制平台。ODL 支持分布式集群，不仅可以管理更大的网络，性能更好，还可以相互容灾备份，提升系统的可靠性。它包括一系列功能模块，可以动态地组合，提供不同的服务。

ODL 主要的功能模块有拓扑管理、转发管理、主机监测、交换机管理等。ODL控制平台引入了模型驱动的设计思想，构建了服务抽象层 MD-SAL，是控制器模块化的核心，能够自动适配底层不同的设备，使开发者专注于业务应用的开发。

2. ONOS

ONOS（Open Network Operating System）顾名思义就是要定义一个开放的网络操作系统，其核心的服务对象是服务提供商。既然服务对象要达到运营商的级别，那么其重点就需要考虑可靠性与性能，并能够在白盒系统上创建高性能可编程的运营商网络。

ONOS 的北向接口抽象层和 API 可以使得应用开发变得更加简单，而通过南向接口抽象层和API则可以管控OpenFlow或传统设备。北向接口基于具有全局网络视图的框架，南向接口包括 OpenFlow 和 Netconf，以便能够管理虚拟和物理交换机。ONOS的核心是分布式的，因此可以水平扩展，架构如图1-15所示。

ONOS 在诞生之初就是为了对抗 ODL，希望能成为控制器的主流。目前主要的参与者包括 AT&T、CIENA、VERIZON、NTT、爱立信、华为、NEC、INTEL、富士通等。

3. Tungsten Fabric
Tungsten Fabric是由OpenContrail（由Juniper开源的SDN控制器）向Linux基金会迁移并更名而来的。Tungsten Fabric是一个可扩展的多云网络平台，能够与包括Kubernetes和OpenStack在内的多个云平台集成，并且支持私有云、混合云和公有云部署。

## 云平台
1. OpenStack

OpenStack社区将Nova项目中的网络模块和块存储模块剥离出来，成立了两个新的核心项目，分别是 Neutron 和 Cinder.

Neutron通过插件的方式对众多的网络设备提供商进行支持，比如Cisco、Juniper等，同时也支持很多流行的技术，比如Openvswitch、OpenDaylight和SDN等.

Neutron的插件分为Core Plugin和Service Plugin两类。Core Plugin负责管理和维护Neutron的Network、Subnet和Port三类核心资源的状态信息，这些信息是全局的，只需要也只能由一个Core Plugin管理。Havana版本中实现了ML2（Modular Layer 2）Core Plugin用于取代原有的Core Plugin。对三类核心资源进行操作的REST API被neutron-server看作Core API，由Neutron原生支持。
- Network：代表一个隔离的二层网段，是为创建它的租户而保留的一个广播域。Subnet和Port始终被分配给某个特定的Network。Network的类型包括Flat、VLAN、VxLAN、GRE等。
- Subnet：代表一个IPv4/v6的CIDR地址池，以及与其相关的配置，如网关、DNS等，该Subnet中的 VM 实例随后会自动继承该配置。Sunbet必须关联一个Network。
- Port：代表虚拟交换机上的一个虚机交换端口。VM 的网卡 VIF 连接 Port后，会拥有 MAC 地址和 IP 地址。Port 的 IP 地址是从 Subnet 地址池中分配的。

Service Plugin 即为除 Core Plugin 以外其他的插件，包括 l3 router、firewall、loadbalancer、VPN、metering 等，主要实现 L3~L7的网络服务。这些插件要操作的资源比较丰富，对这些资源进行操作的 REST API 被 neutron-server 看作 Extension API，需要厂家自行进行扩展。

OpenStack已被Kubernetes打败.

2. Kubernetes

Kubernetes，又称为k8s（首字母为k、首字母与尾字母之间有8个字符、尾字母为 s，所以简称为k8s），或者简称为“kube”，设计初衷是在主机集群之间提供一个能够自动化部署、可扩展、应用容器可运营的平台。在整个k8s生态系统中，能够兼容大多数的容器技术实现，比如Docker与Rocket(已淘汰)。

## 网络编排

NFV 给网络带来极大的灵活性和敏捷性，但它们的实现依赖于新的管理系统和自动化编排系统。在 NFV 体系中，引入了全新的管理和编排系统——NFV MANO（NFV Management and Orchestration）系统，编排器作为其中的核心部件，是网络灵活调整和资源动态调度的关键，是下一代网管系统的核心。

NFV 编排器由两层构成：服务编排和资源编排，可以控制新的网络服务，并将VNF集成到虚拟架构中，NFV编排器还能验证并授权NFV基础设施（NFVI）的资源请求。VNF管理器能够管理VNF的生命周期。VIM能够控制并管理NFV基础设施，包括了计算、存储和网络等资源。为了使NFV MANO行之有效，它必须与现有系统中的应用程序接口（API）集成，以便跨多个网络域使用多厂商技术。同样，OSS/BSS也需要与MANO实现互操作。

## 网络数据分析
1. PNDA

2016年8月16日，Linux基金会发布了一个网络数据分析平台PNDA（Platform for Network Data Analytics）.

PNDA旨在通过集成、缩放和管理一组公开的数据处理技术，并提供部署分析应用和服务的端到端平台来降低复杂性。PNDA 能够支持批量的实时数据流探索和分析，甚至可以达到每秒数百万消息的规模。

2. SNAS

SNAS（Streaming Network Analytics System）是一个实时跟踪和分析网络路由拓扑数据的框架。系统将从网络的第2层和第3层挖掘和收集数据，包括IP信息、服务质量及物理和设备规范。

## 网络集成
OPNFV（Open Platform for NFV）是运营商级的开源网络集成参考平台。NFV架构里包含多个开源组件，不同开源组件间的集成和测试非常关键。OPNFV则提供了多组件持续开发、集成和测试的开源方案，并不断地向上游组织输出电信级 NFV平台的增强特性。

OPNFV 目前已经集成了 OpenStack、ODL、ONOS、DPDK、ONAP、FD.IO 等多个关键组件，发布了7个版本、超过60多个集成套件和几十个自动化测试工具，为NFV集成和测试提供了大量开源参考方案和自动化框架。

# linux virtual network
![虚拟网络结构](/misc/img/net/Image00019_net.jpg)

虚拟机的网络功能由虚拟网卡（vNIC）提供，Hypervisor可以为每个虚拟机创建一个或多个 vNIC. 站在虚拟机的角度，这些 vNIC 等同于物理的网卡。为了实现与传统物理网络等同的网络结构，与 NIC 一样，Switch 也被虚拟化为虚拟交换机（vSwitch）. 各个vNIC连接在vSwitch的端口上，最后这些vSwitch通过物理Server的物理网卡访问外部的物理网络. 由此可见，一个虚拟的二层网络结构，主要是完成两种网络设备的虚拟化：NIC硬件与交换设备.

## MACVTAP
传统的Linux网络虚拟化技术采用的是TAP+Bridge方式，将虚拟机连接到虚拟的TAP网卡，然后将TAP网卡绑定到Linux Bridge. 这种解决方案实际上就是使用软件，用服务器的CPU模拟网络，但这种技术主要有三个缺点：
- 每台宿主机内都存在Bridge会使网络拓扑变得复杂，相当于增加了交换机的级联层数
- 同一宿主机上虚拟机之间的流量直接在Bridge完成交换，使流量监控、监管变得困难
- Bridge是软件实现的二层交换技术，会加大服务器的负担

针对云计算中的复杂网络问题，业界主要提出了两种技术标准进行扩展：802.1Qbg与802.1Qbh.

802.1Qbh Bridge Port Extension主要由VMware与 Cisco 提出，尝试从接入层到汇聚层提供一个完整的虚拟化网络解决方案，尽可能达到通过软件定义一个可控网络的目的. 它扩展了传统的网络协议，因此需要新的网络设备支持，成本较高.

802.1Qbg Edge Virtual Bridging（EVB）主要由 HP 等公司提出，尝试以较低成本利用现有设备改进软件模拟的网络. 802.1Qbg 的一个核心概念是 VEPA，它通过端口汇聚和数据分类转发，把宿主机上原来由 CPU 和软件来做的网络处理工作转移到接入层交换机上，减轻宿主机的CPU 负载. 同时，使得在一级的交换机上做虚拟机网络流量监控成为可能.

为支持这种新的虚拟化网络技术，Linux 引入了新的网络设备模型——MACVTAP，用来简化虚拟化环境下的桥接网络，代替传统的TAP+Bridge组合，同时支持新的虚拟化网络技术，如 802.1 Qbg. 和 TAP 设备一样，每一个 MACVTAP设备都拥有一个对应的 Linux 字符设备，因此能直接被 KVM/QEMU使用，方便完成网络数据交换工作.

MACVTAP的实现基于传统的 MACVLAN. MACVLAN允许在主机的一个网络接口上配置多个虚拟的网络接口，这些网络接口有自己独立的 MAC 地址，也可以配置 IP 地址进行通信. MACVLAN 下的虚拟机或者容器和主机在同一个网段中，共享同一个广播域. MACVLAN 和 Bridge 比较相似，但因为它省去了 Bridge，所以配置和调试起来比较简单，而且效率也相对更高.

同一个物理网卡上的各个MACVTAP设备，都可以拥有属于自己的 MAC地址和IP地址。使用MACVTAP，管理员不再需要建立网桥br0，并且同时把物理网卡eth0、连接虚拟机的TAP设备tap0和tap1加入网桥br0中，而是只需要在物理网卡eth0上建立两个MACVTAP设备，并让虚拟机直接使用这两个MACVTAP设备就可以了.

MACVTAP设备支持3种操作模式：
- VEPA模式：VEPA模式是默认模式. 在这种模式下，两个在同一个物理网卡上的 MACVTAP 设备（都处于 VEPA 模式）通信，网络数据会从一个MACVTAP 设备通过底层的物理网卡发往外界的交换机. 此时，外界交换机必须支持Hairpin模式，只有这样才可以把网络数据重新送回物理网卡，传送给此物理网卡上的另一个MACVTAP设备.
- 桥接模式：在桥接模式下，同一个物理网卡上的所有桥接模式的MACVTAP设备直接两两互通，它们之间的通信，网络数据不会经过外界交换机
- 私有模式：在私有模式时，类似于VEPA模式时外界交换机不支持Hairpin模式的情况. 此时，同一个物理设备上的MACVTAP设备之间不能通信

## Open vSwitch
Open vSwitch是一个具有产品级质量的虚拟交换机，它使用C语言进行开发，从而充分考虑了在不同虚拟化平台间的移植性，同时它遵循Apache2.0许可，因此对商用也非常友好.

对于虚拟网络来说，交换设备的虚拟化是很关键的一环，vSwitch负责连接vNIC与物理网卡，同时也桥接同一物理Server内的各个vNIC. Linux Bridge已经能够很好地充当这样的角色，为什么还需要Open vSwith呢？

在传统数据中心中，网络管理员通过对交换机的端口进行一定的配置，可以很好地控制物理机的网络接入，完成网络隔离、流量监控、数据包分析、Qos配置、流量优化等一系列工作.

但是在云环境中，仅凭物理交换机的支持，管理员无法区分被桥接的物理网卡上流淌的数据包属于哪个VM、哪个OS及哪个用户，Open vSwitch的引入则使云环境中虚拟网络的管理以及对网络状态和流量的监控变得容易.

比如，可以像配置物理交换机一样，将接入到Open vSwitch（Open vSwitch同样会在物理Server上创建一个或多个vSwitch供各个虚拟机接入）上的各个VM分配到不同的VLAN中实现网络的隔离. 也可以在Open vSwitch端口上为VM配置Qos，同时Open vSwitch也支持包括NetFlow、sFlow很多标准的管理接口和协议，可以通过这些接口完成流量监控等工作.

此外，Open vSwitch也提供了对Open Flow的支持，可以接受Open Flow Controller的管理.

总之，Open vSwitch在云环境中的各种虚拟化平台上（比如KVM）实现了分布式的虚拟交换机，一个物理 Server 上的 vSwitch 可以透明地与另一个 Server上的vSwitch连接在一起.

![Open vSwitch软件结构](/misc/img/net/Image00023_net.jpg)

其中ovs-vswitchd是最重要的模块，实现了虚拟机交换机的后台，负责与远程的Controller进行通信，例如通过OpenFlow协议与OpenFlow Controller通信，通过sFlow协议同sFlow Trend通信. 此外，ovs-switchd也负责同内核态模块通信，基于netlink机制下发具体的规则和动作到内核态的 datapath。datapath 负责执行数据交换，也就是把从接收端口收到的数据包在流表（Flow Table）中进行匹配，并执行匹配到的动作。每个datapath都和一个流表关联，当datapath接收数据后，会在流表中查找可以匹配的Flow，执行对应的动作，比如转发数据到另外的端口。ovsdb-server是一个轻量级的数据库服务器，主要用来记录被ovs-switchd的配置信息。

Open vSwitch还包括了一系列的命令行工具，主要包括：
- ovs-vsctl：查询和更新ovs-vswitchd的配置信息
- ovsdb-client:ovsdb-server的客户端命令行工具
- ovs-appctl：用来配置运行中的Open vSwitch daemon
- ovs-dpctl：用来配置内核模块中的datapath
- ovs-ofctl：通过OpenFlow协议查询和控制OpenFlow交换机和控制器

## Linux Network Namespace
如果不考虑内存、CPU等其他共享的资源，仅从网络的角度来看，Network Namespace就和一台虚拟机一样，它可以在一台机器上模拟出多个完整的协议栈.

每个新的Network Namespace都默认有一个本地回环LO接口，此外，所有的其他网络设备，包括物理/虚拟网络接口、网桥等，只能属于一个Network Namespace，每个Socket也只能属于一个Network Namespace.

创建 Network Namespace 也非常简单，使用 ip netns add 后面跟着要创建的Namespace 名称，如果相同名字的 Namespace 已经存在，会产生"Cannot create namespace"的错误.

ip netns命令创建的Network Namespace会出现在/var/run/netns/目录下，如果需要管理其他不是ip netns创建的Network Namespace，只要在这个目录下创建一个指向对应 Network Namespace 文件的链接就可以.

ip命令提供了ip netns exec子命令，可以在对应的Network Namespace中执行任意命令(但和网络无关的命令执行结果和在外部执行没有区别)，比如要看Network Namespace 中有哪些网卡: `ip netns exec ns1 ip addr`.

有了不同的Network Namespace后，也就有了网络的隔离，而要把两个网络连接起来，Linux提供了VETH pair. 可以把VETH pair当作双向的管道，从一端发送的网络数据，可以直接被另外一端接收到，也可以想象成两个 Namespace 直接通过一个特殊的虚拟网卡连接起来，可以直接通信.

可以使用ip link add type veth 创建一对VETH pair，系统自动生成VETH0和VETH1两个网络接口，如果需要指定它们的名字，则可以使用ip link add vethfoo type veth peer name vethbar，此时创建出来的两个名字就是vethfoo和vethbar. 需要记住的是**VETH pair无法单独存在，删除其中一个，另一个也会自动消失**.

然后，可以把这对VETH pair分别放到创建的两个Namespace里，可以使用ip link set DEV netns NAME来实现.

虽然 VETH pair 可以实现两个 Network Namespace 之间的通信，但是当多个Namespace需要通信的时候，就无能为力了. 涉及多个网络设备之间的通信，首先想到的是交换机和路由器. 因为这里要考虑的只是同一个网络，所以只用到交换机的功能，也就是brigde. 即先创建VETH pair，比如veth0与veth1，再将一个VETH接口veth0加入Network Namespace，另一个VETH接口veth1加入Bridge即可.

## NAT
> 在ipv6逐渐普及的现今, nat不再是重点, 因此这里不会很深入.

网络地址转换（NAT,Network Address Translation）是一种在IP数据包通过路由器或防火墙时重写来源IP地址或目的IP地址的技术. 这种技术被普遍使用在有多台主机通过一个公有IP地址访问外部网络的私有网络中，NAT也可以被称作IP伪装（IP Masquerading），可以分为目的地址转换（DNAT）和源地址转换（SNAT）两类.

DNAT 主要用在从公网访问内部私网的网络数据流中. 比如从公网访问地址为IP1的公网IP地址，NAT网关根据设定的DNAT规则，把IP数据报文包头内的目的IP地址IP1修改为内部的私网IP地址192.168.1.10，从而把IP数据报文发送给地址为192.168.1.10的内部服务器. DNAT可以用来将内部私网服务器上的服务暴露给公网使用.

SNAT 主要应用在从内部私网访问公网的网络数据流中. 比如内部私网 IP 地址为192.168.1.20的机器想访问外部公网IP地址为IP2的服务，NAT网管根据设定的SNAT规则，把IP数据报文包头内的源IP地址192.168.1.20修改为NAT网关自己的公网IP地址IP1。这样内网中没有公网IP地址的机器也能访问外部公网中的服务了.

Linux中的NAT功能一般通过iptables/nftables实现. iptables/nftables是基于Linux内核的功能强大的防火墙. eBPF也实现了NAT.

## 虚拟网络隔离技术
### 1. 虚拟局域网（VLAN）

LAN（Local Area Network，本地局域网）中的计算机通常使用Switch连接. 一般来说，两台计算机连入同一个Switch时，它们就在同一个 LAN 中. 一个 LAN 表示一个广播域，含义就是：LAN 中的所有成员都会收到任意一个成员发出的广播包.

VLAN（Virtual LAN，虚拟局域网）表示一个带有VLAN功能的Switch能将自己的端口划分出多个LAN. 计算机发出的广播包可以被同一个LAN中的其他计算机收到，但位于其他LAN的计算机则无法收到。简单地说，**VLAN将一个交换机在逻辑上分成了多个交换机，限制了广播的范围，在二层将计算机隔离到不同的VLAN中**.

需要注意的是，VLAN实现的只是二层的隔离，比如二层广播包arp无法跨越 VLAN 的边界，但在三层上（比如IP地址）是可以通过路由器让两个VLAN互通的.

使用VLAN，能够更好地控制广播风暴，提高网络整体的安全性，也能使网络管理更加简单直观. 不同的VLAN由VLAN tag（VID）标明，IEEE 802.1Q规定了VLAN tag的格式. 在Linux上要使用VLAN，需要加载8021q的内核模块.

现在的交换机几乎都是支持 VLAN 的. 交换机的端口通常有两种配置模式：Access和Trunk:

其中，Access 端口被打上了 VLAN tag，表明该端口属于哪个 VLAN,Access口只能属于一个 VLAN。Access 端口都是直接与计算机网卡相连的，这样从该网卡出来的数据包流入Access端口后就被打上了所在的 VLAN tag。

Trunk 端口一般用于交换机之间的连接，可以允许多个 VLAN 通过，可以接收和发送多个 VLAN 的报文。如果划分了 VLAN，但是没有配置 Trunk，那么交换机之间的每个VLAN 间通信都要通过一条线路来实现。

![Linux VLAN](/misc/img/net/Image00042_net.jpg)

在Linux中可以通过Linux Bridge、物理网卡等模拟交换机的VLAN环境。上图中, eth0 是宿主机上的物理网卡，有一个命名为eth0.10的子设备与之相连。eth0.10 就是 VLAN 设备，其 VLAN ID 就是 VLAN 10。eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚拟机 VM1 的虚拟网卡 vent0也挂在brvlan10 上。

这样配置的效果等同于宿主机用软件实现了一个虚拟交换机，上面定义了一个VLAN10, eth0.10和vnet0 都分别接到 VLAN10 的 Access端口上。而 eth0 就相当于一个 Trunk 端口，VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。

但是Linux在VLAN模拟上有一个不足，即需要多少个VLAN，就得创建多少个Bridge,Trunk端口也需要创建同样数量的类似eth0.10的虚口。这是由于**Bridge的转发表没有VLAN tag的维度**，要实现不同VLAN独立转发，只能使用多个Bridge实例实现转发表的隔离.

## 2. VxLAN
VxLAN（Virtual Extensible Local Area Network，虚拟局域网扩展）是基于隧道（Tunnel）的一种网络虚拟化技术.

隧道是一个虚拟的点对点的连接，提供了一条通路使封装的数据报文能够在这个通路上传输，并且在通路的两端分别对数据报文进行封装及解封装。某个协议的报文要想穿越IP网络在隧道中传输，必须要经过封装与解封装两个过程。隧道提供了一种某一特定网络技术的PDU穿过不具备该技术转发能力的网络的手段，如组播数据包穿过不支持组播的网络.

**VxLAN将二层报文用三层协议进行封装，可以对二层网络在三层范围内进行扩展**, 比如把二层网络的整个数据帧封装在UDP报文中，送到VxLAN隧道对端。隧道对端的虚拟或者物理网络设备再将其解封装，取出里面的二层网络数据帧发送给真正的目的节点.

VxLAN协议头使用了24bit表示VLAN ID，可以支持1600多万个VLAN ID。RFC协议7348号中定义了VxLAN协议。

VxLAN应用于数据中心内部，使虚拟机可以在互相连通的三层网络范围内迁移，而不需要改变IP地址和MAC地址，保证业务的连续性。

## 3. 通用路由封装GRE
GRE（RFC1701）也是基于隧道的一种网络虚拟化技术. 与VxLAN相比，GRE使用的是IP报文而非UDP作为传输协议。同时，不同于VxLAN只能封装二层以太网数据帧，GRE可以封装多种不同的协议，包括IP报文（RFC2784,RFC2890）、ARP、以太网帧（NVGRE,RFC 7637）等.

相比于VxLAN,GRE更加灵活，可以支持的协议也更多。但是目前物理网卡支持GRE协议的还不是很多，大部分GRE协议的处理还要依靠主机CPU，会增加CPU的负载。

## 4. 通用网络虚拟化封装（Geneve）
为了应对 VLAN 只能有4094的上限，利用隧道技术，产生了诸如 VxLAN、NVGRE、STT（无状态传输隧道）等多种技术来实现虚拟网络的隔离要求。但是这类技术互相不能兼容，所以提出了通用网络虚拟化封装 Geneve（Generic Network Virtualization Encapsulation）.

Geneve技术的RFC正式标准还没产生，还处于IETF草案的阶段。Geneve主要的目的是适应虚拟网络技术的发展和隔离要求，定义一种通用的网络虚拟化隧道封装协议，能够尽可能地兼容目前的VxLAN、NVGRE等正式RFC标准的功能，并且提供高可扩展性来应对以后虚拟网络技术的发展。

Geneve综合了VxLAN和NVGRE两者的特点，首先使用了UDP作为传输协议，同时吸收了GRE可以封装多种不同类型的数据包的优点。

# 高性能数据平面
数据平面的性能在很大程度上取决于网络 I/O 的性能，而网络数据包从网卡到用户空间的应用程序需要经历多个阶段, 如下图

![Linux VLAN](/misc/img/net/Image00050_net.jpg)

当数据包到达网卡后，通过 DMA（Direct Memory Access）复制到主机的内存空间并触发中断，网络协议栈处理完数据分组后再交由用户空间的应用程序进行处理，整个过程的多个阶段都存在着不可忽视的开销，主要有以下几点:
1. 网卡中断

轮询与中断是操作系统与硬件设备进行 I/O 通信的两种主要方式. 在一般情况下，网络数据包的到来都是不可预测的，若采用轮询模式，则会造成很高的CPU负载，因此主流操作系统都会采用中断的方式来处理网络的请求.

然而，随着高速网络接口等技术的迅速发展，10 Gbit/s、40 Gbit/s 甚至100 Gbit/s的网络接口已经出现. 随着网络I/O速率的不断提升，网卡面对大量的高速数据分组将会引发频繁的中断，每次中断都会引起一次上下文切换（从当前正在运行的进程切换到因等待网络数据而阻塞的进程），操作系统都需要保存和恢复相应的上下文，造成较高的时延，并引起吞吐量下降.

2. 内存拷贝

为了使位于用户空间的应用程序能够处理网络数据，内核首先需要将收到的网络数据从内核空间拷贝到用户空间，同样，为了能够发送网络数据，也需要进行从用户空间到内核空间的数据拷贝。每次拷贝都会占用大量的 CPU、内存带宽等资源，代价昂贵。

3. 锁

在Linux内核的网络协议栈实现中，存在大量的共享资源访问。当多个进程需要对某一共享资源进行操作时，就需要通过锁的机制来保证数据的一致。然而，为共享资源上锁或者去锁的过程中通常需要几十纳秒。此外，锁的存在也降低了整个系统的并发性能。

4. 缓存未命中

缓存能够有效提高系统性能，如果因不合理的设计而造成频繁的缓存未命中，会严重削弱数据平面的性能。以Intel XEON 5500为例，在L3（Last Level Cache）命中与未命中条件下，数据操作耗时相差数倍。如果在系统设计时忽视这一点，存在频繁的跨核调用，由此带来的缓存未命中会造成严重的数据读写时延，从而降低系统的整体性能。

接下来，针对这些开销因素，介绍一些主要的优化技术手段和项目.

## 高性能数据面基础
对于数据面的包处理而言，使用的主流硬件平台一般大致可分为硬件加速器、网络处理器及通用处理器。依据处理复杂度、成本、功耗等因素的不同，这些硬件平台在各自特定的领域发挥着作用。硬件加速器和网络处理器由于其专属性，往往具有高性能、低成本的优点。而通用处理器则在复杂多变的数据包处理上更有优势。同时，通用处理器由于性能的不断提升及其丰富的生态，为软件定义网络（SDN）提供了快速迭代的平台。

下面就通用处理器上的高性能数据包处理做一些介绍，包含高速数据面的软件开发技术，处理器平台上提供的有助于提升数据面处理性能的硬件特性。

### 1. 内核旁路
在通用处理器上开发高性能数据处理应用，首先要考虑的问题是选择一个好的开发平台。现有的主流开发平台有两大类：一类是基于操作系统的内核；另一类是内核旁路方案，即绕过内核中的低效模块，直接操作硬件资源。在NFV应用中，从性能方面考虑，选择后者的居多.

1. 内核的性能问题
在操作系统的设计中，内核通过硬件抽象和硬件隔离的方法，给上层应用程序的开发带来了简便性，但也导致了一些性能的下降. 在网络方面，主要体现在整体吞吐率的减少和报文延迟的增加上. 这种程度的性能下降对大多数场景来说可能不是问题，因为整体系统的瓶颈更多地会出现在业务处理逻辑和数据库上面. 但对NFV这样的纯网络应用而言，内核的性能就有些捉襟见肘，性能优化显得很有必要. 特别是随着网络硬件的发展，10G 网卡是服务器的入门级配置，25G 网卡正在普及，100G, 200G, 400G网卡也在应用中，内核所带来的性能下降是高速网络应用急需解决的问题.

![数据包在内核中的处理](/misc/img/net/Image00051_net.jpg)

从上图中可以看出一个数据包从网卡到应用程序要经过内核中的驱动、协议栈处理，然后从内核的内存复制到用户空间的内存中，加上系统调用要求的用户到内核空间的切换，都会导致内核性能的下降.

2. 内核旁路技术
内核旁路技术，就是应用程序不通过内核而是直接操作硬件, 没有用户空间和内核空间之间的切换损耗.

3. 开源方案

内核旁路之后，应用程序直接和硬件打交道，但也需要解决硬件的抽象接口、内存分配和CPU调度等问题，甚至还有网络协议栈的处理. 这方面有DPDK、Netmap、OpenOnload及XDP等开源框架，在一定程度上起到了硬件抽象和隔离功能，简化了应用程序开发.

1. DPDK

DPDK是一个全面的网络内核旁路解决方案，不仅支持众多的网卡类型，也有多种内存和CPU调度的优化方案. 在DPDK之上还有VPP、fstack等网络应用和网络协议栈的实现.

1. Netmap

Netmap是一个高效的收发报文的 I/O 框架，已经集成在 FreeBSD 的内部，也可以在Linux下编译使用. 和DPDK不同的是，Netmap并没有彻底地从内核接管网卡，而是采用一个巧妙的Netmap ring结构来从内核管理的网卡上直接接收和发送数据。

现代网卡一般都使用多个缓冲区（buffer），并有一个叫NIC ring的环形数组。这些缓冲区是操作系统和网卡硬件共享的，网卡将接收的网络数据放到这些缓冲区之后，操作系统能通过相应的mbufs指针读出，发包的流程则正好相反。

Netmap 把网卡的缓冲区从内核映射到用户空间，并且实现了自己的发送和接收报文的netmap_ring来对应网卡的 NIC ring。现代网卡一般都支持多队列，每个队列对应着一个netmap_ring。

将Netmap接口（netmap_if）绑定到网卡时，应用程序可以选择附加一个或多个Netmap ring。可以提高单个进程的吞吐量和灵活性；而如果只使用一个Netmap ring的话，则可以通过每个Netmap ring 对应一个进程/CPU core的方式来构建多进程的高性能系统.

3. OpenOnload

OpenOnload是一个开源的、高性能的Linux应用程序加速器，可为TCP和UDP应用提供更低的、可预测的延迟和更高的吞吐率。和DPDK与Netmap不同的是，前两者都是高性能的 I/O 框架，而 OpenOnload 更多的是一个内核旁路的协议栈。OpenOnload在用户空间实现了TCP和UDP的协议处理，又通过和内核共享部分协议栈信息的方式较好地解决了应用程序的兼容性问题，在金融等领域应用较为广泛。OpenOnload虽然是开源项目，但由于一些知识产权的限制，现在只能用在Solarflare及获得其许可的网卡上。

OpenOnload的底层I/O主要通过EF_VI技术来实现。EF_VI绕过内核协议栈把网卡中部分网络流量直接发送到用户空间的协议栈中, 比如交由DPDK用户空间的api来处理。每个 EF_VI实例可以访问一条特定的 RX 队列，RX队列对内核是不可见的。在默认情况下，这个队列也不接收数据，直到创建一个 EF_VI “过滤器”把数据导入队列中。这个过滤器只是一个隐藏的流控制规则。用户用 ethtool等常用工具看不到这个规则，但实际上它已经存在网卡中了。对于 EF_VI 来说，除了分配 RX 队列并且管理流控制规则，剩下的任务就是提供一个API 让用户空间可以访问这个队列。

另外一部分流量仍然保留在内核中进行处理，这种技术能够灵活地利用内核和旁路方案两方面的优势，在 DPDK 社区称为“分叉驱动”。要使用这种技术，需要一个支持多队列的网卡，同时也要支持流控制和SR-IOV。

4. XDP
XDP绕过了内核的协议栈部分，在继承内核的I/O部分的基础上，提供了介于原有内核和完整内核旁路之间的另一种选择.

![XDP报文处理流程](/misc/img/net/Image00056_net.jpg)
上图为XDP报文处理流程，中间部分是XDP的包处理引擎。这个引擎采用了一个BPF（Berkeley Packet Filter）的程序解释器，能够把XDP的业务逻辑从内核中隔离出来。即使XDP的业务代码出现错误，也不会导致内核的崩溃，达到了完整内核旁路技术类似的效果。内核的 I/O 部分接收报文之后，直接交给 XDP，由XDP的业务逻辑决定报文的下一步是直接丢弃，是转发，还是本地处理。XDP绕过了内核原先的协议栈处理之后，性能得到较大的提高，是现在内核NFV高速网络处理方面一个不错的选择。

### 2. 平台增强
以IA（Intel Architecture）多核通用处理器举例.

1. 多核及亲和性

利用CPU的亲和性能够使一个特定的任务在指定的核上尽量长时间地运行而不被迁移到其他处理器。在多核处理器上，每个核自己本身会缓存着任务使用的信息，而任务可能会被操作系统调度到其他核上。每个核之间的L1、L2缓存是非共享的，如果任务频繁地在各个核间进行切换，就需要不断地使原来核上的缓存失效，如此一来缓存命中率就低了。当绑定核后，任务就会一直在指定的核运行，大大增加了缓存的命中率。对网络包处理而言，显然可以提高吞吐量和降低延时。

2. Intel数据直接 I/O 技术
Intel数据直接 I/O（Data Direct I/O）技术简称DDIO，是从Intel Xeon E5系列处理器开始引进的功能. DDIO技术能够支持以太网控制器将I/O流量直接传输到处理器高速缓存（LLC）中，缩短将其传输到系统内存的路线，从而降低功耗和I/O延迟。同时，DDIO不依赖外部设备并不需要任何软件的参与.

在没有DDIO的系统中，来自以太网控制器的报文通过DMA最先进入处理器的系统内存，当CPU核需要处理这个报文时，它会从内存中读取该报文至缓存，也就是说在CPU真正处理报文之前，就发生了内存的读和写。同样地，如果处理器发送一个报文，需要从内存中读取该报文并写入缓存，再将报文回写到内存中，之后通知以太网控制器通过DMA发送出去。

在具有DDIO的系统中，来自以太网控制器的报文直接传输至缓存，对于报文的数据处理来说，避免了多次的内存读写，在提高性能、降低延时的同时也降低了功耗。

3. Hugepage (大页)
对于Linux实现而言，只采用了分页机制，而没有用分段机制，这样虚拟地址和线性地址总是一致的.

> intel的段机制也转成分页来处理.

分页机制是指把物理内存分成固定大小的块，按照页表管理，一般常规页的大小为4KB。按下图, 如果按照常规页的大小，将线性地址映射为物理地址，需要读取至少三次页目录表和页表，也就是为了完成这个转换需要访问四次内存。为了加快处理器的内存地址转换过程，处理器在硬件上对页表做了缓存，就是 TLB（Translation Look-aside Buffer），它存储了从线性地址到物理地址的直接映射。当处理器需要进行内存地址转换时，它先查找TLB，如果TLB命中，则无须多次访问页表就可以直接得到最终的物理地址，大大缩短了地址转换的时间。如果TLB不命中，则读取内存中的页表按下图进行地址转换，如果在页表中都没找到索引，则产生缺页中断，重新分配物理内存，再进行地址转换.

![从线性地址到物理地址转换（4KB页）](/misc/img/net/Image00060_net.jpg)

随着程序的变大或者程序内存使用的增加，TLB也就变得十分有限，导致TLB不命中的情况出现.

大页的出现改善了这一状态。大页，顾名思义，就是分页的基本单位变大，下两图所示，可以采用2MB或者1GB的大页。它可以减少页表级数，也就是地址转换时访问内存的次数，同时减少TLB不命中的情况。一个使用了2MB内存的程序，TLB中只需要存有1个页表表项就能保证不会出现TLB不命中的情况。对于网络包处理程序，内存需要高频访问，在设计程序时，可以利用大页尽量独占内存防止内存溢出，提高TLB命中率。

![从线性地址到物理地址转换（2MB页）](/misc/img/net/Image00062_net.jpg)
![从线性地址到物理地址转换（1GB页）](/misc/img/net/Image00063_net.jpg)

4. NUMA

在多核处理器平台中，有时需要将多个处理器像单一系统那样运转，则需要具备对多个处理器及其内存系统进行管理的模式。一般有两个模式：对称多处理（SMP）和非一致性内存访问（NUMA）。SMP 模式将多个处理器、内存系统和 I/O 设备都通过一条总线连接起来。在SMP模式下，所有的硬件资源都是共享的，多个处理器之间没有区别、平等地访问内存和I/O外部设备，并且每个处理器访问内存的任何地址所需时间是相同的，因此SMP也被称为一致内存访问结构（UMA,Uniform Memory Access Architecture）。

很显然，SMP 的缺点是扩展性有限，每一个共享的环节都可能造成系统扩展的瓶颈，而最受限制的则是内存。当内存访问达到饱和的时候，增加处理器并不能获得更高的性能，系统总线成为效率瓶颈；处理器与内存之间的通信延迟也增大。

NUMA（Non-Uniform Memory Access Architecture）即非一致性内存访问技术，它的基本特征是具有多个处理器模块（Node），每个处理器模块具有独立的本地内存、I/O 设备等，处理器模块之间通过高速互联的接口连接起来。由于 Node 访问本地内存比访问其他节点的内存的速度要快一些，为了解决非一致性访问内存对性能的影响，NUMA调度器负责将进程尽量在同一节点的CPU之间调度，除非负载太高，才迁移到其他节点。

NUMA技术解决了SMP系统可扩展性问题，它已成为当今高性能服务器的主流体系结构之一。比如Intel Xeon 5500系列系统，2颗CPU支持NUMA的系统结构，每颗CPU物理上有4个核心。利用NUMA技术，在设计数据包处理程序时，在内存分配上使处理器尽量使用靠近其所在节点的内存，可以水平扩展包处理能力。

### 3 DPDK
DPDK的广泛应用很好地证明了IA多核处理器可以解决高性能数据包处理的需求。其核心思想可以归纳成以下几个方面：
- 轮询模式：DPDK 轮询网卡是否有网络报文的接收或放送，这样避免了传统网卡驱动的中断上下文的开销，当报文的吞吐量大的时候，性能及延时的改善十分明显。
- 用户态驱动：DPDK 通过用户态驱动的开发框架在用户态操作设备及数据包，避免了不必要的用户态和内核态之间的数据拷贝和系统调用。同时，为开发者开拓了更广阔的天地，比如快速迭代及程序优化。
- 降低访问存储开销：高性能数据包处理意味着处理器需要频繁访问数据包。显然降低访问存储开销可以有效地提高性能。DPDK使用大页降低TLB 未命中率，保持缓存对齐避免处理器之间缓存交叉访问，利用预取等指令提高缓存的访问率等。
- 亲和性和独占：利用线程的CPU亲和绑定的方式，将特定的线程指定在固定的核上工作，可以避免线程在不同核间频繁切换带来的开销，提高可扩展性，更好地达到并行处理提高吞吐量的目的。
- 批处理：DPDK 使用批处理的概念，一次处理多个包，降低了一个包处理的平均开销。
- 利用IA新硬件技术：IA的新指令、新特性都是DPDK挖掘数据包处理性能的源泉。比如利用vector指令并行处理多个报文，原子指令避免锁开销等。
- 软件调优：软件调优散布在 DPDK 代码的各个角落，包括利用 threshhold的提高 PCI 带宽的使用率，避免 Cache Miss（缓存不命中）以及 Branch Mispredicts（分支错误预测）的发生等。
- 充分挖掘外部设备潜能：以网卡为例，一些网卡的功能，例如 RSS、Flow director、TSO等技术可以被用来加速网络的处理。比如RSS可以将包负载分担到不同的网卡队列上，DPDK 多线程可以分别直接处理不同队列上的数据包。除以太网设备网卡以外，DPDK现已支持多种其他设备，例如crypto设备，这些专用硬件可以被DPDK应用程序用来加速其网络处理。

1. 开发模型

基于上面的技术点，DPDK建议用户使用两种开发模型：
- Run-to-Completion模型

    Run-to-Completion 模型指一个报文从收到、处理结束，再发送出去，都由一个核处理，一气呵成。该模型的初衷是避免核间通信带来的性能下降。如下图所示，在该模型下，每个执行单元在多核系统中分别运行在各自的逻辑核上，也就是多个核上执行一样的逻辑程序。为了可线性扩展吞吐量，可以利用网卡的硬件分流机制，如RSS，把报文分配到不同的硬件网卡队列上，每个核针对不同的队列轮询，执行一样的逻辑程序，从而提高单位时间处理的网络量。

    ![](/misc/img/net/Image00065_net.jpg)

- Pipeline模型

    虽然 Run-to-Completion 模型有许多优势，但是针对单个报文的处理始终集中在一个CPU核，无法利用其他CPU核，并且程序逻辑的耦合性太强，可扩展性有限。Pipeline模型的引入正好弥补了这个缺点，它指报文处理像在流水线上一样经过多个执行单元。如下图所示，在该模型下，每个执行单元分别运行在不同的CPU核上，各个执行单元之间通过环形队列连接。这样的设计可以将报文的处理分为多步，将不同的工作交给不同的模块，使得代码的可扩展性更强。

    ![](/misc/img/net/Image00066_net.jpg)

2. 实现框架

DPDK由一系列可用于包处理的软件库组成，能够支持多种类型设备，包括以太网设备、加密设备、事件驱动设备等，这些设备以PMD（Polling Mode Driver）的形式存在于DPDK中，并提供了一系列用于硬件加速的软件接口.

- 核心库（Core Libraries）：这部分是DPDK程序的基础，它包括系统抽象内存管理、无锁环、缓存池等
- 流分类（Packet Classification）：支持精确匹配、最长匹配和通配符匹配，提供常用的包处理查表操作
- 软件加速库（Accelerated SW Libraries）：一些常用的包处理软件库的集合，比如IP分片、报文重组、排序等
- Stats：提供用于查询或通知统计数据的组件
- QoS：提供网络服务质量相关组件，比如限速（Meter）和调度（Scheduler）
- 数据包分组架构（Packet Framework）：提供了搭建复杂的多核Pipeline模型的基础组件

3. 核心库

核心库是DPDK程序的核心也是基础，几乎所有基于DPDK开发的程序都依赖它。核心库包括系统抽象层、内存管理、无锁环、缓存池等。

系统抽象层屏蔽了各种特异环境，为开发者提供了一套统一的接口，包括DPDK的加载/启动；支持多进程和多线程；核亲和/绑定操作；系统内存的管理；总线的访问，设备的加载；CPU特性的抽象；跟踪及调试函数；中断的处理；Alarm处理。

除系统抽象层以外，无锁环、MemPool及Mbuf的管理也是DPDK的核心所在。

DPDK的rte_ring结构提供了一个支持多生产者和多消费者的无锁环。它是一个先进先出（FIFO）队列，简单且高速，支持成批进队列和出队列。它已用于Memory Pool 的管理，同时也可以作为不同执行单元间的通信方式。其结构可以简单地表示为环的形式，生产者和消费者逐自进入各自的Head和Tail指针控制环中对象的移动（入队列，出队列）。

DPDK的rte_mempool是负责管理从内存中分配mempool的库。mempool是一个对象池，池中的对象用rte_ring管理。在mempool中还引入了Object Cache（对象缓存）的概念，用于加速对象的分配和释放过程。具体可参见DPDK的开发者手册。

DPDK的rte_mbuf则提供了一种数据结构，它可用于封装网络帧缓存或控制消息缓存。rte_mbuf以ring的形式存在于MemPool中，rte_mbuf就是mempool中的对象。mbuf的结构经过精心设计，其头部大小为两个Cache Line（缓存行），原则上将基础性的、频繁访问的数据放在第一个Cache Line，而将功能扩展性的数据放在第二个Cache Line。对于单个mbuf存放不下的大数据包，mbuf还有指向下一个mbuf结构的指针来形成帧链表的结构。

## NFV和NFC基础设施
### 1. 网络功能虚拟化
当使用普通的服务器平台作为运行网络功能的目标平台时，每一个网络功能业务都希望通过基础设施层获得最大可能的网络带宽。基础设施层通常用PCIe网络设备将数据包引入通用处理器，当然，这样的PCIe网络设备在裸机上是物理设备，而在虚拟机上则是虚拟设备。

1. NFVi数据平面加速

对于一个VNF（虚拟化网络功能）应用，快速地从NFVi获取网络帧是后续业务逻辑的基础，这就涉及虚拟主机接口（Host I/O Interface）。从NFVi的视角来看，虚拟主机接口是其面向虚拟主机提供的北向虚接口；从VNF的视角来看，虚拟主机接口是承载其运行的主机I/O设备。

配合不同类型虚拟主机接口，NFVi 提供了不同的数据面策略，Bypass 和 Relay就是两种比较典型的数据面策略。从数据面的角度，前者依赖外部系统提供NFVi数据面，绕过了整个Host软件部分，后者则由Host软件提供NFVi数据面。

再回过头看虚拟主机接口。网络设备按照不同的虚拟化实现方式，可以粗略地分为全模拟（Fully-Emulated）、半虚拟化（Para-Virtualized）和硬直通（Pass-thru）。对于主流的VMM及其网络设备，DPDK支持相对都比较完善。全模拟和半虚拟化类型的虚拟主机接口主要与 NFVi 的 Relay 策略一起工作，而硬直通 NFVi 一般采用Bypass 策略。如下图所示，E1000就是由 VMM 全模拟的设备接口，Virtio 是QEMU/KVM下的半虚拟化设备，VF则是基于SR-IOV的功能，可用于硬直通。

![不同的虚拟主机接口实现方式](/misc/img/net/Image00078_net.jpg)

在网络功能虚拟化场景下，对网络带宽都有一定的要求。相对于全模拟设备方式，半虚拟化和硬直通是更为主流的使用方式.

2.半虚拟化

Virtio是QEMU/KVM下的半虚拟化设备框架，支持多种设备类型，设备定义规范也以开源社区的方式维护。

virtio-net是网卡设备类型的代表。该设备整体由QEMU模拟，例如当Virtio-net基于PCIe总线时，QEMU通过Trap-Emulation的方式模拟了访问PCIe Bus、PCIe CSR、BAR Registers、Interrupt等行为。设备中传输的网络帧，本质是通过共享内存的方式，由虚拟主机内的前端设备驱动和后端设备双方按照队列规范进行入队列和出队列的操作。

QEMU为Virtio后端设备的实现提供了一层vHost抽象，这样无论QEMU进程本身、Kernel，还是另外的独立进程，都可以有不同的适配。vHost-user 就是一个独立于QEMU进程的Virtio设备后端的实现接口，QEMU进程和独立进程按照vHost协议通过Unix Socket互相交互。

如果给这些交互分类，可以分为与设备业务相关部分和无关部分。与设备相关的部分包括功能协商、设置多队列、MTU等。与设备业务无关的部分，主要是搭建前后端共享数据通道，即前端设备如何与后端设备共享内存并互相通知。在这部分工作完成之后，后端设备软件就可以向共享的队列里进行入队列和出队列的操作。

通过 DPDK 的 vHost-user 库，用户可以在自己的进程中轻松地与虚拟机进行网络帧的传输。在Open vSwitch中，DPDK模式的NETDEV（即OVS-DPDK）便是使用vHost-user构建其面向虚拟主机的虚接口。

由于Virtio的设备驱动默认后端是由软件在相同架构的CPU上进行数据入队列和出队列的操作，故而被归到半虚拟化设备中。而随着Virtio硬化的出现，这种分类方式的边界也会逐渐变得模糊，这在后续的章节中会继续展开。

总的来说，类似Virtio的这种半虚拟化设备有如下一些特点：
- 开放的标准化设备。
- 相对较好的性能。
- 硬件设备无关性。

由于该类虚拟主机接口需要接入软件定义的 NFVi 数据面，这样天然地将 VNF与硬件设备解耦。另一方面，由于其设备完全由软件模拟，状态热迁移也更可实现。在使用DPDK vHost后端实现对接QEMU的vHost-user后，其I/O性能提高了一个数量级，并可以随着CPU的数量线性提升，使得其在NFV场景下的可用性大大提升。

3. 硬直通

硬直通将一个硬件设备的能力直接赋予某一个虚拟主机，使得虚拟主机可以获得和裸机下极其相近的性能。但一台设备如果需要被赋予多个虚拟主机，就需要设备能够被切片，或者说能有设备及总线级别的多路复用技术。

这是为什么硬直通通常和SR-IOV联系在一起的原因，SR-IOV是一种PCIe总线多路复用技术。支持SR-IOV的设备，可以将自身切分成多个VF（Virtual Function），它们与PF（Physical Function）一样，在主机侧呈现为独立标准的PCIe设备。SR-IOV是一种总线虚拟化技术，或者多路复用技术，VF本身并不一定必须使用在虚拟人机里。真正将其与虚拟机结合的，是硬直通技术本身。

那硬直通的本质是什么呢？其主要解决了三部分的问题：设备BAR配置空间的访问、DMA 内存请求的直达、中断请求的送达。第一部分有很多方法解决，比如trap-emulation的方式，或通过MMIO的页表映射。第二部分和第三部分需要平台特性的支持，这个特性就是IOMMU（比如Intel VT-d、AMD-Vi）。

IOMMU支持DMA重映射和中断重映射。DMA重映射支持一级甚至二级地址重映射，使得DMA请求可以使用虚拟I/O地址访问主存，不再要求DMA地址的物理连续性。中断重映射支持将设备中断重定向到vCPU的虚拟中断控制器。所以，可以说，硬直通是直接得益于平台I/O虚拟化技术的。

DPDK驱动对网卡设备的驱动支持非常全，支持SR-IOV的大多数厂商都提供了VF驱动，可以在厂商驱动目录下找到相关文件。

使用硬直通和SR-IOV技术的VF在VM中有一些特点：
- 几乎和裸机一致的性能。
- 设备驱动对运行环境（VM或Bare-Metal）无感知。
- 硬件设备依赖。

硬直通在带来出色性能的同时也引入了另一个课题，就是硬直通如何友好地支持云化。首当其冲的挑战是如何支持热迁移。

另一个课题由PF/VF模型引入，VF往往不被赋予一些改变设备全局设置的能力，它需要 PF 作为代理操作，这样就需要一个 VF 和 PF 之间的消息通道。VFD（VF Daemon）是由AT&T公司发起的一个开源项目，该项目给出了一种实现方式，统一抽象不同厂商可能共同面临的一个 VF 代理请求的工作，它的工作方式如下图所示。DPDK各驱动也对该方案提供全面的支持。

![VFD的工作方式](/misc/img/net/Image00079_net.jpg)

4. 下一代虚拟主机接口

NFV 剧烈地重构着网络形态，并依旧不断地生长着。从技术角度讲，有两个关键的因素驱使着各项技术发展，分别是更高的速度和更好的云化。这两个因素有时又是一对矛盾体，更高的性能往往需要更多的硬件亲和，而云化从某种角度又要淡化硬件的特性。

如果硬件规范是统一的，或能抽象统一在一个协定下，则是解决这对矛盾体的一种方式。先不说去除多样性本身是否好，现实中在业界要达成一致的抽象困难重重。

另一种方式，或许就是不同技术向着相同的方向各自演进、互相融合，最终产生新的满足下一代NFV要求的虚拟主机接口。

对于半虚拟化虚拟主机接口，其本身云化能力已经非常完善。所以，其改进方向主要集中在追求更好的网络带宽性能。以Virtio为例，最新的v1.1 Spec引入Packed Ring Layout，主要是以减少内存访问次数为核心的优化方式。由于硬件通过总线访问内存延迟要远高于CPU访问内存，新Layout的引入同样也使得Virtio的Ring格式对于硬件访问更友好。

那最终Virtio是否可以像硬直通一样，由硬件设备直接向虚拟主机提供I/O呢？vDPA（vHost Data Path Acceleration）便是这样的实践。它并不改变Virtio设备模拟的特征，PCIe总线、设备的CSR、BAR 配置寄存器等依旧通过陷入模式的方式委托vHost处理。VDPA加速vHost如下图所示。

![VDPA加速vHost](/misc/img/net/Image00080_net.jpg)

针对vHost的数据平面，利用之前提到的平台I/O虚拟化特性、IOMMU的DMA和中断重映射，使得设备可以直接读写虚拟主机内存和投递中断。这样地址转换、帧buffer的复制及额外的中断中继开销至少可以进一步降低，比纯软件的零拷贝的中继效率更高。倘若硬件本身还能按照 Virtio 规范中定义的方式操作 Ring，则整个数据平面就完全地“硬直通”了。

虚拟主机接口数据平面的软硬之间无缝切换成为可能，硬件提供的是加速能力，而且被特定的硬件规范约束。当然，这里NFV仍旧对Virtio Spec有依赖，但对于NFVi是否使用特定的硬件已经没有了依赖。

那硬直通呢？半虚拟化大多是VMM原生的，硬直通具有跨VMM的一致性。硬直通虽然具有最接近裸机的性能，但也有硬件解耦性不够和云化关键特质热迁移能力的不足的问题。这个课题是否可解？答案也是肯定的，硬直通也在慢慢变得软化。从Linux中VFIO模块支持VFIO-PCI到VFIO-MDEV，一个明显的特征就是VF的PCIe的CSR和配置管理不一定需要和硬件一一对应，控制面可以由陷入模式的方式完成，是不是和半虚拟化很像？再进一步，硬直通的数据平面是否可以软化呢？一旦数据平面可以由软件提供，并接入 NFVi在Host 的数据平面，厂商硬件依赖的硬直通也完成了去设备硬件依赖的属性。

可以看到，虚拟主机接口正在以不同的路径，朝着更好的NFV奔向同一个目标——下一代虚拟主机接口。其特征是NFVi根据VNF的选择提供虚拟主机接口，双方遵照服务质量协议，NFVi运营商可以根据自身需要使用额外无缝硬件加速功能。

### 2. 从虚拟机到容器的网络I/O虚拟化
相比于基于ASIC等专有硬件的网络功能，虚拟化技术将网络功能与底层硬件彻底解耦，不仅为开发人员提供了更加灵活的开发环境，为产品更新提供了更短的迭代周期，更保证了网络功能的高度隔离性、易用性及安全性。凭借这些优势，基于Hypervisor的虚拟化技术，比如KVM、HyperV，已广泛应用于NFV环境中。然而，近两年随着网络速度的不断上升、互联网数据量的不断膨胀，一种更加轻量级的虚拟化技术——容器虚拟化，正逐渐广泛应用于NFV场景中。

基于Hypervisor的虚拟化技术与容器虚拟化技术有一定区别。基于 Hypervisor 的虚拟化技术通过一层中间软件将固定的硬件资源抽象为众多的虚拟化资源（通常称为虚拟机）。每一个虚拟机都具有独立的操作系统，运行于完全独立的上下文中。因此，通常一台主机上运行有多个操作系统。而与完全模拟硬件资源的Hypervisor虚拟化不同，容器虚拟化是一种资源隔离技术。利用操作系统的命名空间和Cgroups资源分配，容器虚拟化将用户程序隔离于不同的资源实体中运行，而每一个资源实体就称为容器。在容器虚拟化中，同一主机上的所有容器共享主机操作系统，不对底层的硬件资源进行模拟。相比于虚拟机，容器是一种更加轻量级的虚拟化，能更高效地利用系统资源，有更快的启动时间，更易于部署和维护。

1. 面向虚拟机的I/O加速方案
在基于 Hypervisor 的虚拟化中，I/O 虚拟化主要包括两种方式，一种是基于SR-IOV的硬件虚拟化方案，另一种是基于Virtio的半虚拟化。针对这两种方式，DPDK提供了相应的两种用户态加速方案，分别为DPDK PF驱动、DPDK VF驱动和基于DPDK的软件交换机.
![面向虚拟机的网络I/O加速方案](/misc/img/net/Image00082_net.jpg)

网卡的SR-IOV技术将一个网卡虚拟化成许多VF，并将VF暴露给虚拟机，使每个虚拟机都可以独享虚拟网卡资源，从而获得能够与本机性能媲美的 I/O 性能。为加速基于 SR-IOV的网络，DPDK 为其提供了相应的用户态的网卡驱动：PF 驱动和VF驱动。通过在虚拟机中使用用户态的VF驱动，虚拟机就可以实现更高效的网络I/O性能。

Virtio是一种半虚拟化技术的通信协议规范，已经广泛应用于虚拟化环境中。在Virtio环境中，前端的Virtio驱动和后端的vHost设备互联，利用主机端的虚拟交换机实现网络通信。为加速基于Virtio的网络，DPDK为前端的Virtio PCI设备提供了用户态驱动（Virtio Polling Mode Driver），并且支持了用户态的 vHost 设备——vHost-user。通过在虚拟机中使用Virtio PMD和软件交换机中使用vHost-user，可以大幅提高虚拟机的网络I/O性能。

2. 面向容器的网络I/O加速方案

Linux为容器提供了十分丰富的网络I/O方案，如主机网络和Docker默认使用的网桥，然而基于内核的网络 I/O 方案在性能上往往无法满足追求高吞吐、低延迟的NFV 业务的需求。并且，因为虚拟机和容器是两种完全不同的虚拟化技术，因此面向虚拟机的网络I/O加速方案无法直接应用于容器网络。此外，现有大部分的基于容器的应用都运行于Kubernetes环境下，这就要求I/O加速方案也能同时支持Kubernetes运行环境。

为了增强容器网络I/O的性能，Intel为Docker容器也提出了与虚拟化I/O加速方案类似的两个I/O加速方案：SR-IOV网络插件和Virtio-user.
![面向容器的网络I/O加速方案](/misc/img/net/Image00083_net.jpg)

SR-IOV网络插件将网卡的SR-IOV技术应用到容器中，通过将网卡VF加入容器的网络命名空间，容器运行时可以直接看到网卡。利用DPDK的用户态VF驱动，容器运行时可以实现高速的网络收发包。Virtio-user是DPDK提出的一种遵从Virtio规范的用户态虚拟设备。与QEMU模拟的Virtio PCI设备相同的是，Virtio-user同样是Virtio的前端设备，可以和任何vHost后端设备，如DPDK vHost-user和Linux Kernel的vHost-net进行通信。然而，与虚拟机使用的Virtio PCI设备不同的是，Virtio-user是为容器量身定制的，并负责网络I/O。在使用Virtio-user的软件交换机方案中，每个容器运行时都有一个 Virtio-user 设备，此设备将 DPDK 使用的大页内存共享给后端的软件交换机；在共享的内存空间中创建Virtio Ring结构，并按照Virtio规范定义的Ring操作方式实现通信。

为支持更加灵活、用户自定义的网络模型，Kubernetes 提供了符合 Container Network Interface（CNI）容器网络规范的网络插件接口。为了将SR-IOV和Virtio-user应用到基于Kubernetes的容器环境中，Intel为两种I/O加速方案分别提供了SR-IOV CNI插件和vHost-user CNI插件。

### 3. NFVi平台设备抽象
NFVi 需要为 VM 提供比较好的性能，所以其本身对性能的要求就更高。基于DPDK优化的NFVi是业内主流的方式。NFVi南向接口需要直接面对主机硬件，利用DPDK的硬件抽象框架，能帮助NFVi软件减少不同硬件带来的差异性。

DPDK提供多种设备抽象并在不断扩展，已被支持的设备包括ETHERDEV、EVENTDEV、CRYPTODEV、RAWDEV、BBDEV、SECURITYDEV等。这些设备大多面向功能进行设备抽象，其支持的硬件从功能固定的网卡、加解密加速卡，到可编程的Smart-NIC、FPGA，甚至面向无线Base Band的设备都提供了支持。而很多抽象设备都有基于 CPU ISA 的实现，比如使用 AES 指令实现的CRYPTODEV实例和OPDL库实现的EVENTDEV。

1. CRYPOTDEV

CRYPTODEV 是对加解密设备的抽象，提供一组 API，加解密请求被封装在一组Context中，由DPDK应用实例向加解密设备发起加解密的请求。CPU在对称加解密处理上拥有不错的性能，所以 CRYPTODEV 除支持多厂商的 PMD 外，还有多个软件实现的加解密设备。

另外，DPDK还为CRYPTODEV提供了一种SCHDULER PMD，它可以根据不同的策略将加解密请求调度到多个加解密设备上。该机制为多设备聚合提供了可能，也会异构加解密（CPU和加解密设备）加速提供了软件框架。

CRYPTODEV在NFV的一种应用场景是IPSec,DPDK提供了IPSec GW的示例，也在FD.IO、VPP中提供了基于CRYPTODEV的IPSec实现。

1. EVENTDEV

EVENTDEV是一种抽象事件设备，其主要作用是为DPDK应用提供事件调度设备接口，便于应用采用事件驱动模型编程，而不需要理解调度设备的具体实现细节。

比如一个EVENTDEV设备提供了3个队列和6个端口，将6个CPU以Pipeline的方式串联起来。其中，Q1和Q2按照Atomic的机制分发调度事件，Q3出口对应单端口，所以不构成调度。通过配置 API 可以将队列和端口关联起来。在运行时，对于每一个CPU，可以通过端口ID进行事件的入队列和出队列操作。

EVENTDEV支持3种队列调度类型，分别是Atomic、Ordered及Parallel。Atomic调度类型保证数据流的原子性，同一时间不会将同一条流调度到多个 CPU。Ordered调度类型允许将数据流调度到多个CPU，但保证多个CPU下游出队列时，必须按照流分发的次序出队列。Parallel调度类型则允许将数据流调度到多个CPU，但不提供数据流的次序完整性。

可以看到，采用 EVENTDEV 方式编程，可以非常方便地设计需要水平扩展的NFVi数据面。

3. RAWDEV

RAWDEV为在DPDK中还未有该设备抽象的设备提供了一个选项。以Intel支持 PR（Partial Re-Configuration）的 FPGA 设备为例，该设备呈现为一个普通 PCIe设备，设备驱动可以对FPGA部分运算资源（AFU）进行运行中的功能加载。

在DPDK中，在RAWDEV下实现了一个该设备的驱动，为应用提供加载FPGA逻辑的API。该设备驱动将可用AFU设备挂到一个IFPGA的总线上，注册在该总线上的FPGA逻辑驱动将会对总线上的设备进行驱动绑定，并最终向DPDK框架呈现相对应的抽象设备接口（例如ETHERDEV、CRYPTODEV等）。

可以看到 RAWDEV 为 DPDK 设备框架引入了灵活性，当某设备功能还不具有普适性时，仍可以通过该方式得到开源社区的支持，在更多厂商有类似诉求并达成共识后，可以单独抽象出来成为新的设备抽象类型。

4. Representor端口

软件方案实现的Virtual Switch如OVS已经被很多云服务提供商接受，随着网络带宽要求的进一步升级，很多厂商也在尝试使用硬件加速Virtual Switch来提升虚拟机网络性能。随着NIC内嵌的交换功能变得越来越强大，越来越多的功能（比如ACL、tunnel）被引入 NIC，传统的 ethdev 设备模型不能完全涵盖这些硬件的特性，所以Kernel和DPDK都提出了switchdev的概念。

内嵌 Switch 功能的网卡通过 SR-IOV 方式暴露出多个端口，用户把 Virtual Function 直接分配（pass-through）给虚拟机，设备交由VM直接访问。在host上，可以通过PF 内核驱动对VF做管理配置

对于内嵌Switch offload功能的NIC,ip等命令提供的配置能力是有限的，我们希望依然用丰富的Switch配置接口对VF端口、流表进行配置。因此，在硬件加速Virtual Switch的场景下，我们需要为VF提供端口，既为了通过端口下发配置，也为了从这些端口中接受Switch 快速路径处理不了的报文。

在DPDK中，为了通过PF对VF进行配置，需要给应用呈现一个VF的端口，通过Ethdev API对VF端口做管理控制，比如设置MAC/VLAN，通过rte_flow lib配置转发流表等。如图3-31所示，在DPDK中，Representor端品是PF驱动初始化时创建出来的以太网端口，它给应用程序提供了一个标准的端口视角。每一个 ethdev都用一个 Switch info 属性，用来标志此端口属于哪一个Switch dev。rte_eth_dev_info_get API可以获取device的flag，通过检查RTE_ETH_DEV_REPRESENTOR bit判断这个端口是否为Representor端口。

对于希望创建 Representor 端口的 PF 设备，用户通过参数列表指定需要管理的VF.

PF driver根据用户的需求决定为哪些VF创建Representor端口，它们基于PF的端口而存在，也就是说当PF的端口被销毁时，其所有相关的Representor端口也将随之被销毁。这里以i40e为例，讨论Representor端口在PF driver中的实现。

在PF（Driver Probe）驱动探测的时候，会用rte_eth_devargs_parse API解析设备参数，如果用户设置了 Representor 端口参数，PF 驱动不仅创建自己的 ethdev，还会为用户指定的VF分别创建ethdev，并将这些ethdev归入同一个Switch Domain，如图3-32所示。PF 驱动会在VF的ethdev中注册一组eth_dev_ops,App就可以通过标准的ethdev API配置VF,i40e实现了很丰富的配置接口，例如promiscuous mode设置、mac addr配置、vlan filter等。

Representor 端口和一般的以太网端口不同的是，由于它主要用来做管理配置，因此可以不实现收发包函数。如果需要处理Exception Path的流量，则需要实现相应的收发包函数接收来自对应VF的异常报文。

通过VF Representor 端口调用的Ethernet API最终都是通过PF driver实现的，Representor 端口的作用就是封装了PF对VF的配置函数，使得用户可以通过标准的ethdev API对VF进行管理。

## OVS-DPDK
Open vSwitch（OVS）是一个产品级质量的多层虚拟交换机，基于Apache 2.0许可。OVS 的设计初衷是支持可编程自动化网络大规模部署及拓展，能够支持标准网络管理接口和协议，如NetFlow、sFlow、IPFIX、RSPAN、CLI、LACP、802.1ag等。同时，它还需要支持与其他现有虚拟交换方案的混合部署，例如 VMware 公司的vNetwork、思科公司的Nexus 1000V等。

由于丰富的功能和优秀的稳定性，OVS 在网络部署中得到了广泛的应用。源于近些年云计算的快速发展，很多云服务提供商将OVS用做大量虚拟机对外的快速数据通道，基于OVS发行版进行了功能添加和优化，一些基于硬件加速的OVS方案在近些年也得到了广泛的关注。

OVS 基本功能包括如下4大方面。
- 自动化控制：OVS 支持OpenFlow，用户可以通过ovs-ofctl使用OpenFlow协议连接交换机实现查询和控制
- QoS：支持拥塞管理和流量整形
- 安全：支持VLAN 隔离、流量过滤等功能，保证了虚拟网络的安全性
- 监控：支持Netflow、SFlow、SPAN、RSPAN等网络监控技术

### 1. OVS-DPDK 概述
1. OVS-DPDK 基本原理

最初的OVS版本通过Linux内核数据通道进行数据分发。然而，用户能够得到的最终吞吐量受限于Linux网络协议栈的性能。DPDK作为用户态的高性能网络数据处理库，通过提供一系列的Poll Mode 驱动，能够绕过Kernel的性能瓶颈，在物理网卡和用户态之间提供高速数据传输服务。通过将DPDK集成到OVS中，交换机的快速数据路径被切换到用户态，OVS-DPDK原理如下图所示。

![OVS-DPDK原理](/misc/img/net/Image00090_net.jpg)

如下图所示为OVS-DPDK组件示意图。OVS的交换端口通过netdev表示，在OVS-DPDK中，netdev-dpdk是DPDK加速过的I/O端口，包括物理网卡librte_eth（DPDK支持市面上各种主流网卡，由对应的PMD进行驱动）、虚拟接口librte_vhost（DPDK版的用户态vHost实现，可工作在服务器端和客户端为虚拟机和宿主机之间提供高速数据传输通道）、dpif-netdev（提供了用户态的转发功能）、ofproto（OpenFlow交换机的具体实现部分）、ovsdb-server（用来维护OVS的交换流表配置并负责与上层的SDN控制器的通信）。

![OVS-DPDK组件示意图](/misc/img/net/Image00091_net.jpg)

2. OVS-DPDK 版本及功能

因为OVS-DPDK是基于DPDK提供的库，当DPDK的API有所变化时，OVS也需要做出相应的修改才能适配，所以对OVS和DPDK的版本有一定要求来保证兼容性。OVS-DPDK的很多新功能也依赖于DPDK新功能的添加。因此，建议使用OVS官网推荐的DPDK版本进行编译。

### 2. OVS-DPDK性能优化
#### 1. 多虚拟机环境下OVS-DPDK的性能影响因素
OVS-DPDK的主要设计目标是获得高吞吐量的交换能力。这里讨论影响OVS-DPDK的主要因素及高阶配置用法。影响OVS-DPDK吞吐量的关键因素论述如下。

1. vHost enqueue burst size

收发函数是数据通道中被调用最多的部分，如果在数据收发中，每次调用收发函数只发送1个包，对CPU运算资源会产生很大的浪费。每次收发函数能够批量处理一批数据包，能大大提升CPU的使用效率。

在DPDK中，默认的TX/RX burst size最大值为32,OVS基于DPDK，也采用这个值。当OVS-DPDK需要支持的端口数较少时，各个端口依然能够批量收发数据包。但是，当OVS-DPDK 部署在类似云服务的环境中时，可能会有几十个甚至上百个虚拟机。相应地，OVS 中会有相应数量的 vHost-user 端口提供后端服务。这样就会存在vHost端口burst size过小的问题。在物理网卡每次RX收到的一批数据包中，如果数据包去往不同的VM，则需要调用多个vHost-user端口的TX函数，每个TX 函数调用发送的包数量也会很小，这是严重影响吞吐量的因素。

1. 流匹配开销

在 OVS 中，每个数据包需要根据流匹配结果决定下一步的数据流向和动作。OVS-DPDK 提供三层流匹配方案（EMC、dpcls和ofproto），如下图所示.
![三层流匹配方案](/misc/img/net/Image00104_net.jpg)

在OVS-DPDK中，每个DPDK PMD线程都会包含一个EMC表，EMC匹配为精确匹配，EMC表会缓存一定数量的流信息。当数据包的五元组（源IP、源端口、目的IP、目的端口和协议号）完全匹配EMC表中的表项时，能够立刻获得相对应的下一步动作信息。

但因空间限制和效率的问题，在OVS-DPDK中EMC表项大小默认为8192。一旦发生EMC失配，将会通过dpcls进行流匹配。dpcls表中包含若干个子表，采用通配符的方式进行模糊匹配，该匹配可以只匹配部分字段，例如只匹配源MAC地址。

dpcls匹配流成功后，会将当前流添加到EMC表中，这样的话，该流的后续数据包可以进行快速处理。当dpcls也失配时，OVS只能将数据包通过ofproto传给OpenFlow控制器进行处理，这个查询是最消耗时间的，将会是EMC查询时间的数十倍。

为了解决以上影响吞吐量的关键问题，用户可以合理利用OVS-DPDK提供的以下功能进行性能优化。
- 批处理发包

    为了解决之前提到的影响 OVS 吞吐量的 vHost TX size（发送大小）问题，OVS-DPDK提出了批处理发包的概念。在此之前，当OVS收到一批数据时，如果匹配了不同的流，但是发送端口如是同一个，OVS 则会进行集体发送来提高效率。但是，对于比较慢的前端，比如虚拟机中Virtio-net驱动，vHost端口的TX size依然很小，于是批处理发包应运而生。

    简单说来，当每个端口得知自己有数据包要发送的时候，如果TX的数据包数量很小，并不是立刻发送，而是等待一些时间，直到超时，或者达到了最大TX size的时候才会发送，示例命令如下。

    $ ovs-vsctl set Open_vSwitch.other_config:tx-flush-interval=50

    需要注意的是，这个参数会同时影响吞吐量和延时。数值设置得越大，吞吐量提高得越大，而且延时也会相应提高。50 ms这个参考值，是基于x86平台在PVP测试场景中经过测试达到的一个推荐值，并不适用所有场景。在自己网络部署中，需要针对不同的应用场景自行优化这个值的设置。

    可以通过如下命令查看当前的每个端口批量发送数据包的平均值。

    $ ovs-appctl dpif-netdev/pmd-stats-show

    在云服务提供商的宿主机上，一般会运行较多的虚拟机，而且虚拟机中运行的多数是慢速的内核驱动设备，数据离散度较高，该功能的开启能够显著提高多虚拟机环境下的网络吞吐量，对于云服务提供商会有较大的帮助。

- 使用多队列提高EMC命中率

    提高OVS-DPDK整体吞吐量是很多网络开发人员的主要任务。配置网卡和vHost端口的多队列是一个能够快速提高吞吐量的方法。首先，利用网卡多队列功能，可以利用更多的CPU 核参与到数据转发中来，提供更高的运算能力。其次，根据前面描述的EMC特性，EMC表是基于每个EMC线程的，提供多队列，可以提高总的EMC表项数，配合网卡自带的RSS等分流功能，可以提高EMC命中率，大大减少数据包匹配查找的时间。在OVS中，配置多队列的命令如下。

    $ ovs-vsctl set Interface <DPDK interface> options:n_rxq=<integer>

- PMD线程和CPU 核的手动绑定

    OVS-DPDK会对所有的PMD 线程和可用于DPDK PMD的CPU核进行默认绑定。但在有些场景下，需要根据每个 PMD 线程的工作负载程度重新配置 CPU 的绑定关系，以获得更好的性能和能耗表现。在OVS-DPDK中，可以利用以下命令进行配置。

    多队列端口的CPU核绑定示例: ovs-vsctl set interface dpdk-p0 options:n_rxq=4 other_config: pmd-rxq-affinify="0:3,1:7,3:8" 

    该命令将dpdk-p0端口进行如下绑定：队列0,CPU Core 3；队列1,CPU Core 7；队列2，不绑定；队列3,CPU Core 8。

#### 2. GSO
对于通常情况，网卡的MTU一般是1500，而在Linux应用层，能够发送的数据包远远大于这个长度，这个时候就需要对数据包进行分片才能通过网卡发送。对于TCP/IP 数据包来说，每个分片过的TCP包都需要通过内核协议栈进行处理；而内核协议栈进行数据包分片的计算开销是很大的，是很多场景下的性能瓶颈。针对这一问题，用户希望在传输层和网络层避免这些切片操作，将这些操作尽可能地延后。现在主流的方法有两种：
- 硬件网卡进行切片，将之前所述的计算任务卸载到网卡。
- 依然由处理器进行切片，但是将切片动作延迟至发送网卡驱动之前。

现在市面上的主流网卡基本都能支持TSO（TCP Segmentation Offload），如果网卡支持 TSO,TCP 包交由网卡进行切片是最高效的。但是网卡切片的灵活性并不够，对于非TCP包的切片，例如VXLAN、GRE包等，很多网卡并不支持切片卸载。因此，基于软件的 GSO 被提了出来，作为网卡切片卸载功能缺失下的有效补充。采用基于软件的GSO（Generic Segmentation Offload）有以下好处：不依赖于硬件网卡，对虚拟机中部署的虚拟网络设备也适用，例如Virtio设备；易于拓展，当有新的协议需要支持分片时，基于软件的 GSO 更容易快速实现，相比硬件解决方案更具有灵活性。

现在的应用程序编写逻辑一般是在数据包发送给网卡前，先判断网卡是否支持当前包类型的硬件切片卸载，如果网卡支持，则由网卡切片并发送；如果网卡不支持，则调用GSO进行切片，然后发送给网卡驱动。下图所示为GSO原理示意图。

![GSO原理示意图](/misc/img/net/Image00106_net.jpg)

GSO在DPDK 17.11中被开发集成，作为库提供给用户使用。现在支持三种类型的数据包：TCP、VXLAN和 GRE。在DPDK 18.08中，UDP包的切片也被集成到GSO的库中。DPDK GSO集成到OVS的工作正在进行中，初步计划会在OVS 2.11版本中发布。

在OVS-DPDK中，GSO的最常见的应用场景是跨主机的虚拟机之间的通信。当Qemu启动时，Qemu配置参数host_tso4会通知Virtio,vHost后端有TSO能力，请Virtio 设备发送大包时无须分片。需要注意的是，必须同时打开 csum，否则 VM 中的Virtio设备仍会进行分片动作，Qemu参考命令如下:

```
-netdev type=vhost-user,id=mynet1,chardev=char0,vhostforce \
-device virtio-net-pci,mac=00:00:00:00:00:01,netdev-mynet1,mrg_rxbuf=on,csum=on,host_tso4=on
```

vHost收到大包后，转送给DPDK物理端口进行发送，在物理端口驱动进行数据包发送前，OVS-DPDK调用GSO lib 进行TCP包的分片，以此来卸载虚拟机中CPU对数据包分片的工作。

## FD.IO：用于报文处理的用户面网络协议栈
FD.IO是许多项目和库的一个集合，基于DPDK并逐渐演化，支持在通用硬件平台上部署灵活可变的业务。FD.IO为软件定义基础设施的开发者提供了一个平台，可以创建多个项目，开发基于软件的报文处理创新方案，便于设计高吞吐量、低延时和有效利用资源的应用程序，并能够应用在多个平台上（x86、ARM和PowerPC）和部署在不同的环境中（裸机、虚拟机和容器）。

FD.IO的一个关键项目是VPP（Vector Packet Processing，矢量报文处理）。VPP是高度模块化的项目，新开发的功能模块很容易被集成进VPP，而不影响VPP底层的代码框架。这就给了开发者很大的灵活性，可以创新不计其数的报文处理解决方案。

除了 VPP,FD.IO 充分利用 DPDK 特性以支持额外的项目，包括 NSH_SFC、Honeycomb 和ONE来加速网络功能虚拟化的数据面。此外，FD.IO还与其他关键的开源项目进行集成，以支持网络功能虚拟化和软件定义网络。目前已经集成的开源项目包括OPNFV、OpenStack和OpenDaylight。

VPP 是一个模块化和可扩展的软件框架，用于创建网络数据平面应用程序。更重要的是，VPP 代码为现代通用处理器平台而生，并把重点放在优化软件和硬件接口上，以便用于实时的网络输入输出操作和报文处理。

VPP充分利用通用处理器优化技术，包括矢量指令（例如Intel SSE和AVX）及I/O和CPU缓存间的直接交互（例如 Intel DDIO），以达到最好的报文处理性能。利用这些优化技术的好处是：使用最少的CPU核心指令和时钟周期处理每个报文。在最新的Intel Xeon-SP处理器上，可以达到Tbps的处理性能。

VPP是一个有效且灵活的数据平面，它包括一系列按有向图组织的转发图形节点和一个软件框架。该软件框架包含基本的数据结构、定时器、驱动程序、在图形节点间分配CPU时间片的调度器、性能调优工具，比如计数器和内建的报文跟踪功能。

VPP采用插件架构，插件与直接内嵌于VPP框架中的模块一样被平等对待。原则上，插件是实现某一特定功能的转发图形节点，但也可以是一个驱动程序，或另外的CLI，或API绑定。插件能被插入VPP有向图的任意位置，从而有利于快速灵活地开发新功能。因此，插件架构使开发者能够充分利用现有模块快速开发出新功能。

![VPP架构的报文处理有向图](/misc/img/net/Image00109_net.jpg)

输入节点轮询（或中断驱动）接口的接收队列，得到批量报文。接着把这些报文按照下个节点功能组成一个矢量（vector）或一帧（frame）。比如，输入节点收集所有IPv4的报文并把它们传递给ip4-input节点；输入节点收集所有IPv6的报文并把它们传递给ip6-input节点。当ip6-input节点被调度时，它取出这一帧报文，利用双次循环（dual-loop）、四次循环（quad-loop）及预取报文到 CPU 缓存技术处理报文，以达到最优性能。这能够通过减少缓存未命中数来有效利用CPU缓存。当ip6-input节点处理完当前帧的所有报文后，会把报文传递到后续不同的节点。例如，如果某报文校验失败，就被传送到error-drop节点，正常报文被传送到ip6-lookup节点。一帧报文依次通过不同的图形节点，直到它们被interface-output节点发送出去。

按照网络功能一次处理一帧报文，有几个好处：
- 从软件工程的角度看，每一个图形节点是独立和自治的。VPP 图形节点的处理逻辑如图3-40所示。
- 从性能的角度看，首要好处是可以优化CPU指令缓存（i-cache）的使用。当前帧的第一个报文加载当前节点的指令到指令缓存，当前帧的后续报文就可以“免费”使用指令缓存。VPP充分利用了CPU的超标量结构，使报文内存加载和报文处理交织进行，更有效地利用CPU处理流水线。
- VPP也充分利用了CPU的预测执行功能来达到更好的性能。从预测重用报文间的转发对象（比如邻接表和路由查找表），以及预先将报文内容加载到CPU的本地数据缓存（d-cache）供下一次循环使用，这些有效使用计算硬件的技术，使得VPP可以利用更细粒度的并行性。

依靠有向图处理的特性，使 VPP 成为一个松耦合、高度一致的软件架构。每一个图形节点利用一帧报文作为输入和输出的最小处理单位，提供了松耦合的特性。通用功能被组合到每个图形节点中，提供了一致的架构。

在有向图中的节点是可替代的。当这个特性与 VPP 支持动态加载插件节点相结合时，有趣的新功能可以被快速开发，而不需要新建和编译一个定制的代码版本。

###  vBRAS
OpenBRAS项目是由中国电信、英特尔、浪潮和其他一些公司共同发起的开源项目，它基于转控分离架构.