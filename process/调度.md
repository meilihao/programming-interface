# 调度
参考:
- [如何更改 Linux I/O 调度器来调整性能](https://linux.cn/article-8179-1.html)
- [Linux进程和线程的调度与优先级](https://vaqeteart.github.io/categories/study/os/linux_schedule_priority.html)
- [深入 Linux 的进程优先级](https://linux.cn/article-7325-1.html)
- [linux进程调度](https://peterpan980927.cn/2017/12/18/Linux%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/)
- [Linux 内核源码分析之进程概要及调度时机-](https://www.tuicool.com/articles/qUNrYbi)

常用的调度指标包括:
1. 与性能相关: 吞吐量、周转时间(从被发起直至执行结束所需的时间)、晌应时间
1. 非性能指标: 公平性、资源利用率;
1. 某些任务、场景特有的需求: 实时任务的实时性、终端设备的能耗

实时操作系统的特点是确定性,即实时任务的完成时间应该是有明确上界的.

以Linux为代表的一系列os会为每个CPU核心分配一个本地运行队列, 即可以理解为每个CPU核心有一个本地调度器. linux中目前使用的调度实体(单个任务)粒度负载追踪（Per-Entity Load Tracking, PELT）机制, PELT通过记录每个任务的历史执行状况来表示任务的当前负载.

> 在Linux 3.8以前, 内核以每个CPU核心的运行队列为粒度来计算负载, 认为运行队列长的负载就高, 导致负载追踪不够精确.

linux的负载均衡策略是层级化的, 调度域（scbedulingdomain）是拥有相同特性CPU核心的集合, 这些核心间可以进行负载均衡. 一个调度域保存一个或多个调度组（schedul1nggroup）. 调度组是一个调度域内进行负载均衡的整体单位.

Linux的能耗感知调度（Energy Aware Scheduling, EAS）可以通过将任务调度到不同的CPU核心上以达到节省能耗的目标. EAS通过能耗模型了解每个CPU核心的容量（处理能力）和功率. 通过CPU容量和PELT的负载信息, EAS可以最大限度地在不影响性能的情况下进行降低能耗的调度.

EAS适用于中/低负载场景, 处于高负载或满负载的状态, EAS很难通过任务迁移来降低能耗. 当任一CPU核心的当前负载超过80％的最高容量时, Linux会开启负载均衡并关闭EAS;而当没有任何CPU核心的负载超过80％
时, Linux会开启EAS并关闭负载均衡.

调度策略是在两大矛盾中寻找平衡: 进程响应速度(低延迟)和最大系统利用率(高吞吐量).

调度算法:
- 优先级(最基本)

## 进程
进程可以分为CPU密集型和I/O密集型两种. I/O密集型的进程大部分时间都在用来提交I/O请求和等待I/O请求; CPU密集型进程大部分时间都花在执行代码上.

调度策略主要在两个矛盾之间寻找平衡：进程响应时间短和最大的系统利用率.

> 为了保证交互式应用和桌面系统的性能，一般Linux更倾向于优先调度I/O消耗型进程

### 进程优先级
在 Linux 里面，进程大概可以分成两种
1. 实时进程 : 静态(实时)优先级, 支持 posix.1b标准. 快速记忆: 权力大而快.

	就是需要尽快执行返回结果的进程.

	优先级变化范围：0~MAX_RT_PRIO-1间, 默认MAX_RT_PRIO配置为100. **值越大，优先级越高**. Linux下查看实时优先级命令： `ps -eo state,uid,pid,rtprio`的`RTPRIO`列, `-`表示非实时进程.

1. 普通进程 : 动态(非实时,nice值). 快速记忆: 负重运行慢.

	大部分的进程其实都是这种

	nice值得优先级范围是从-20到+19，默认是0,**nice值越大，优先级越小**. Linux系统的nice值代表的是时间片的比例，因此越小的nice值，占有CPU的时间也就会越长. `ps -el`的`NI`列.

因为**任何实时进程(需要尽快返回结果)的优先级都高于普通进程**. 因此实时优先级和nice优先级处于两个不相交的范畴, 具体映射关系如下:

![linux进程优先级映射关系](/misc/img/process/linux_process_priority.jpg)

> 默认情况下，进程的nice是从父进程继承来的.
> `ps -el`的`NI`列包含 `-` 这个符号，该符号的含义是指：该进程是实时进程.Linux中可以通过renice命令调整进程的优先级
> `ps -eo state,uid,pid,rtprio`的`RTPRIO`列包含`-` 这个符号，该符号的含义是指：该进程不是实时进程

### 调度类型
> 可通过`chrt`命令查看及调整.

> 时间片是分配给每个可运行进程的处理器可用时间段. linux的完全公平调度(cfs)没有使用时间片.

> 非抢占模式下, 除非进程自己主动停止, 否则会一直执行.

每个Linux进程都按照以下调度类型被调度:
- 实时调度策略 by Real-Times cheduler, RT

	- SCHED_FIFO
	  
	  先进先出的**实时**进程, 不使用时间片. 高优先级的进程可以抢占低优先级的进程,而相同优先级的进程遵循先来先得,此时除非主动让出否则想运行多久便运行多久
	
	- SCHED_RR
	  
	  时间片轮转的**实时**进程, 使用时间片. 保证对所有相同优先级的实时进程公平地分配CPU时间即只有当它的时间片用完，内核会把它放到进程队列的末尾; 而高优先级的任务也是可以抢占低优先级的任务

- 实时调度策略 by DeadLine scheduler, DL
	- SCHED_DEADLINE
	  
	  **实时**进程, 按照任务的deadline进行调度的. 当产生一个调度点的时候,DL调度器总是选择其deadline距离当前时间点最近的那个任务并调度它执行

	  其参数sched_runtime, sched_deadline和sched_period分别对应于任务的预期运行时间、截止时长和周期, 单位是纳秒

- 普通调度策略 by Completely Fair Scheduler, CFS

	- SCHED_NORMAL : 可用nice或renice调整, 范围是`19(最低) ~ -20(最高)`, 默认为0.
	  
	  普通的分时进程, 比如与用户交互的任务

	  > SCHED_NORMAL即以前的SCHED_OTHER
	- SCHED_BATCH

	  后台进程,几乎不需要和前端进行交互, 不影响交互时可以降低它的优先级
	- SCHED_IDLE

	  特别空闲的时候才跑的进程, 同样意味着它的优先级最低

	  > SCNED_IDLE与Linux的idle调度类没有直接关系. idle调度类主要是在当前cpu核心的运行队列上没有任务需要运行时, 让CPU执行idle任务, 从而节约系统能耗.

Linux的RT调度器使用多级优先级队列的方式调度任务.

Linux中, 每个任务会有实时优先级（也称为静态优先级）, 对应于sched_priority. 实时任务的实时优先级可设置的范围一般为［0,100）, 数值越大则优先级越高. 非实时任务的实时优先级必须被设置为0;  SCHED_FIFO和SCHED_RR的实时任务优先级范围为［1, l00）, 因此RT调度器管理的任务永远先于CFS调度器管理的任务.

Linux提供了不同类型的调度器, 不同类型的任务在系统中被统一调度的设计类似于多级队列: Linux会优先调度DL调度器管理的任务, 其次是RT调度器管理的任务, 只有当前两种调度器的任务被执行完后, 才会调度CFS调度器管理的任务

> chrm命令可修改进程的调度策略和优先级

SCHED_NORMAL是普通进程调度策略,其他两者都是实时进程调度策略. 它们的主要区别就是通过优先级来区分的: 所有优先级值在0-99范围内的，都是实时进程，所以这个优先级范围也可以叫做实时进程优先级，而100-139范围内的是非实时进程.

系统的整体优先级策略是：如果系统中存在需要执行的实时进程，则优先执行实时进程. 直到实时进程退出或者主动让出CPU时，才会调度执行非实时进程. 即 **任何时候，实时进程的优先级都高于普通进程，实时进程只会被更高级的实时进程抢占，同级实时进程之间是按照FIFO（一次机会做完）或者RR（多次轮转）规则调度的**.

SCHED_FIFO 与 SCHED_RR 的区别是:当进程的调度策略为前者时,当前实时进程将一直占用 CPU 直至自动退出, **除非有更紧迫的、优先级更高的实时进程**需要运行时,它才会被抢占 CPU;当进程的调度策略为后者时,它与其它实时进程以实时轮流算法去共同使用 CPU，用完时间片放到运行队列尾部.

![Linux 进程优先级与 nice 值及实时进程优先级的关系](/misc/img/process/v2-488659493625064c6227293720c117c9_hd.jpg)

> priority就是ps命令中看到的PRI值或者top命令中看到的PR值, 越小说明优先级越高.
> nice值虽然不是priority，但是它确实可以影响进程的优先级
> 实时进程, 只有静态优先级;普通进程根据动态优先级进行调度
> 动态优先级(`dynamic_prio=max(100, min(static_prio-bonus+5, 139))`)是由静态优先级（`static_prio=MAX_RT_PRIO +nice+ 20`, 静态优先级范围在100~139之间）调整而来.
> 在系统中可以使用chrt命令来查看、设置一个进程的实时优先级状态
> 进程的平均睡眠时间也即bonus

### macOS/iOS调度器
macOS和iOS等系统上使用的面向用户体验的调度器-GCD（Grand Central Dispatch）. GCD使用了基于应用场景的实时优先级调度策略以满足程序对时延的需求.

### linux task_struct 调度
参考:
- [深入解读Linux进程调度系列（总览）](https://blog.csdn.net/Vince_/article/details/89054330)
- [Linux进程调度-CFS调度器](https://www.cnblogs.com/LoyenWang/p/12495319.html)

linux调度器是以模块的形式提供的, 该模块结构被称为调度器类(scheduler classes), 允许多种不同的可动态添加的调度算法并存, 调度属于自己范畴的进程, 每个调度器都有自己的优先级. 基础的调度器代码在`kernel/sched/core.c`, 它会按照优先级顺序遍历调度类, 拥有一个可执行进程的最高优先级的调度类胜出, 再去选择要执行哪个程序.

在task_struct中,有一个成员变量`unsigned int policy`保存了调度策略, 它有以下定义:
```c
// https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/sched.h#L114
/*
 * Scheduling policies
 */
#define SCHED_NORMAL		0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE		6
```

配合调度策略的还有优先级 ,也在task_struct中
```c
// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L678
	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;
```

优先级其实就是一个数值,对于实时进程,优先级的范围是0~99;对于普通进程,优先级的范围是100~139. 数值越小,优先级越高. 从这里可以看出,所有的实时进程都比普通进程优先级要高.

#### 调度类
一个进程从被创建开始,到被调度执行,再到被抢占或者主动让出CPU,整个过程调度器需要完成将进程纳入管理、调度进程执行和记录进程占用CPU的时间等工作。考虑多个进程的情况,还需要完成选择下一个进程、进程从一个CPU切换到另一个CPU等任务.

内核定义了sched_class结构体(调度类、调度器类)表示这些任务,每一个任务对应一个字段(回调函数)

> 从Linux 2.6.23开始，Linux引入scheduling class的概念，目的是将调度器模块化, 这提高了可扩展性，添加一个新的调度器也变得简单起来.

进程执行在CPU上,所以CPU需要记录正在和将要运行在它上的进程的情况,内核以rq(runqueue)结构体描述它.

每个CPU都有一个rq对象,内核定义了rq类型的每CPU变量runqueues与它们对应。clock_task字段表示进程累计运行的时间,有可能不包括中断处理等时间(与系统的配置有关),clock可能比它大.

rq、CPU和task_struct关系表:
- task_cpu(task_struct* p) : 进程所属的cpu
- cpu_rq(cpu): cpu对应的rq
- task_rq(p): cpu_rq(task_cpu(p)), 进程所属的rq
- cpu_curr(cpu): (cpu_rq(cpu)->curr), cpu当前执行的进程

rq结构体并没有直接与task_struct关联的字段,所以进程也并不由它直接管理,实际的管理者是cfs、rt和dl字段.

cfs_rq维护了一个由sched_entity对象组成的红黑树(下文称之为进程时间轴红黑树),task_struct的se字段就是sched_entity类型,正是它关联了cfs_rq和task_struct.

rt_prio_array里的MAX_RT_PRIO等于100,实时进程的优先级是0~99,queue字段定义了100个链表,实时进程根据优先级链接到链表上,优先级与
queue数组的下标相等。queue数组的链表链接的是sched_rt_entity结构体,也就是task_struct的rt字段.

dl_rq维护了一个由sched_dl_entity对象(task_struct的dl字段)组成的红黑树。

rq的cfs管理完全公平调度的进程,rt管理实时进程,dl管理最后期限调度的进程.

进程调度还有另外一种以组来管理进程的方式,也就是占用CPU时间不以单个进程计算,以一组进程来衡量,以上三个结构体都有额外的字段来实现该功能, 这里忽略该情况.

sched_class结构体是调度器的行为指南, 不同的调度器就需要不同的调度类.

内核定义的stop_sched_class、dl_sched_class、rt_sched_class、fair_sched_class和idle_sched_class分别对应stop调度、最后期限调度(或者称为最早截止时间优先调度)、实时调度、完全公平调度和idle调度.

```c
// https://elixir.bootlin.com/linux/v6.6.29/source/kernel/sched/sched.h#L548
/* CFS-related fields in a runqueue */
struct cfs_rq {
	struct load_weight	load;
	unsigned int		nr_running; // TASK_RUNNING的进程数
	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
	unsigned int		idle_nr_running;   /* SCHED_IDLE */
	unsigned int		idle_h_nr_running; /* SCHED_IDLE */

	s64			avg_vruntime;
	u64			avg_load;

	u64			exec_clock;
	u64			min_vruntime;
#ifdef CONFIG_SCHED_CORE
	unsigned int		forceidle_seq;
	u64			min_vruntime_fi;
#endif

#ifndef CONFIG_64BIT
	u64			min_vruntime_copy;
#endif

	struct rb_root_cached	tasks_timeline; // tasks_timeline.rb_root: sched_entity对象组成的红黑树的root; tasks_timeline.rb_leftmost: 红黑树最左边的叶子

	/*
	 * 'curr' points to currently running entity on this cfs_rq.
	 * It is set to NULL otherwise (i.e when none are currently running).
	 */
	struct sched_entity	*curr; // 当前sched_entity
	struct sched_entity	*next; // 下一个sched_entity

#ifdef	CONFIG_SCHED_DEBUG
	unsigned int		nr_spread_over;
#endif

#ifdef CONFIG_SMP
	/*
	 * CFS load tracking
	 */
	struct sched_avg	avg;
#ifndef CONFIG_64BIT
	u64			last_update_time_copy;
#endif
	struct {
		raw_spinlock_t	lock ____cacheline_aligned;
		int		nr;
		unsigned long	load_avg;
		unsigned long	util_avg;
		unsigned long	runnable_avg;
	} removed;

#ifdef CONFIG_FAIR_GROUP_SCHED
	unsigned long		tg_load_avg_contrib;
	long			propagate;
	long			prop_runnable_sum;

	/*
	 *   h_load = weight * f(tg)
	 *
	 * Where f(tg) is the recursive weight fraction assigned to
	 * this group.
	 */
	unsigned long		h_load;
	u64			last_h_load_update;
	struct sched_entity	*h_load_next;
#endif /* CONFIG_FAIR_GROUP_SCHED */
#endif /* CONFIG_SMP */

#ifdef CONFIG_FAIR_GROUP_SCHED
	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */

	/*
	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
	 * (like users, containers etc.)
	 *
	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
	 * This list is used during load balance.
	 */
	int			on_list;
	struct list_head	leaf_cfs_rq_list;
	struct task_group	*tg;	/* group that "owns" this runqueue */

	/* Locally cached copy of our task_group's idle value */
	int			idle;

#ifdef CONFIG_CFS_BANDWIDTH
	int			runtime_enabled;
	s64			runtime_remaining;

	u64			throttled_pelt_idle;
#ifndef CONFIG_64BIT
	u64                     throttled_pelt_idle_copy;
#endif
	u64			throttled_clock;
	u64			throttled_clock_pelt;
	u64			throttled_clock_pelt_time;
	u64			throttled_clock_self;
	u64			throttled_clock_self_time;
	int			throttled;
	int			throttle_count;
	struct list_head	throttled_list;
#ifdef CONFIG_SMP
	struct list_head	throttled_csd_list;
#endif
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
};

// https://elixir.bootlin.com/linux/v6.6.29/source/kernel/sched/sched.h#L667
/* Real-Time classes' related field in a runqueue: */
struct rt_rq {
	struct rt_prio_array	active; // priority array
	unsigned int		rt_nr_running;
	unsigned int		rr_nr_running;
#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
	struct {
		int		curr; /* highest queued rt task prio */ // rt进程的最高优先级
#ifdef CONFIG_SMP
		int		next; /* next highest */ // rt进程的次高优先级
#endif
	} highest_prio;
#endif
#ifdef CONFIG_SMP
	unsigned int		rt_nr_migratory;
	unsigned int		rt_nr_total;
	int			overloaded;
	struct plist_head	pushable_tasks;

#endif /* CONFIG_SMP */
	int			rt_queued;

	int			rt_throttled; // rt进程被禁止
	u64			rt_time; // 实时进程累计执行时间
	u64			rt_runtime; // 实时进程最大执行时间
	/* Nests inside the rq lock: */
	raw_spinlock_t		rt_runtime_lock;

#ifdef CONFIG_RT_GROUP_SCHED
	unsigned int		rt_nr_boosted;

	struct rq		*rq;
	struct task_group	*tg;
#endif
};

// https://elixir.bootlin.com/linux/v6.6.29/source/kernel/sched/sched.h#L708
/* Deadline class' related fields in a runqueue */
struct dl_rq {
	/* runqueue is an rbtree, ordered by deadline */
	struct rb_root_cached	root; // root.rb_root: sched_dl_entity对象组成的红黑树的根; root.rb_leftmost, 红黑树最左边的叶子

	unsigned int		dl_nr_running; // TASK_RUNNING的进程数

#ifdef CONFIG_SMP
	/*
	 * Deadline values of the currently executing and the
	 * earliest ready task on this rq. Caching these facilitates
	 * the decision whether or not a ready but not running task
	 * should migrate somewhere else.
	 */
	struct {
		u64		curr; // 当前sched_dl_entity
		u64		next; // 下一个sched_dl_entity
	} earliest_dl;

	unsigned int		dl_nr_migratory;
	int			overloaded;

	/*
	 * Tasks on this rq that can be pushed away. They are kept in
	 * an rb-tree, ordered by tasks' deadlines, with caching
	 * of the leftmost (earliest deadline) element.
	 */
	struct rb_root_cached	pushable_dl_tasks_root;
#else
	struct dl_bw		dl_bw;
#endif
	/*
	 * "Active utilization" for this runqueue: increased when a
	 * task wakes up (becomes TASK_RUNNING) and decreased when a
	 * task blocks
	 */
	u64			running_bw;

	/*
	 * Utilization of the tasks "assigned" to this runqueue (including
	 * the tasks that are in runqueue and the tasks that executed on this
	 * CPU and blocked). Increased when a task moves to this runqueue, and
	 * decreased when the task moves away (migrates, changes scheduling
	 * policy, or terminates).
	 * This is needed to compute the "inactive utilization" for the
	 * runqueue (inactive utilization = this_bw - running_bw).
	 */
	u64			this_bw;
	u64			extra_bw;

	/*
	 * Maximum available bandwidth for reclaiming by SCHED_FLAG_RECLAIM
	 * tasks of this rq. Used in calculation of reclaimable bandwidth(GRUB).
	 */
	u64			max_bw;

	/*
	 * Inverse of the fraction of CPU utilization that can be reclaimed
	 * by the GRUB algorithm.
	 */
	u64			bw_ratio;
};

// https://elixir.bootlin.com/linux/v6.6.29/source/kernel/sched/sched.h#L964
/*
 * This is the main, per-CPU runqueue data structure.
 *
 * Locking rule: those places that want to lock multiple runqueues
 * (such as the load balancing or the thread migration code), lock
 * acquire operations must be ordered by ascending &runqueue.
 */
struct rq {
	/* runqueue lock: */
	raw_spinlock_t		__lock;

	/*
	 * nr_running and cpu_load should be in the same cacheline because
	 * remote CPUs use both these fields when doing load calculation.
	 */
	unsigned int		nr_running; // TASK_RUNNING状态的进程数
#ifdef CONFIG_NUMA_BALANCING
	unsigned int		nr_numa_running;
	unsigned int		nr_preferred_running;
	unsigned int		numa_migrate_on;
#endif
#ifdef CONFIG_NO_HZ_COMMON
#ifdef CONFIG_SMP
	unsigned long		last_blocked_load_update_tick;
	unsigned int		has_blocked_load;
	call_single_data_t	nohz_csd;
#endif /* CONFIG_SMP */
	unsigned int		nohz_tick_stopped;
	atomic_t		nohz_flags;
#endif /* CONFIG_NO_HZ_COMMON */

#ifdef CONFIG_SMP
	unsigned int		ttwu_pending;
#endif
	u64			nr_switches;

#ifdef CONFIG_UCLAMP_TASK
	/* Utilization clamp values based on CPU's RUNNABLE tasks */
	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
	unsigned int		uclamp_flags;
#define UCLAMP_FLAG_IDLE 0x01
#endif

	struct cfs_rq		cfs; // 完全公平调度rq
	struct rt_rq		rt; // 实时调度rq
	struct dl_rq		dl; // deadline调度

#ifdef CONFIG_FAIR_GROUP_SCHED
	/* list of leaf cfs_rq on this CPU: */
	struct list_head	leaf_cfs_rq_list;
	struct list_head	*tmp_alone_branch;
#endif /* CONFIG_FAIR_GROUP_SCHED */

	/*
	 * This is part of a global counter where only the total sum
	 * over all CPUs matters. A task can increase this counter on
	 * one CPU and if it got migrated afterwards it may decrease
	 * it on another CPU. Always updated under the runqueue lock:
	 */
	unsigned int		nr_uninterruptible;

	struct task_struct __rcu	*curr; // 当前执行的进程
	struct task_struct	*idle; // idle进程
	struct task_struct	*stop; // stop进程
	unsigned long		next_balance;
	struct mm_struct	*prev_mm;

	unsigned int		clock_update_flags;
	u64			clock; // cpu累计运行的时间, 单位ns
	/* Ensure that all clocks are in the same cache line */
	u64			clock_task ____cacheline_aligned; // 进程累计占用cpu的时间
	u64			clock_pelt;
	unsigned long		lost_idle_time;
	u64			clock_pelt_idle;
	u64			clock_idle;
#ifndef CONFIG_64BIT
	u64			clock_pelt_idle_copy;
	u64			clock_idle_copy;
#endif

	atomic_t		nr_iowait;

#ifdef CONFIG_SCHED_DEBUG
	u64 last_seen_need_resched_ns;
	int ticks_without_resched;
#endif

#ifdef CONFIG_MEMBARRIER
	int membarrier_state;
#endif

#ifdef CONFIG_SMP
	struct root_domain		*rd;
	struct sched_domain __rcu	*sd;

	unsigned long		cpu_capacity;
	unsigned long		cpu_capacity_orig;

	struct balance_callback *balance_callback;

	unsigned char		nohz_idle_balance;
	unsigned char		idle_balance;

	unsigned long		misfit_task_load;

	/* For active balancing */
	int			active_balance;
	int			push_cpu;
	struct cpu_stop_work	active_balance_work;

	/* CPU of this runqueue: */
	int			cpu; // 对应的cpu
	int			online;

	struct list_head cfs_tasks;

	struct sched_avg	avg_rt;
	struct sched_avg	avg_dl;
#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
	struct sched_avg	avg_irq;
#endif
#ifdef CONFIG_SCHED_THERMAL_PRESSURE
	struct sched_avg	avg_thermal;
#endif
	u64			idle_stamp;
	u64			avg_idle;

	unsigned long		wake_stamp;
	u64			wake_avg_idle;

	/* This is used to determine avg_idle's max value */
	u64			max_idle_balance_cost;

#ifdef CONFIG_HOTPLUG_CPU
	struct rcuwait		hotplug_wait;
#endif
#endif /* CONFIG_SMP */

#ifdef CONFIG_IRQ_TIME_ACCOUNTING
	u64			prev_irq_time;
#endif
#ifdef CONFIG_PARAVIRT
	u64			prev_steal_time;
#endif
#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
	u64			prev_steal_time_rq;
#endif

	/* calc_load related fields */
	unsigned long		calc_load_update;
	long			calc_load_active;

#ifdef CONFIG_SCHED_HRTICK
#ifdef CONFIG_SMP
	call_single_data_t	hrtick_csd;
#endif
	struct hrtimer		hrtick_timer;
	ktime_t 		hrtick_time;
#endif

#ifdef CONFIG_SCHEDSTATS
	/* latency stats */
	struct sched_info	rq_sched_info;
	unsigned long long	rq_cpu_time;
	/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */

	/* sys_sched_yield() stats */
	unsigned int		yld_count;

	/* schedule() stats */
	unsigned int		sched_count;
	unsigned int		sched_goidle;

	/* try_to_wake_up() stats */
	unsigned int		ttwu_count;
	unsigned int		ttwu_local;
#endif

#ifdef CONFIG_CPU_IDLE
	/* Must be inspected within a rcu lock section */
	struct cpuidle_state	*idle_state;
#endif

#ifdef CONFIG_SMP
	unsigned int		nr_pinned;
#endif
	unsigned int		push_busy;
	struct cpu_stop_work	push_work;

#ifdef CONFIG_SCHED_CORE
	/* per rq */
	struct rq		*core;
	struct task_struct	*core_pick;
	unsigned int		core_enabled;
	unsigned int		core_sched_seq;
	struct rb_root		core_tree;

	/* shared state -- careful with sched_core_cpu_deactivate() */
	unsigned int		core_task_seq;
	unsigned int		core_pick_seq;
	unsigned long		core_cookie;
	unsigned int		core_forceidle_count;
	unsigned int		core_forceidle_seq;
	unsigned int		core_forceidle_occupation;
	u64			core_forceidle_start;
#endif

	/* Scratch cpumask to be temporarily used under rq_lock */
	cpumask_var_t		scratch_mask;

#if defined(CONFIG_CFS_BANDWIDTH) && defined(CONFIG_SMP)
	call_single_data_t	cfsb_csd;
	struct list_head	cfsb_csd_list;
#endif
};

// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L683
// task_struct的调度类执行其调度策略的逻辑
const struct sched_class	*sched_class;

// https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L1747
struct sched_class {
	const struct sched_class *next; // 链表. 它将五个sched_class对象链接成链表,链表头为&stop_sched_class,接下来分别为 dl_sched_class、rt_sched_class 、 fair_sched_class 和 idle_sched_class,它们在链表中的顺序对应了它们的优先级

#ifdef CONFIG_UCLAMP_TASK
	int uclamp_enabled;
#endif

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); // 将进程插入可执行队列
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); // 将进程从可执行队列删除
	void (*yield_task)   (struct rq *rq);
	bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt);

	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags); // 检查是否应该抢占rq的当前进程

	struct task_struct *(*pick_next_task)(struct rq *rq); // 确定下一个将要被调度执行的进程

	void (*put_prev_task)(struct rq *rq, struct task_struct *p); // 处理将要被抢占的进程
	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);

#ifdef CONFIG_SMP
	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags); // 为进程选择cpu
	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);

	void (*task_woken)(struct rq *this_rq, struct task_struct *task); // 进程已被唤醒

	void (*set_cpus_allowed)(struct task_struct *p,
				 const struct cpumask *newmask);

	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);
#endif

	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued); // 时钟中断发生
	void (*task_fork)(struct task_struct *p); // 新进程被创建
	void (*task_dead)(struct task_struct *p); // 进程已死

	/*
	 * The switched_from() call is allowed to drop rq->lock, therefore we
	 * cannot assume the switched_from/switched_to pair is serliazed by
	 * rq->lock. They are however serialized by p->pi_lock.
	 */
	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
			      int oldprio);

	unsigned int (*get_rr_interval)(struct rq *rq,
					struct task_struct *task);

	void (*update_curr)(struct rq *rq);

#define TASK_SET_GROUP		0
#define TASK_MOVE_GROUP		1

#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_change_group)(struct task_struct *p, int type);
#endif
};
```

这个结构定义了很多种方法,用于在队列上操作task, 其第一个成员变量，是一个指针，指向下一个调度类.

看部分sched_class定义的与调度有关的函数:
- enqueue_task向就绪队列中添加一个进程,当某个进程进入可运行状态时,调用这个函数
- dequeue_task 将一个进程从就就绪队列中删除
- pick_next_task 选择接下来要运行的进程
- put_prev_task 用另一个进程代替当前运行的进程
- set_curr_task 用于修改调度策略
- task_tick 每次周期性时钟到的时候,这个函数被调用,可能触发调度

sched_class有几种实现:
- stop_sched_class : 优先级最高的任务会使用这种策略,会中断所有其他线程,且不会被其他任务打断
- dl_sched_class : 对应上面的deadline调度策略
- rt_sched_class : 对应RR算法或者FIFO算法的调度策略,具体调度策略由进程的task_struct->policy指定;
- fair_sched_class : 普通进程的调度策略 // 对于普通进程来讲,公平是最重要的
- idle_sched_class : 空闲进程的调度策略

它们其实是放在一个链表上的,即有顺序关系.

![](/misc/img/process/1771657-20200314235232449-1386087933.png)

在调度核心代码kernel/sched/core.c中，使用的方式是task->sched_class->xxx_func，task_struck结构体中包含了任务所使用的调度器，进而能找到对应的函数指针来完成调用执行.

```c
// https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c#L3905
/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those loose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely((prev->sched_class == &idle_sched_class ||
		    prev->sched_class == &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = pick_next_task_fair(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto restart;

		/* Assumes fair_sched_class->next == idle_sched_class */
		if (!p) {
			put_prev_task(rq, prev);
			p = pick_next_task_idle(rq);
		}

		return p;
	}

restart:
#ifdef CONFIG_SMP
	/*
	 * We must do the balancing pass before put_next_task(), such
	 * that when we release the rq->lock the task is in the same
	 * state as before we took rq->lock.
	 *
	 * We can terminate the balance pass as soon as we know there is
	 * a runnable task of @class priority or higher.
	 */
	for_class_range(class, prev->sched_class, &idle_sched_class) {
		if (class->balance(rq, prev, rf))
			break;
	}
#endif

	put_prev_task(rq, prev);

	for_each_class(class) {
		p = class->pick_next_task(rq);
		if (p)
			return p;
	}

	/* The idle class should always have a runnable task: */
	BUG();
}

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L6961
struct task_struct *
pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	struct cfs_rq *cfs_rq = &rq->cfs;
	struct sched_entity *se;
	struct task_struct *p;
	int new_tasks;

again:
	if (!sched_fair_runnable(rq))
		goto idle;

#ifdef CONFIG_FAIR_GROUP_SCHED
	if (!prev || prev->sched_class != &fair_sched_class)
		goto simple;

	/*
	 * Because of the set_next_buddy() in dequeue_task_fair() it is rather
	 * likely that a next task is from the same cgroup as the current.
	 *
	 * Therefore attempt to avoid putting and setting the entire cgroup
	 * hierarchy, only change the part that actually changes.
	 */

	do {
		struct sched_entity *curr = cfs_rq->curr; // 取出当前正在运行的任务 curr

		/*
		 * Since we got here without doing put_prev_entity() we also
		 * have to consider cfs_rq->curr. If it is still a runnable
		 * entity, update_curr() will update its vruntime, otherwise
		 * forget we've ever seen it.
		 */
		if (curr) {
			if (curr->on_rq) // 如果依然是可运行的状态，也即处于进程就绪状态，则调用 update_curr 更新 vruntime
				update_curr(cfs_rq);
			else
				curr = NULL;

			/*
			 * This call to check_cfs_rq_runtime() will do the
			 * throttle and dequeue its entity in the parent(s).
			 * Therefore the nr_running test will indeed
			 * be correct.
			 */
			if (unlikely(check_cfs_rq_runtime(cfs_rq))) {
				cfs_rq = &rq->cfs;

				if (!cfs_rq->nr_running)
					goto idle;

				goto simple;
			}
		}

		se = pick_next_entity(cfs_rq, curr); // pick_next_entity 从红黑树里面，取最左边的一个节点
		cfs_rq = group_cfs_rq(se);
	} while (cfs_rq);

	p = task_of(se); // task_of 得到下一个调度实体对应的 task_struct

	/*
	 * Since we haven't yet done put_prev_entity and if the selected task
	 * is a different task than we started out with, try and touch the
	 * least amount of cfs_rqs.
	 */
	if (prev != p) { // 如果继任和前任不一样，这就说明有一个更需要运行的进程了，此时就需要更新红黑树了. 前面前任的 vruntime 更新过了，put_prev_entity 放回红黑树，会找到相应的位置，然后 set_next_entity 将继任者设为当前任务
		struct sched_entity *pse = &prev->se;

		while (!(cfs_rq = is_same_group(se, pse))) {
			int se_depth = se->depth;
			int pse_depth = pse->depth;

			if (se_depth <= pse_depth) {
				put_prev_entity(cfs_rq_of(pse), pse);
				pse = parent_entity(pse);
			}
			if (se_depth >= pse_depth) {
				set_next_entity(cfs_rq_of(se), se);
				se = parent_entity(se);
			}
		}

		put_prev_entity(cfs_rq, pse);
		set_next_entity(cfs_rq, se);
	}

	goto done;
simple:
#endif
	if (prev)
		put_prev_task(rq, prev);

	do {
		se = pick_next_entity(cfs_rq, NULL);
		set_next_entity(cfs_rq, se);
		cfs_rq = group_cfs_rq(se);
	} while (cfs_rq);

	p = task_of(se);

done: __maybe_unused;
#ifdef CONFIG_SMP
	/*
	 * Move the next running task to the front of
	 * the list, so our cfs_tasks list becomes MRU
	 * one.
	 */
	list_move(&p->se.group_node, &rq->cfs_tasks);
#endif

	if (hrtick_enabled(rq))
		hrtick_start_fair(rq, p);

	update_misfit_status(p, rq);

	return p;

idle:
	if (!rf)
		return NULL;

	new_tasks = newidle_balance(rq, rf);

	/*
	 * Because newidle_balance() releases (and re-acquires) rq->lock, it is
	 * possible for any higher priority task to appear. In that case we
	 * must re-start the pick_next_entity() loop.
	 */
	if (new_tasks < 0)
		return RETRY_TASK;

	if (new_tasks > 0)
		goto again;

	/*
	 * rq is about to be idle, check if we need to update the
	 * lost_idle_time of clock_pelt
	 */
	update_idle_rq_clock_pelt(rq);

	return NULL;
}
```

调度的时候是从优先级最高的调度类到优先级低的调度类,依次执行. 而对于每种调度类,有自己的实现CFS就是[fair_sched_class](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L11110).

对于同样的pick_next_task选取下一个要运行的任务这个动作,不同的调度类有自己的实现. fair_sched_class的实现是[pick_next_task_fair](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L6961),rt_sched_class的实现是pick_next_task_rt. 我们会发现这两个函数是操作不同的队列,[pick_next_task_rt](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/rt.c#L1627)操作的是rt_rq,pick_next_task_fair操作的是cfs_rq.

这样整个运行的场景就串起来了,在每个CPU上都有一个队列rq,这个队列里面包含多个子队列,例如rt_rq和cfs_rq,不同的队列有不同的实现方式,cfs_rq就是用红黑树实现的. 

当某个CPU需要找下一个任务执行的时候,会按照优先级依次调用调度类,不同的调度类操作不同的队列: rt_sched_class先被调用,它会在rt_rq上找下一个任务,只有找不到的时候,才轮到fair_sched_class被调用,它会在cfs_rq上找下一个任务. 这样保证了实时任务的优先级永远大于普通任务. 

![sched全局](/misc/img/process/sched_task)

#### 进程被创建
_do_fork创建进程分为两步,首先调用copy_process创建新进程, 然后调用wake_up_new_task唤醒新进程。copy_process中间会调用sched_fork.

rt_prio根据task_struct的prio字段的值是否小于100(MAX_RT_PRIO)判断进程是否为实时进程,如果不是,那么新进程会选择完全公平调度,sched_class字段被置为fair_sched_class。sched_class设置完毕后,调用它的task_fork字段的回调函数,为新进程设置调度策略相关的初始状态.

#### 唤醒进程
除了wake_up_new_task之外,内核还提供了wake_up_process(唤醒处于TASK_NORMAL状态的进程)和wake_up_state唤醒进程,后二者都调用try_to_wake_up实现.

try_to_wake_up只能唤醒state参数指定状态的进程,它可以是几个状 态 的 组 合 , 比 如 wake_up_process 传 递 的 TASK_NORMAL 就 等 于
TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE。另外,ttwu是try_to_wake_up的缩写。

第 2 步 , select_task_rq 为 进 程 选 择 CPU , 它 调 用 p->sched_class->select_task_rq计算得到指定的CPU号,然后根据CPU的情况和进程允
许的CPU集合等因素调整。

第3步,调用p->sched_class->enqueue_task。

在第4步中,check_preempt_curr检查被唤醒的进程是否抢占目标CPU当前执行的进程,主要逻辑如下。再强调一次,被唤醒的进程将要在目标CPU上执行,目标CPU不一定是当前正在执行ttwu的CPU,rq
也是目标CPU的rq.

check_preempt_curr很好地体现了sched_class的优先级,for_each_class展开后是一个for循环:for (class=&stop_sched_class;class;class=class->next).

如果rq当前进程的sched_class和被唤醒进程的sched_class相同,调用sched_class->check_preempt_curr决定执行顺序;如果rq当前进程的sched_class优先级更高,第一个break被执行,退出函数;而如果被唤醒进程的sched_class优先级更高,会调用resched_curr(rq).

resched_curr会设置rq当前进程的TIF_NEED_RESCHED标志,下一次中断发生时可能会进行进程切换。如果目标CPU不是当前CPU(前者是被唤醒进程所属的CPU,后者是正在执行这段代码的CPU),当前CPU会触发一个CPU间中断,目标CPU处理中断,然后切换进程。

wake_up_new_task和ttwu的主要逻辑基本一致.

总结,唤醒进程过程中,涉及 sched_class 的 select_task_rq 、check_preempt_curr和task_waken等操作.

#### 时钟中断
唤醒进程可能导致进程切换(set_tsk_need_resched),但它本身并不是一个可控的事情,还需要一个常规的机制切换进程,就是时钟中断.

传统的处理器以timer_interrupt函数作为irqaction的hander注册到内核,时钟中断发生时最终由timer_interrupt处理.

现代处理器的中断处理程序为apic_timer_interrupt, apic_timer_interrupt通过BUILD_INTERRUPT宏定义,它保存当前的上
下文环境,调用smp_apic_timer_interrupt,最终执行ret_from_intr。

smp_apic_timer_interrupt类似于do_IRQ,它调用entering_irq,通过local_apic_timer_interrupt函数处理中断,然后调用exiting_irq扫尾。
local_apic_timer_interrupt获得当前cpu的clock_event_device对象,回调该对象的event_handler字段指向的函数。

clock_event_device对象是由每cpu变量lapic_events表示的,根据系统运行的阶段和内核的编译选项,event_handler字段可能有三种值:
tick_handle_periodic、tick_nohz_handler和hrtimer_interrupt。这三种情况都可以完成更新进程占用CPU时间、设置thread_info的flags字段的
TIF_NEED_RESCHED标志的任务.

1. tick_handle_periodic函数
从periodic就可以看出tick_handle_periodic对应周期性的时钟,它
会调用tick_periodic,后者实现了时钟中断处理的主要逻辑,代码如
下。由于这种情况下中断是由硬件周期性产生的,所以软件上并不需
要设置下一个中断.

如果当前cpu负责更新系统时间,就调用do_timer(因为是周期性的中断,所以传递给do_timer的参数是1)更新jiffies,然后调用update_wall_time更新时间。

接着由update_process_times完成几个重要任务:
1. 调用account_process_tick给进程运行时间加上一个tick,如果中断前进程运行在用户态,就增加在进程用户态运行时间,否则增
加内核态运行时间。如果进程在一个tick内在用户态和内核态均运行过,那也只能以中断那一刻的状态为准。
2. 调用run_local_timers运行hrtimer和定时器。
3. 调用scheduler_tick

scheduler_tick调用sched_class的task_tick,task_tick一般需要计算当 前 进 程 是 否 需 要 已 经 用 光 它 的 CPU 时 间 , 如 果 是 , 则 调 用set_tsk_need_resched 置 位thread_info的flags字段的TIF_NEED_RESCHED标志.

update_rq_clock已经出现多次了,它负责更新rq的时间.

rq->clock表示该CPU累计运行时间,单位为纳秒 。update_rq_clock_task更新各进程占用CPU的累计时间,传递至它的
delta 有 可 能 会 减 去 中 断 处 理 ( IRQ_TIME_ACCOUNTING ) 和PARAVIRT(PARAVIRT_TIME_ACCOUNTING)占用CPU的时间,然
后加到rq->clock_task字段上,也就是说rq->clock_task逻辑上表示进程占用CPU的有效时间。

1. tick_nohz_handler函数
tick_nohz_handler对应低精度的时钟状态下clock_event_device对象的event_handler.

tick_sched_do_timer会调用tick_do_update_jiffies64更新时间,tick_sched_handle则负责调用update_process_times完成上文介绍的任
务。最后,调用tick_program_event设置下一个时钟中断。整体上,每次时钟中断发生会由tick_sched_do_timer 和tick_sched_handle实现主要逻辑,然后设置时钟设备来触发下一次中断.

1. hrtimer_interrupt函数
hrtimer_interrupt 是 为 hrtimer(high resolution timer) 机 制 设 计 的 ,hrtimer定时器的执行有三种情况:时钟中断、HRTIMER_SOFTIRQ软
中断和run_local_timers函数(由update_process_times函数调用),前两种情况都涉及hrtimer_interrupt函数,下面分析第一种情况。需要说
明的是,hrtimer_interrupt只是负责运行到期的hrtimer定时器,并不负责计算进程占用CPU的时间和进程调度等任务,有一个专门的hrtimer
负责该任务。

hrtimer与timer都是定时器,是内核中实现延迟、异步操作的有力工具,在驱动中随处可见,内核的某些机制如工作队列也使用了它们。timer是软中断的一种,它所能延迟的时间以jiffy为单位;hrtimer既是一个时钟中断相关的概念,又是一种软中断,它所能延迟的时间以纳秒为单位,具体值与时钟设备有关,一般比jiffy要小几个数量级,二者的应用场景互补。请注意,本节讨论的hrtimer在nohz模式中
的高精度模式的背景下才有意义.

hrtimer 机 制 共 有 三 个 关 键 结 构 体 : hrtimer_cpu_base 、hrtimer_clock_base和hrtimer。hrtimer_cpu_base与hrtimer_clock_base是
一对多的关系,由数组实现;hrtimer_clock_base与hrtimer也是一对多的关系,由红黑树实现。为了简洁,在不引起歧义的情况下,除非特
别 指 明 , 本 章 下 文 中 均 以 hrtimer 指 代 hrtimer 结 构 体 , 以 timer 指 代hrtimer 对 象 ( 不 要 与 普 通 定 时 器 混 淆 ) , 以 cpu_base 表 示
hrtimer_cpu_base对象,以clock_base表示hrtimer_clock_base对象。以启动和运行表示hrtimer和定时器的两种不同状态,与定时炸弹类似,启
动表示定时炸弹开始倒计时,运行表示炸弹爆炸。

hrtimer_cpu_base 对 象 是 由 每 cpu 变 量 hrtimer_bases 表 示 的 , 由hrtimers_init函数赋初值。hrtimer_cpu_base结构体的clock_base字段类
型为hrtimer_clock_base数组,该数组有8个元素,每一个元素对应一种类 型 的 hrtimer_clock_base 对 象 。 这 8 种 类 型 分 别 为
HRTIMER_BASE_MONOTONIC 、 HRTIMER_BASE_REALTIME 、HRTIMER_BASE_BOOTTIME 、 HRTIMER_BASE_TAI 和 它 们 4 个 的
XXX_SOFT。

hrtimer_clock_base包含红黑树的根,可以访问hrtimer对象组成的红黑树.

```c
// https://elixir.bootlin.com/linux/v6.6.29/source/include/linux/hrtimer.h#L118
/**
 * struct hrtimer - the basic hrtimer structure
 * @node:	timerqueue node, which also manages node.expires,
 *		the absolute expiry time in the hrtimers internal
 *		representation. The time is related to the clock on
 *		which the timer is based. Is setup by adding
 *		slack to the _softexpires value. For non range timers
 *		identical to _softexpires.
 * @_softexpires: the absolute earliest expiry time of the hrtimer.
 *		The time which was given as expiry time when the timer
 *		was armed.
 * @function:	timer expiry callback function
 * @base:	pointer to the timer base (per cpu and per clock)
 * @state:	state information (See bit values above)
 * @is_rel:	Set if the timer was armed relative
 * @is_soft:	Set if hrtimer will be expired in soft interrupt context.
 * @is_hard:	Set if hrtimer will be expired in hard interrupt context
 *		even on RT.
 *
 * The hrtimer structure must be initialized by hrtimer_init()
 */
struct hrtimer {
	struct timerqueue_node		node; // node表示红黑树的节点
	ktime_t				_softexpires; // timer被执行的最早时间
	enum hrtimer_restart		(*function)(struct hrtimer *); // 到期时执行的回调
	struct hrtimer_clock_base	*base; // 指向clock_base
	u8				state;
	u8				is_rel;
	u8				is_soft;
	u8				is_hard;
};

// https://elixir.bootlin.com/linux/v6.6.29/source/include/linux/hrtimer.h#L159
/**
 * struct hrtimer_clock_base - the timer base for a specific clock
 * @cpu_base:		per cpu clock base
 * @index:		clock type index for per_cpu support when moving a
 *			timer to a base on another cpu.
 * @clockid:		clock id for per_cpu support
 * @seq:		seqcount around __run_hrtimer
 * @running:		pointer to the currently running hrtimer
 * @active:		red black tree root node for the active timers
 * @get_time:		function to retrieve the current time of the clock
 * @offset:		offset of this clock to the monotonic base
 */
struct hrtimer_clock_base {
	struct hrtimer_cpu_base	*cpu_base; // 指向cpu_base
	unsigned int		index;
	clockid_t		clockid; // clock base的种类
	seqcount_raw_spinlock_t	seq;
	struct hrtimer		*running;
	struct timerqueue_head	active; // active.head, 红黑树的根;active.next, 指向下一个将要到期的timer 
	ktime_t			(*get_time)(void);
	ktime_t			offset;
} __hrtimer_clock_base_align;
```

hrtimer表示用户的一次延迟操作申请, 其state字段有两种值:HRTIMER_STATE_INACTIVE表示timer未启
动,HRTIMER_STATE_ENQUEUED表示timer已启动。内核中提供了hrtimer_active、hrtimer_is_queued 和hrtimer_callback_running三个函数
来判断hrtimer的当前状态,其中hrtimer_callback_running表示timer在运行.

内核提供了几个使用hrtimer常用的函数:
- hrtimer_init: 初始化timer
- hrtimer_start: 启动timer
- hrtimer_start_expires: 同上
- hrtimer_start_range_ns: 同上
- hrtimer_forward: 将timer延期
- hrtimer_cancel: 取消timer, 如果正运行, 等待执行完成
- hrtimer_try_to_cancel: 取消timer, 如果正运行, 返回-1, 等待运行返回1, 否则返回0

hrtimer_init第二个参数clock_id表示hrtimer_clock_base对象的类型,下文以HRTIMER_BASE_MONOTONIC为例。第三个参数为hrtimer_mode类型,有多种可能:HRTIMER_MODE_ABS表示绝对时间;HRTIMER_MODE_REL表示相对时间; HRTIMER_MODE_PINNED表示hrtimer倾向当前cpu,HRTIMER_MODE_SOFT表示timer会在软中断中执行,也可以是它们的组合.

hrtimer_init会获得当前cpu的cpu_base,根据clock_id的值将clock_base字段相应的数组元素赋值给timer的base字段。hrtimer_startxxx最终都通过hrtimer_start_range_ns实现.

传递至该函数的timer有可能处于HRTIMER_STATE_ENQUEUED状态,果真如此,remove_hrtimer会将其从它所属的红黑树上删除。
计算后的tim表示基准值,会被hrtimer_set_expires_range_ns赋值给timer的_softexpires字段,tim与delta_ns合成后的值表示到期时间,被
赋值给timer的node的expires字段。

当 前 时 间 不 早 于 _softexpires 表 示 的 时 间 timer 就 可 以 执 行 , 但expires才能作为设置时钟中断时间的依据。通俗地讲,_softexpires是
随缘时间,如果时钟中断发生时恰巧满足它的条件就执行timer;expires是最后时间,如果没有随缘成功,会依据它来触发时钟中断,多数情况下二者是相同的。

完成赋值后,enqueue_hrtimer将timer插入红黑树,如果新插入的timer是整个树中最先到期的(timerqueue_node的expires值最小),返
回值leftmost就为真(红黑树是有顺序的,这里最小的值在最左边)。如果leftmost为真,调用hrtimer_reprogram;如果timer属于当前cpu,
hrtimer_reprogram会设置下一个时钟中断。

hrtimer_reprogram最终会调用clockevents_program_event,后者会根据timer的到期时间和时钟设备的参数计算得到下一个时钟中断到来
的以纳秒为单位的时间间隔delta,将该时间转化为时钟设备对应的clock周期数,最后设置生效.

新插入的timer以ktime(一般为纳秒)为单位,而如果leftmost为真,软件上会立即根据timer的到期时间设置时钟设备的寄存器触发下一个中断,所以利用hrtimer可以实现比jiffy小很多的时间间隔,如果硬件支持的话.

2. hrtimer_interrupt函数解析
每次时钟中断发生都会执行hrtimer_interrupt,由它运行之前插入红黑树的已到期的timer,它会调用`__hrtimer_run_queues`.

hrtimer_interrupt 会 处 理 当 前 CPU 的 cpu_base 对 应 的 每 一 个clock_base上的timer。它首先由hrtimer_update_base得到当前时间后计
算得到针对各clock_base的时间basenow,然后在while循环内按照顺序遍 历 红 黑 树 上 的 timer , 如 果 basenow 表 示 的 时 间 不 早 于 timer 的_softexpires时间,调用__run_hrtimer执行timer,否则就根据timer的node.expires计算得到下一个时钟中断的时间expires_next。

注意,timer插入红黑树的时候排序是以expires值为依据的,与_softexpires的值无关,所以顺序访问timer并不代表_softexpires值也是从 小 到 大 访 问 的 。 举 个 例 子 , `timer1 : {[_softexpires:100], [expires:100]} , timer2 : {[_softexpires:90], [expires:110]} , basenow :
95`。在红黑树中,timer1无疑在timer2左边,这时timer1得不到执行,循环直接退出,即使timer2的_softexpires值小于basenow。这也是随缘不成功的一种情况。

__run_hrtimer负责执行timer,并对timer进行处理

__run_hrtimer调用__remove_hrtimer将timer从红黑树上删除,最后将timer的状态修改为HRTIMER_STATE_INACTIVE,执行timer的回调
函数后,如果函数返回值不等于HRTIMER_NORESTART,就重新将timer 插 入 红 黑 树 。 虽 然 timer 的 状 态 被 修 改 为
HRTIMER_STATE_INACTIVE,但是在执行完毕之前,它的状态依然是active,因为此时base->running==timer成立。如果执行完毕后,timer
被重新enqueue,它的状态又会变成HRTIMER_STATE_ENQUEUED。所以只有在timer执行完毕,并且没有被再次enqueue的情况下才是真正
inactive的。

3.10 版 的 内 核 的 实 现 有 所 不 同 , 它 定 义 了HRTIMER_STATE_CALLBACK 和 HRTIMER_STATE_MIGRATE 两 个额外的状态,分别表示timer被执行和迁移到其他CPU,只要不是HRTIMER_STATE_INACTIVE状态,hrtimer_active都为真。

最后,处理完毕所有到期的timer之后,hrtimer_interrupt会根据计算得到的expires_next的值来设置下一个时钟中断。下一个时钟中断到
来后,重复该流程。

需 要 强 调 的 是 , timer 的 回 调 函 数 返 回 值 目 前 有HRTIMER_NORESTART和HRTIMER_RESTART两种,返回其他值编
译器当然也不会报错,但为了未来的扩展请不要这么做。如果函数返回HRTIMER_RESTART,函数返回前必须通过调用hrtimer_forward函
数等方式将timer延期,否则程序就成了无限循环,这肯定不是你想要的。原因是timer被重新插入红黑树后,它依然会是下一个到期的
timer,且到期时间始终不变。另外,timer的操作依然是在中断上下文中,需要遵守中断函数的编写原则.

3. hrtimer的使用和sched_timer

hrtimer的使用和sched_timerhrtimer的使用与进程调度并没有直接关系,插入本节的原因一方面是hrtimer与时钟中断关系密切,另一方面有一个特殊的hrtimer定时器与进程调度有关系。

从驱动工程师的角度,使用hrtimer只需要负责timer的初始化与启动即可,timer的运行由内核控制。

如 果 只 是 需 要 timer 在 某 时 间 后 执 行 一 次 , 使 用 hrtimer_init 和hrtimer_start即可。需要注意的是,不要在my_dev_func中进行复杂操
作,如果需要可以利用工作队列等来实现;my_dev_func应该返回HRTIMER_NORESTART.

如果需要timer每隔一段时间间隔执行一次,有两个方案可以选择,下面分别以sched_timer和设备的轮询为例进行介绍。
前文说过,完成进程调度相关任务的是一个特殊的timer,它就是每cpu变量tick_cpu_sched的sched_timer字段表示的内嵌hrtimer结构体,
称之为sched_timer。

sched_timer在tick_setup_sched_timer函数中完成初始化.

tick_sched_timer 在 每 次 sched_timer 到 期 的 时 候 执 行 , 它 调 用tick_sched_do_timer 和 tick_sched_handle ( 调 用 update_process_times )完成进程调度相关的任务。然后通过hrtimer_forward增加到期时间, 最后返回HRTIMER_RESTART重新加入红黑树。这样,在不进行重新设置的情况下,sched_timer就会每隔tick_period执行一次。

这就是循环使用hrtimer的第一种方案:使timer的function字段的函数返回HRTIMER_RESTART。第二种方案是通过不断将timer插入红黑树实现.

第一种方案相对简洁,但在timer的function回调函数中不能进行复杂的操作;第二种情况常见于需要I/O读写等操作的驱动中用来实现设
备的轮询。

最后,必须注意的是在使用普通定时器可以满足需要的情况下, 优先选择普通定时器,hrtimer一般用于满足时间间隔小于jiffy的需求。因为hrtimer需要额外进行设置时钟等操作,所以高精度并不是没有代价的。

总结,时钟中断场景中涉及sched_class的task_tick操作.

### [wait_event](https://elixir.bootlin.com/linux/v5.11/source/include/linux/wait.h#L314)
wait_event实现进程调度: 定义一个wait结构, 然后设置进程睡眠, 如果有其他进程唤醒该进程后, 判断条件是否满足, 如果满足则删除wait对象, 否则进程继续睡眠.

- wait_event_timeout : 与wait_event的区别是有timeout. 超时后进程同样恢复运行
- wait_event_interruptible : 此时进程处于可中断睡眠, 可以接收信号; 而wait_event设置进程处于不可中断睡眠, 不能接收信号
- wait_event_interruptible_timeout : 与wait_event_interruptible相比, 超时后进程同样恢复运行
- wait_event_interruptible_exclusive : 与wait_event_interruptible的区别是排他性等待

	> 非排他性等待: 有一些进程在等待队列中, 当唤醒时, kernel会唤醒所有的进程

	> 排他性等待: 设置该flag后, 当唤醒时, kernel会唤醒所有非排他性进程和一个排他性进程

### 调度
分两种:
- 主动调度
- 抢占调度

#### 主动调度
![主动让出调度](/misc/img/process/schedule.png)

进程切换由schedule或者__schedule函数实现,前者调用而后者实现,当前进程被另一个由调度器指定的进程替代.

schedule一般在以下几种情况下执行。
1. 进程主动放弃CPU,如sleep、等待某事件或条件等。
1. 内核同步导致进程无法获得执行权,比如信号量、互斥锁等。
1. 进程处理中断或异常后,如果需要返回用户空间,调用resume_userspace将要返回用户空间时,如果它的TIF_NEED_RESCHED标志被置位,会执行schedule。
1. 进程处理中断或异常后,如果需要继续在内核空间继续执行,分为以下两种情况:内核是可抢占的(CONFIG_PREEMPT=y),resume_kernel(见4.3小节)继续执行时检查抢占是否使能,抢占使能的情况下,可能会执行schedule。抢占被禁止的情况下,不会执行schedule。如果内核是不可抢占的(CONFIG_PREEMPT=n),进程在内核态执行不会被抢占.

`__schedule`是进程切换的核心.

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3499
asmlinkage __visible void __sched schedule(void)
{
	struct task_struct *tsk = current;

	sched_submit_work(tsk);
	do {
		preempt_disable();
		__schedule(false);
		sched_preempt_enable_no_resched();
	} while (need_resched());
	sched_update_worker(tsk);
}
EXPORT_SYMBOL(schedule);
```

这段代码的主要逻辑是在`__schedule`函数中实现的:
```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3499
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;
	if (!preempt && prev->state) {
		if (signal_pending_state(prev->state, prev)) {
			prev->state = TASK_RUNNING;
		} else {
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf); // 获取下一个任务，task_struct *next 指向下一个任务
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		psi_sched_switch(prev, next, !task_on_rq_queued(prev));

		trace_sched_switch(preempt, prev, next);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf); // 当选出的继任者和前任不同，就要进行上下文切换，继任者进程正式进入运行
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
```

首先，. task_struct *prev(`prev = rq->curr`) 指向这个 CPU 的任务队列上面正在运行的那个进程 curr, 因为一旦将来它被切换下来，那它就成了前任了.

如果进程因为占用CPU的时间到了而被抢占,进程依然会处于可运行状态(TASK_RUNNING)。但如果进程是因为等待资源等原因
放弃CPU,一般需要更改它的状态,比如wait_event_interruptible会将进程状态置为TASK_INTERRUPTIBLE,然后调用schedule,这种情况
下进程不能再插入可运行队列。

第1步`!preempt && prev->state`就对应这种情况, deactivate_task 会 调 用 dequeue_task , 进 而 调 用 sched_class 的dequeue_task将进程从可运行队列删除

第2步`pick_next_task`, 3.10版内核中会调用pre_schedule和put_prev_task分别调用sched_class的pre_schedule和put_prev_task做进程切换前的准备工作,然后调用pick_next_task负责选择下一个将要占用CPU执行的进程。5.05 版 内 核 去 掉 了 pre_schedule 和 put_prev_task , 直 接 调 用pick_next_task

pick_next_task比较有趣,它首先判断rq的可运行进程的数量是否与完全公平队列的可运行进程的数量相等。相等则意味着其他几个调度器并没有可运行进程,直接调用fair_sched_class的pick_next_task选择下一个进程即可。

如果其他调度器存在可运行进程,那么按照四个sched_class的优先级顺序先后调用它们的pick_next_task选择下一个进程,找到即止。这里又一次体现了sched_class的优先级,也从一定程度上实现了进程的 优 先 级 机 制 。 比 如 实 时 进 程 和 普 通 进 程 分 别 由 rt_sched_class 和
fair_sched_class管理,rt_sched_class的优先级更高,所以实时进程一般会优先执行。类似的事情在我们的生活中比比皆是,例如总会有一群
人享受优先占用资源的便利,有些情况是为了整体利益最大化,有些则不一定。程序的世界虽然相对简单,内核有一视同仁的准则,但进
程的优先级很大程度上掌握在程序员手中,程序员也应该从系统的角度出发,不可仅考虑自身模块。

第2步完成后,prev对应当前进程,next对应下一个将要执行的情况。

第3步,context_switch是整个__schedule的核心.

#### 进程上下文切换
上下文切换主要干两件事情,一是切换进程空间,也即虚拟内存;二是切换寄存器和CPU上下文
```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3402
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
	       struct task_struct *next, struct rq_flags *rf)
{
	prepare_task_switch(rq, prev, next);

	/*
	 * For paravirt, this is coupled with an exit in switch_to to
	 * combine the page table reload and the switch backend into
	 * one hypercall.
	 */
	arch_start_context_switch(prev);

	/*
	 * kernel -> kernel   lazy + transfer active
	 *   user -> kernel   lazy + mmgrab() active
	 *
	 * kernel ->   user   switch + mmdrop() active
	 *   user ->   user   switch
	 */
	if (!next->mm) {                                // to kernel
		enter_lazy_tlb(prev->active_mm, next);

		next->active_mm = prev->active_mm;
		if (prev->mm)                           // from user
			mmgrab(prev->active_mm);
		else
			prev->active_mm = NULL;
	} else {                                        // to user
		membarrier_switch_mm(rq, prev->active_mm, next->mm);
		/*
		 * sys_membarrier() requires an smp_mb() between setting
		 * rq->curr / membarrier_switch_mm() and returning to userspace.
		 *
		 * The below provides this either through switch_mm(), or in
		 * case 'prev->active_mm == next->mm' through
		 * finish_task_switch()'s mmdrop().
		 */
		switch_mm_irqs_off(prev->active_mm, next->mm, next);

		if (!prev->mm) {                        // from kernel
			/* will mmdrop() in finish_task_switch(). */
			rq->prev_mm = prev->active_mm;
			prev->active_mm = NULL;
		}
	}

	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	prepare_lock_switch(rq, next, rf);

	/* Here we just switch the register state and the stack. */
	switch_to(prev, next, prev);
	barrier(); // barrier语句是一个编译器指令,用于保证switch_to和finish_task_switch的执行顺序,不会因为编译阶段优化而改变, 忽略即可

	return finish_task_switch(prev);
}
```

内核线程不会访问用户空间的内存,而各进程内核空间的内存是相同的,所以内核线程使用上一个进程的内存是可行的.

如果next不是内核线程,则调用switch_mm_irqs_off切换内存。

如果prev是内核线程,它将被next替代,所以没有必要再为它保留active_mm,但是在它之前被调度执行时,mm_count的引用计数被增
加了,可能导致active_mm不能得到完整释放,rq->prev_mm = oldmm将它传递给rq,进程切换完成后根据mm_count的值做进一步处理.

首先是内存空间的切换. 接下来，就是switch_to. 它就是寄存器和栈的切换, 它与arch相关，它调用到了 __switch_to_asm. 这是一段汇编代码，主要用于栈的切换.

当进程 A 在内核里面执行 switch_to 的时候，内核态的指令指针也是指向这一行的. 但是在 switch_to 里面，将寄存器和栈都切换到成了进程 B 的，唯一没有变的就是指令指针寄存器. 当 switch_to 返回的时候，指令指针寄存器指向了下一条语句 finish_task_switch. 但这个时候的 finish_task_switch 已经不是进程 A 的 finish_task_switch 了，而是进程 B 的 finish_task_switch 了.

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/include/asm/switch_to.h#L47
#define switch_to(prev, next, last)					\
do {									\
	((last) = __switch_to_asm((prev), (next)));			\
} while (0)
```

switch_to三个参数的理解:
- prev和next很好理解: prev指向当前进程，next指向被调度的进程
- last

	用于保存由谁触发切换回prev.

	比如切换流程:`A->B->C->A`, 当切换回A时, last就是C. 此时`finish_task_switch(prev)`就是`finish_task_switch(C)`

```x86asm
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/entry/entry_64.S#L223
// 这里是64bit的, 32bit的在arch/x86/entry/entry_32.S
/*
 * %rdi: prev task
 * %rsi: next task
 */
.pushsection .text, "ax"
SYM_FUNC_START(__switch_to_asm)
	/*
	 * Save callee-saved registers
	 * This must match the order in inactive_task_frame
	 */
	pushq	%rbp
	pushq	%rbx
	pushq	%r12
	pushq	%r13
	pushq	%r14
	pushq	%r15

	/* switch stack */
	movq	%rsp, TASK_threadsp(%rdi)
	movq	TASK_threadsp(%rsi), %rsp

#ifdef CONFIG_STACKPROTECTOR
	movq	TASK_stack_canary(%rsi), %rbx
	movq	%rbx, PER_CPU_VAR(fixed_percpu_data) + stack_canary_offset
#endif

#ifdef CONFIG_RETPOLINE
	/*
	 * When switching from a shallower to a deeper call stack
	 * the RSB may either underflow or use entries populated
	 * with userspace addresses. On CPUs where those concerns
	 * exist, overwrite the RSB with entries which capture
	 * speculative execution to prevent attack.
	 */
	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
#endif

	/* restore callee-saved registers */
	popq	%r15
	popq	%r14
	popq	%r13
	popq	%r12
	popq	%rbx
	popq	%rbp

	jmp	__switch_to
SYM_FUNC_END(__switch_to_asm)
.popsection
```

最终，到了 __switch_to 函数.

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/kernel/process_64.c#L425
// 这里是64bit的, 32bit的在arch/x86/kernel/process_64.c
/*
 *	switch_to(x,y) should switch tasks from x to y.
 *
 * This could still be optimized:
 * - fold all the options into a flag word and test it with a single test.
 * - could test fs/gs bitsliced
 *
 * Kprobes not supported here. Set the probe on schedule instead.
 * Function graph tracer not supported too.
 */
__visible __notrace_funcgraph struct task_struct *
__switch_to(struct task_struct *prev_p, struct task_struct *next_p)
{
	struct thread_struct *prev = &prev_p->thread;
	struct thread_struct *next = &next_p->thread;
	struct fpu *prev_fpu = &prev->fpu;
	struct fpu *next_fpu = &next->fpu;
	int cpu = smp_processor_id();

	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
		     this_cpu_read(irq_count) != -1);

	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
		switch_fpu_prepare(prev_fpu, cpu);

	/* We must save %fs and %gs before load_TLS() because
	 * %fs and %gs may be cleared by load_TLS().
	 *
	 * (e.g. xen_load_tls())
	 */
	save_fsgs(prev_p);

	/*
	 * Load TLS before restoring any segments so that segment loads
	 * reference the correct GDT entries.
	 */
	load_TLS(next, cpu);

	/*
	 * Leave lazy mode, flushing any hypercalls made here.  This
	 * must be done after loading TLS entries in the GDT but before
	 * loading segments that might reference them.
	 */
	arch_end_context_switch(next_p);

	/* Switch DS and ES.
	 *
	 * Reading them only returns the selectors, but writing them (if
	 * nonzero) loads the full descriptor from the GDT or LDT.  The
	 * LDT for next is loaded in switch_mm, and the GDT is loaded
	 * above.
	 *
	 * We therefore need to write new values to the segment
	 * registers on every context switch unless both the new and old
	 * values are zero.
	 *
	 * Note that we don't need to do anything for CS and SS, as
	 * those are saved and restored as part of pt_regs.
	 */
	savesegment(es, prev->es);
	if (unlikely(next->es | prev->es))
		loadsegment(es, next->es);

	savesegment(ds, prev->ds);
	if (unlikely(next->ds | prev->ds))
		loadsegment(ds, next->ds);

	x86_fsgsbase_load(prev, next);

	/*
	 * Switch the PDA and FPU contexts.
	 */
	this_cpu_write(current_task, next_p);
	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));

	switch_fpu_finish(next_fpu);

	/* Reload sp0. */
	update_task_stack(next_p);

	switch_to_extra(prev_p, next_p);

	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
		/*
		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
		 * does not update the cached descriptor.  As a result, if we
		 * do SYSRET while SS is NULL, we'll end up in user mode with
		 * SS apparently equal to __USER_DS but actually unusable.
		 *
		 * The straightforward workaround would be to fix it up just
		 * before SYSRET, but that would slow down the system call
		 * fast paths.  Instead, we ensure that SS is never NULL in
		 * system call context.  We do this by replacing NULL SS
		 * selectors at every context switch.  SYSCALL sets up a valid
		 * SS, so the only way to get NULL is to re-enter the kernel
		 * from CPL 3 through an interrupt.  Since that can't happen
		 * in the same task as a running syscall, we are guaranteed to
		 * context switch between every interrupt vector entry and a
		 * subsequent SYSRET.
		 *
		 * We read SS first because SS reads are much faster than
		 * writes.  Out of caution, we force SS to __KERNEL_DS even if
		 * it previously had a different non-NULL value.
		 */
		unsigned short ss_sel;
		savesegment(ss, ss_sel);
		if (ss_sel != __KERNEL_DS)
			loadsegment(ss, __KERNEL_DS);
	}

	/* Load the Intel cache allocation PQR MSR. */
	resctrl_sched_in();

	return prev_p;
}
```

没有了`[struct tss_struct](https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/include/asm/processor.h#L411) *tss = &per_cpu(cpu_tss_rw, cpu)`, deleted in v4.19.6~v4.19.7 on ff16701a29cba3aafa0bd1656d766813b2d0a811 for " x86/process: Consolidate and simplify switch_to_xtra() code". 从该commit msg上看, 相应代码被移入了[switch_to_bitmap()](https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/kernel/process.c#L339).

在 x86 体系结构中，提供了一种以硬件的方式进行进程切换的模式，对于每个进程，x86 希望在内存里面维护一个 TSS（Task State Segment，任务状态段）结构. 这里面有所有的寄存器信息. 另外，还有一个特殊的寄存器 TR（Task Register，任务寄存器），指向某个进程的 TSS. 更改 TR 的值，将会触发硬件保存 CPU 所有寄存器的值到当前进程的 TSS 中，然后从新进程的 TSS 中读出所有寄存器值，加载到 CPU 对应的寄存器中.

但是这样有个缺点, 我们做进程切换的时候，没必要每个寄存器都切换，这样每个进程一个 TSS，就需要全量保存，全量切换，动作太大了. 于是，kernel会给每一个 CPU 关联一个 TSS(by cpu_init)，然后将 TR 指向这个 TSS，然后在操作系统的运行过程中，TR 就不切换了，永远指向这个 TSS. TSS 用数据结构 tss_struct 表示.

在 Linux 中，真的参与进程切换的寄存器很少，主要的就是栈顶寄存器. 于是，在 task_struct的成员变量 thread保留了要切换进程时需要修改的寄存器.

#### 指令指针的保存与恢复
task切换:
- 用户栈

	从进程 A 切换到进程 B，在切换内存空间的时候就已经切换用户栈了. 因为每个进程的用户栈都是独立的，都在内存空间里面.

- 内核栈

	在 __switch_to 里面切换，也就是将 current_task 指向当前的 task_struct, 里面的 void *stack 指针，指向的就是当前的内核栈

- 内核栈的栈顶指针

	在 __switch_to_asm 里面已经切换了栈顶指针

- 用户栈的栈顶指针

	如果当前在内核里面的话，它当然是在内核栈顶部的 pt_regs 结构里面呀. 当从内核返回用户态运行的时候，pt_regs 里面有所有当时在用户态的时候运行的上下文信息，就可以开始运行了.


进程调度第一定律: **进程的调度都最终会调用到 __schedule 函数**.

#### 抢占式调度
衡量一个进程的运行时间靠时钟，它每过一段时间会触发一次时钟中断，通知操作系统，因此这是个很好的切入点(时间点)来查看是否需要抢占.

时钟中断处理函数会调用`scheduler_tick()`:
```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3669
/*
 * This function gets called by the timer code, with HZ frequency.
 * We call it with interrupts disabled.
 */
void scheduler_tick(void)
{
	int cpu = smp_processor_id();
	struct rq *rq = cpu_rq(cpu);
	struct task_struct *curr = rq->curr;
	struct rq_flags rf;
	unsigned long thermal_pressure;

	arch_scale_freq_tick();
	sched_clock_tick();

	rq_lock(rq, &rf);

	update_rq_clock(rq);
	thermal_pressure = arch_scale_thermal_pressure(cpu_of(rq));
	update_thermal_load_avg(rq_clock_thermal(rq), rq, thermal_pressure);
	curr->sched_class->task_tick(rq, curr, 0);
	calc_global_load_tick(rq);
	psi_task_tick(rq);

	rq_unlock(rq, &rf);

	perf_event_task_tick();

#ifdef CONFIG_SMP
	rq->idle_balance = idle_cpu(cpu);
	trigger_load_balance(rq);
#endif
}
```

这个函数先取出当前 CPU 的运行队列，然后得到这个队列上当前正在运行中的进程的 task_struct，然后调用这个 task_struct 的调度类的 task_tick 函数，顾名思义这个函数就是来处理时钟事件的. 如果当前运行的进程是普通进程，调度类为 [fair_sched_class](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L11110)，调用的处理时钟的函数为 [task_tick_fair](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L10621):

```c
/*
 * scheduler tick hitting a task of our scheduling class.
 *
 * NOTE: This function can be called remotely by the tick offload that
 * goes along full dynticks. Therefore no local assumption can be made
 * and everything must be accessed through the @rq and @curr passed in
 * parameters.
 */
static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued)
{
	struct cfs_rq *cfs_rq;
	struct sched_entity *se = &curr->se;

	for_each_sched_entity(se) {
		cfs_rq = cfs_rq_of(se);
		entity_tick(cfs_rq, se, queued);
	}

	if (static_branch_unlikely(&sched_numa_balancing))
		task_tick_numa(rq, curr);

	update_misfit_status(curr, rq);
	update_overutilized_status(task_rq(curr));
}
```

根据当前进程的 task_struct，找到对应的调度实体 sched_entity 和 cfs_rq 队列，调用 [entity_tick](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L4496):
```c
static void
entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
{
	/*
	 * Update run-time statistics of the 'current'.
	 */
	update_curr(cfs_rq);

	/*
	 * Ensure that runnable average is periodically updated.
	 */
	update_load_avg(cfs_rq, curr, UPDATE_TG);
	update_cfs_group(curr);

#ifdef CONFIG_SCHED_HRTICK
	/*
	 * queued ticks are scheduled to match the slice, so don't bother
	 * validating it and just reschedule.
	 */
	if (queued) {
		resched_curr(rq_of(cfs_rq));
		return;
	}
	/*
	 * don't let the period tick interfere with the hrtick preemption
	 */
	if (!sched_feat(DOUBLE_TICK) &&
			hrtimer_active(&rq_of(cfs_rq)->hrtick_timer))
		return;
#endif

	if (cfs_rq->nr_running > 1)
		check_preempt_tick(cfs_rq, curr);
}
```

[check_preempt_tick](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L4339) 就是检查是否是时候被抢占了.

```c
/*
 * Preempt the current task with a newly woken task if needed:
 */
static void
check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
{
	unsigned long ideal_runtime, delta_exec;
	struct sched_entity *se;
	s64 delta;

	ideal_runtime = sched_slice(cfs_rq, curr);
	delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
	if (delta_exec > ideal_runtime) {
		resched_curr(rq_of(cfs_rq));
		/*
		 * The current task ran long enough, ensure it doesn't get
		 * re-elected due to buddy favours.
		 */
		clear_buddies(cfs_rq, curr);
		return;
	}

	/*
	 * Ensure that a task that missed wakeup preemption by a
	 * narrow margin doesn't have to wait for a full slice.
	 * This also mitigates buddy induced latencies under load.
	 */
	if (delta_exec < sysctl_sched_min_granularity)
		return;

	se = __pick_first_entity(cfs_rq);
	delta = curr->vruntime - se->vruntime;

	if (delta < 0)
		return;

	if (delta > ideal_runtime)
		resched_curr(rq_of(cfs_rq));
}
```

check_preempt_tick 先是调用 sched_slice 函数计算出的 ideal_runtime. ideal_runtime 是一个调度周期中，该进程运行的实际时间.

sum_exec_runtime 指进程总共执行的实际时间，prev_sum_exec_runtime 指上次该进程被调度时已经占用的实际时间. 每次在调度一个新的进程时都会把它的 se->prev_sum_exec_runtime = se->sum_exec_runtime，所以 sum_exec_runtime-prev_sum_exec_runtime 就是这次调度占用实际时间. 如果这个时间大于 ideal_runtime，则应该被抢占了. 除了这个条件之外，还会通过 __pick_first_entity 取出红黑树中最小的进程. 如果当前进程的 vruntime 大于红黑树中最小的进程的 vruntime，且差值大于 ideal_runtime，也应该被抢占了.

当发现当前进程应该被抢占，不能直接把它踢下来，而是把它标记为应该被抢占. 这是因为进程调度第一定律，一定要等待正在运行的进程调用`__schedule()` 才行啊，所以这里只能先标记一下. 标记一个进程应该被抢占，都是调用 [resched_curr](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L510)，它会调用 [set_tsk_need_resched](https://elixir.bootlin.com/linux/v5.8-rc3/source/include/linux/sched.h#L1806)，标记进程应该被抢占，但是此时此刻，并不真的抢占，而是打上一个标签 TIF_NEED_RESCHED.

**另外一个可能抢占的场景是当一个进程被唤醒的时候.** 当一个进程在等待一个 I/O 的时候，会主动放弃 CPU; 但是当 I/O 到来的时候，进程往往会被唤醒, 这个时候也是一个时机. 当被唤醒的进程优先级高于 CPU 上的当前进程，就会触发抢占. [try_to_wake_up()](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L2519) 调用 [ttwu_queue](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L2400) 将这个唤醒的任务添加到队列当中. ttwu_queue 再调用 [ttwu_do_activate](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L2232) 激活这个任务. ttwu_do_activate 调用 [ttwu_do_wakeup](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L2199). 这里面调用了 check_preempt_curr 检查是否应该发生抢占. 如果应该发生抢占，也不是直接踢走当前进程，而是将当前进程标记为应该被抢占.

具体抢占时机:
- 用户态

	- 64 位的系统调用的链路 do_syscall_64->syscall_return_slowpath->prepare_exit_to_usermode->[exit_to_usermode_loop](https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/entry/common.c#L188)的exit_to_usermode_loop就调用了`schedule()`
	- 对于用户态的进程来讲，从中断中返回的那个时刻，也是一个被抢占的时机.??? v5.8没有`interrupt_entry+prepare_exit_to_usermode`.

		> 在 [arch/x86/entry/entry_64.S](https://elixir.bootlin.com/linux/v5.7.6/source/arch/x86/entry/entry_64.S#L620) 中有中断的处理过程prepare_exit_to_usermode，最终也会调用 exit_to_usermode_loop. 但`SYM_CODE_START(interrupt_entry) with prepare_exit_to_usermode` deleted in v5.7.6~v5.8-rc1 on 75da04f7f3cb416a68475e040175dc013da32de2 for "x86/entry: Remove the apic/BUILD interrupt leftovers".
- 内核态

	对内核态的执行中，被抢占的时机一般发生在 [preempt_enable()](https://elixir.bootlin.com/linux/v5.8-rc3/source/include/linux/preempt.h#L186) 中.

	在内核态的执行中，有的操作是不能被中断的，所以在进行这些操作之前，总是先调用 [preempt_disable()](https://elixir.bootlin.com/linux/v5.8-rc3/source/include/linux/preempt.h#L169)关闭抢占，当再次打开的时候，就是一次内核态代码被抢占的机会. 就像下面代码中展示的一样，preempt_enable() 会调用 [preemptible()](https://elixir.bootlin.com/linux/v5.8-rc3/source/include/linux/preempt.h#L183)，判断是否可以被抢占. 如果可以，就调用 [preempt_schedule](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L4350)->[preempt_schedule_common](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L4316)->__schedule 进行调度.

	```c
	// https://elixir.bootlin.com/linux/v5.8-rc3/source/include/linux/preempt.h#L186
	#define preempt_enable() \
	do { \
		barrier(); \
		if (unlikely(preempt_count_dec_and_test())) \
			__preempt_schedule(); \
	} while (0)

	// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/include/asm/preempt.h#L107
	#ifdef CONFIG_PREEMPTION
	  extern asmlinkage void preempt_schedule_thunk(void);
	# define __preempt_schedule() \
		asm volatile ("call preempt_schedule_thunk" : ASM_CALL_CONSTRAINT)


	// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/entry/thunk_64.S#L40
	#ifdef CONFIG_PREEMPTION
	THUNK preempt_schedule_thunk, preempt_schedule
	```

	在内核态也会遇到中断的情况，当中断返回的时候，返回的仍然是内核态. 这个时候也是一个执行抢占的时机, 会调用的是 [preempt_schedule_irq](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L4424), 其中包含`__schedule(true)`.

#### 完全公平调度(CFS)
cfs实现的四部分:
- 时间记账
- 进程选择
- 调度器入口
- 睡眠和唤醒

在Linux里面,实现了一个基于CFS的调度算法. CFS全称Completely Fair Scheduling,叫完全公平调度, 是linux 默认的普通进程调度器.

CFS的做法是：在所有可运行进程的总数上计算出一个进程应该运行的时间，nice值不再作为时间片分配的标准，而是用于处理计算获得的处理器使用权重. 是否抢占当前进程取决于新的可运行进程消耗了多少处理器使用比. 如果消耗的使用比比当前进程小, 则新进程立刻抢占, 否则继续等待.

> 但可运行进程数量趋向无穷大时, 每个处理器获取cpu使用比和时间片都趋向0, 导致不可接收的cpu切换消耗, cfs通过引入叫最小粒度(默认是1ms)的底线来解决. 因此进程非常多时cfs不是完全的公平调度.

CPU会提供一个时钟,过一段时间就触发一个时钟中断Tick. CFS会为每一个进程安排一个虚拟运行时间vruntime, 如果一个进程在运行,随着时间的增长,也就是一个个tick的到来,进程的vruntime将不断增大;没有得到执行的进程vruntime不变.

显然,那些vruntime少的,原来受到了不公平的对待,需要给它补上,所以会优先运行这样的进程. 

如何结合优先级? 将实际运行时间delta_exec按照一定算法转化为虚拟运行时间vruntime即可:
```txt
虚拟运行时间vruntime += 实际运行时间delta_exec * NICE_0_LOAD/权重
```

在更新进程运行的统计量的时候，就可以看出这个逻辑:
```c
// cfs算法在`kernel/sched/fair.c`

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L844
/*
 * Update the current task's runtime statistics.
 */
static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr;
	u64 now = rq_clock_task(rq_of(cfs_rq));
	u64 delta_exec;

	if (unlikely(!curr))
		return;

	delta_exec = now - curr->exec_start; // 当前的时间，以及这次的时间片开始的时间，两者相减就是这次运行的时间 delta_exec(实际运行时间)
	if (unlikely((s64)delta_exec <= 0))
		return;

	curr->exec_start = now;

	schedstat_set(curr->statistics.exec_max,
		      max(delta_exec, curr->statistics.exec_max));

	curr->sum_exec_runtime += delta_exec;
	schedstat_add(cfs_rq->exec_clock, delta_exec);

	curr->vruntime += calc_delta_fair(delta_exec, curr);
	update_min_vruntime(cfs_rq);

	if (entity_is_task(curr)) {
		struct task_struct *curtask = task_of(curr);

		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
		cgroup_account_cputime(curtask, delta_exec);
		account_group_exec_runtime(curtask, delta_exec);
	}

	account_cfs_rq_runtime(cfs_rq, delta_exec);
}

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L673
/*
 * delta /= w
 */
static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
{
	if (unlikely(se->load.weight != NICE_0_LOAD))
		delta = __calc_delta(delta, NICE_0_LOAD, &se->load);

	return delta;
}
```

因此cfs中nice的值不再影响调度策略, 但nice的相对值才会影响cpu时间的分配比例.

同样的实际运行时间,给高权重的算少了,低权重的算多了,但是当选取下一个运行进程的时候,还是按照最小的vruntime来的,这样高权重的获得的实际运行时间自然就多了.

因此CFS需要一个数据结构来对vruntime进行排序,找出最小的那个. 这个能够排序的数据结构不但需要查询的时候,能够快速找到最小的,更新的时候也需要能够快速的调整排序,要知道vruntime可是经常在变
的,变了再插入这个数据结构,就需要重新排序. 而能够平衡查询和更新速度的是树,在这里使用的是红黑树. 

红黑树的的节点是应该包括vruntime的sched_entity,称为调度实体. 
在task_struct中有这样的成员变量:
```c
struct sched_entity se; // 完全公平算法调度实体
struct sched_rt_entity rt; // 实时调度实体
struct sched_dl_entity dl; // Deadline调度实体
```

看来不光CFS调度策略需要有这样一个数据结构进行排序,其他的调度策略也同样有自己的数据结构进行排序,因为任何一个策略做调度的时候,都是要区分谁先运行谁后运行

进程会根据自己是实时的,还是普通的类型,通过这些成员变量,将自己挂在某一个数据结构里面,和其他的进程排序,等待被调度.

对于普通进程的调度实体定义如下,这里面包含了vruntime和权重load_weight,以及对于运行时间的统计:
```c
// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L451
struct sched_entity {
	/* For load-balancing: */
	struct load_weight		load;
	struct rb_node			run_node;
	struct list_head		group_node;
	unsigned int			on_rq;

	u64				exec_start;
	u64				sum_exec_runtime;
	u64				vruntime;
	u64				prev_sum_exec_runtime;

	u64				nr_migrations;

	struct sched_statistics		statistics;

#ifdef CONFIG_FAIR_GROUP_SCHED
	int				depth;
	struct sched_entity		*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq			*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq			*my_q;
	/* cached value of my_q->h_nr_running */
	unsigned long			runnable_weight;
#endif

#ifdef CONFIG_SMP
	/*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
	struct sched_avg		avg;
#endif
};
```

所有可运行的进程通过不断地插入操作最终都存储在以时间为顺序的红黑树中,vruntime最小的在树的左侧,vruntime最多的在树的右侧. CFS调度策略会选择红黑树最左边的叶子节点作为下一个将获得cpu的任务

CPU也是这样的,每个CPU都有自己的 struct rq 结构,其用于描述在此CPU上所运行的所有进程,其包括一个实时进程队列rt_rq和一个CFS运行队列cfs_rq,在调度时,调度器首先会先去实时进程队列找是否有实时进程需要运行,如果没有才会去CFS运行队列找是否有进行需要运行

[rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L875), [rt_rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L601)和[cfs_rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L501)的定义在[`kernel/sched/sched.h](https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/sched.h)里

cfs_rq.rb_root指向的就是红黑树的根节点,这个红黑树在CPU看起来就是一个队列,不断的取下一个应该运行的进程. cfs_rq.rb_leftmost指向的是最左面的节点

![调度相关的数据结构](/misc/img/process/sched_rq.png)
![](/misc/img/process/1771657-20200314235249852-1440735803.png)

## 亲和性
os提供了处理器亲和性（processor affinity）机制, 允许程序对任务可以使用的CPU核心进行配置.

在Linux中, 任务通过设置cpu_set_t掩码来表示可以执行任务（以线程为单位）的CPU核心集合, 掩码中的每一位都对应于一个CPU核心.

linux提供的操作cpu核心集台掩码的宏(`sched.h`).