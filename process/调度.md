# 调度
参考:
- [如何更改 Linux I/O 调度器来调整性能](https://linux.cn/article-8179-1.html)
- [Linux进程和线程的调度与优先级](https://vaqeteart.github.io/categories/study/os/linux_schedule_priority.html)
- [深入 Linux 的进程优先级](https://linux.cn/article-7325-1.html)
- [linux进程调度](https://peterpan980927.cn/2017/12/18/Linux%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/)

调度策略是在两大矛盾中寻找平衡: 进程响应速度(低延迟)和最大系统利用率(高吞吐量).

调度算法:
- 优先级(最基本)

##　IO
Linux I/O 调度器Linux I/O scheduler控制内核提交读写请求给磁盘的方式. 自从 2.6 内核以来，管理员已经能够更改这个调度器，所以他们可以自定义他们的平台以完全适合他们的需要.

这些调度器是：
1. CFQ （Completely Fair Scheduler完全公平调度器）（cfq） : 它是许多 Linux 发行版的默认调度器；它为每个进程/线程单独创建一个队列来管理请求，然后各队列之间使用时间片来调度.
1. Noop 调度器（noop） ： 基于先入先出（FIFO）队列概念的 Linux 内核里最简单的 I/O 调度器. 此调度程序**最适合于 SSD**
1. 截止时间调度器（deadline） ： 尝试保证请求的开始服务时间

> 具体调度可参考*Linux性能优化大师*的`1.4.3`块层.

## 进程
进程可以分为CPU密集型和I/O密集型两种. I/O密集型的进程大部分时间都在用来提交I/O请求和等待I/O请求; CPU密集型进程大部分时间都花在执行代码上.

调度策略主要在两个矛盾之间寻找平衡：进程响应时间短和最大的系统利用率.

> 为了保证交互式应用和桌面系统的性能，一般Linux更倾向于优先调度I/O消耗型进程

### 进程优先级
在 Linux 里面，进程大概可以分成两种
1. 实时进程 : 静态(实时)优先级, 支持 posix.1b标准. 快速记忆: 权力大而快.

	就是需要尽快执行返回结果的进程.

	优先级变化范围：0~MAX_RT_PRIO-1间, 默认MAX_RT_PRIO配置为100. **值越大，优先级越高**. Linux下查看实时优先级命令： `ps -eo state,uid,pid,rtprio`的`RTPRIO`列, `-`表示非实时进程.

1. 普通进程 : 动态(非实时,nice值). 快速记忆: 负重运行慢.

	大部分的进程其实都是这种

	nice值得优先级范围是从-20到+19，默认是0,**nice值越大，优先级越小**. Linux系统的nice值代表的是时间片的比例，因此越小的nice值，占有CPU的时间也就会越长. `ps -el`的`NI`列.

因为**任何实时进程(需要尽快返回结果)的优先级都高于普通进程**. 因此实时优先级和nice优先级处于两个不相交的范畴, 具体映射关系如下:

![linux进程优先级映射关系](/misc/img/process/linux_process_priority.jpg)

> 默认情况下，进程的nice是从父进程继承来的.
> `ps -el`的`NI`列包含 `-` 这个符号，该符号的含义是指：该进程是实时进程.Linux中可以通过renice命令调整进程的优先级
> `ps -eo state,uid,pid,rtprio`的`RTPRIO`列包含`-` 这个符号，该符号的含义是指：该进程不是实时进程

### 调度类型
> 可通过`chrt`命令查看及调整.

> 时间片是分配给每个可运行进程的处理器可用时间段. linux的完全公平调度(cfs)没有使用时间片.

> 非抢占模式下, 除非进程自己主动停止, 否则会一直执行.

每个Linux进程都按照以下调度类型被调度:
- 实时调度策略

	- SCHED_FIFO
	  
	  先进先出的**实时**进程, 不使用时间片. 高优先级的进程可以抢占低优先级的进程,而相同优先级的进程遵循先来先得,此时除非主动让出否则想运行多久便运行多久
	
	- SCHED_RR
	  
	  时间片轮转的**实时**进程, 使用时间片. 保证对所有相同优先级的实时进程公平地分配CPU时间即只有当它的时间片用完，内核会把它放到进程队列的末尾; 而高优先级的任务也是可以抢占低优先级的任务
	
	- SCHED_DEADLINE
	  
	  **实时**进程, 按照任务的deadline进行调度的. 当产生一个调度点的时候,DL调度器总是选择其deadline距离当前时间点最近的那个任务并调度它执行

- 普通调度策略

	- SCHED_NORMAL : 可用nice或renice调整, 范围是`19(最低) ~ -20(最高)`, 默认为0.
	  
	  普通的分时进程
	- SCHED_BATCH

	  后台进程,几乎不需要和前端进行交互, 不影响交互时可以降低它的优先级
	- SCHED_IDLE

	  特别空闲的时候才跑的进程

SCHED_NORMAL是普通进程调度策略,其他两者都是实时进程调度策略. 它们的主要区别就是通过优先级来区分的: 所有优先级值在0-99范围内的，都是实时进程，所以这个优先级范围也可以叫做实时进程优先级，而100-139范围内的是非实时进程.

系统的整体优先级策略是：如果系统中存在需要执行的实时进程，则优先执行实时进程. 直到实时进程退出或者主动让出CPU时，才会调度执行非实时进程. 即 **任何时候，实时进程的优先级都高于普通进程，实时进程只会被更高级的实时进程抢占，同级实时进程之间是按照FIFO（一次机会做完）或者RR（多次轮转）规则调度的**.

SCHED_FIFO 与 SCHED_RR 的区别是:当进程的调度策略为前者时,当前实时进程将一直占用 CPU 直至自动退出, **除非有更紧迫的、优先级更高的实时进程**需要运行时,它才会被抢占 CPU;当进程的调度策略为后者时,它与其它实时进程以实时轮流算法去共同使用 CPU，用完时间片放到运行队列尾部.

![Linux 进程优先级与 nice 值及实时进程优先级的关系](/misc/img/process/v2-488659493625064c6227293720c117c9_hd.jpg)

> priority就是ps命令中看到的PRI值或者top命令中看到的PR值, 越小说明优先级越高.
> nice值虽然不是priority，但是它确实可以影响进程的优先级
> 实时进程, 只有静态优先级;普通进程根据动态优先级进行调度
> 动态优先级(`dynamic_prio=max(100, min(static_prio-bonus+5, 139))`)是由静态优先级（`static_prio=MAX_RT_PRIO +nice+ 20`, 静态优先级范围在100~139之间）调整而来.
> 在系统中可以使用chrt命令来查看、设置一个进程的实时优先级状态
> 进程的平均睡眠时间也即bonus

### linux task_struct 调度
参考:
- [深入解读Linux进程调度系列（总览）](https://blog.csdn.net/Vince_/article/details/89054330)
- [Linux进程调度-CFS调度器](https://www.cnblogs.com/LoyenWang/p/12495319.html)

linux调度器是以模块的形式提供的, 该模块结构被称为调度器类(scheduler classes), 允许多种不同的可动态添加的调度算法并存, 调度属于自己范畴的进程, 每个调度器都有自己的优先级. 基础的调度器代码在`kernel/sched/core.c`, 它会按照优先级顺序遍历调度类, 拥有一个可执行进程的最高优先级的调度类胜出, 再去选择要执行哪个程序.

在task_struct中,有一个成员变量`unsigned int policy`保存了调度策略, 它有以下定义:
```c
// https://elixir.bootlin.com/linux/latest/source/include/uapi/linux/sched.h#L114
/*
 * Scheduling policies
 */
#define SCHED_NORMAL		0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE		6
```

配合调度策略的还有优先级 ,也在task_struct中
```c
// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L678
	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;
```

优先级其实就是一个数值,对于实时进程,优先级的范围是0~99;对于普通进程,优先级的范围是100~139. 数值越小,优先级越高. 从这里可以看出,所有的实时进程都比普通进程优先级要高.

#### 调度类
从Linux 2.6.23开始，Linux引入scheduling class的概念，目的是将调度器模块化, 这提高了可扩展性，添加一个新的调度器也变得简单起来.

```c
// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L683
// task_struct的调度类执行其调度策略的逻辑
const struct sched_class	*sched_class;

// https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L1747
struct sched_class {
	const struct sched_class *next; // 链表

#ifdef CONFIG_UCLAMP_TASK
	int uclamp_enabled;
#endif

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*yield_task)   (struct rq *rq);
	bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt);

	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);

	struct task_struct *(*pick_next_task)(struct rq *rq);

	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);

#ifdef CONFIG_SMP
	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);

	void (*task_woken)(struct rq *this_rq, struct task_struct *task);

	void (*set_cpus_allowed)(struct task_struct *p,
				 const struct cpumask *newmask);

	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);
#endif

	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
	void (*task_fork)(struct task_struct *p);
	void (*task_dead)(struct task_struct *p);

	/*
	 * The switched_from() call is allowed to drop rq->lock, therefore we
	 * cannot assume the switched_from/switched_to pair is serliazed by
	 * rq->lock. They are however serialized by p->pi_lock.
	 */
	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
			      int oldprio);

	unsigned int (*get_rr_interval)(struct rq *rq,
					struct task_struct *task);

	void (*update_curr)(struct rq *rq);

#define TASK_SET_GROUP		0
#define TASK_MOVE_GROUP		1

#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_change_group)(struct task_struct *p, int type);
#endif
};
```

这个结构定义了很多种方法,用于在队列上操作task, 其第一个成员变量，是一个指针，指向下一个调度类.

看部分sched_class定义的与调度有关的函数:
- enqueue_task向就绪队列中添加一个进程,当某个进程进入可运行状态时,调用这个函数
- dequeue_task 将一个进程从就就绪队列中删除
- pick_next_task 选择接下来要运行的进程
- put_prev_task 用另一个进程代替当前运行的进程
- set_curr_task 用于修改调度策略
- task_tick 每次周期性时钟到的时候,这个函数被调用,可能触发调度

sched_class有几种实现:
- stop_sched_class : 优先级最高的任务会使用这种策略,会中断所有其他线程,且不会被其他任务打断
- dl_sched_class : 对应上面的deadline调度策略
- rt_sched_class : 对应RR算法或者FIFO算法的调度策略,具体调度策略由进程的task_struct->policy指定;
- fair_sched_class : 普通进程的调度策略 // 对于普通进程来讲,公平是最重要的
- idle_sched_class : 空闲进程的调度策略

它们其实是放在一个链表上的,即有顺序关系.

![](/misc/img/process/1771657-20200314235232449-1386087933.png)

在调度核心代码kernel/sched/core.c中，使用的方式是task->sched_class->xxx_func，task_struck结构体中包含了任务所使用的调度器，进而能找到对应的函数指针来完成调用执行.

```c
// https://elixir.bootlin.com/linux/latest/source/kernel/sched/core.c#L3905
/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those loose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely((prev->sched_class == &idle_sched_class ||
		    prev->sched_class == &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = pick_next_task_fair(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto restart;

		/* Assumes fair_sched_class->next == idle_sched_class */
		if (!p) {
			put_prev_task(rq, prev);
			p = pick_next_task_idle(rq);
		}

		return p;
	}

restart:
#ifdef CONFIG_SMP
	/*
	 * We must do the balancing pass before put_next_task(), such
	 * that when we release the rq->lock the task is in the same
	 * state as before we took rq->lock.
	 *
	 * We can terminate the balance pass as soon as we know there is
	 * a runnable task of @class priority or higher.
	 */
	for_class_range(class, prev->sched_class, &idle_sched_class) {
		if (class->balance(rq, prev, rf))
			break;
	}
#endif

	put_prev_task(rq, prev);

	for_each_class(class) {
		p = class->pick_next_task(rq);
		if (p)
			return p;
	}

	/* The idle class should always have a runnable task: */
	BUG();
}

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L6961
struct task_struct *
pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	struct cfs_rq *cfs_rq = &rq->cfs;
	struct sched_entity *se;
	struct task_struct *p;
	int new_tasks;

again:
	if (!sched_fair_runnable(rq))
		goto idle;

#ifdef CONFIG_FAIR_GROUP_SCHED
	if (!prev || prev->sched_class != &fair_sched_class)
		goto simple;

	/*
	 * Because of the set_next_buddy() in dequeue_task_fair() it is rather
	 * likely that a next task is from the same cgroup as the current.
	 *
	 * Therefore attempt to avoid putting and setting the entire cgroup
	 * hierarchy, only change the part that actually changes.
	 */

	do {
		struct sched_entity *curr = cfs_rq->curr; // 取出当前正在运行的任务 curr

		/*
		 * Since we got here without doing put_prev_entity() we also
		 * have to consider cfs_rq->curr. If it is still a runnable
		 * entity, update_curr() will update its vruntime, otherwise
		 * forget we've ever seen it.
		 */
		if (curr) {
			if (curr->on_rq) // 如果依然是可运行的状态，也即处于进程就绪状态，则调用 update_curr 更新 vruntime
				update_curr(cfs_rq);
			else
				curr = NULL;

			/*
			 * This call to check_cfs_rq_runtime() will do the
			 * throttle and dequeue its entity in the parent(s).
			 * Therefore the nr_running test will indeed
			 * be correct.
			 */
			if (unlikely(check_cfs_rq_runtime(cfs_rq))) {
				cfs_rq = &rq->cfs;

				if (!cfs_rq->nr_running)
					goto idle;

				goto simple;
			}
		}

		se = pick_next_entity(cfs_rq, curr); // pick_next_entity 从红黑树里面，取最左边的一个节点
		cfs_rq = group_cfs_rq(se);
	} while (cfs_rq);

	p = task_of(se); // task_of 得到下一个调度实体对应的 task_struct

	/*
	 * Since we haven't yet done put_prev_entity and if the selected task
	 * is a different task than we started out with, try and touch the
	 * least amount of cfs_rqs.
	 */
	if (prev != p) { // 如果继任和前任不一样，这就说明有一个更需要运行的进程了，此时就需要更新红黑树了. 前面前任的 vruntime 更新过了，put_prev_entity 放回红黑树，会找到相应的位置，然后 set_next_entity 将继任者设为当前任务
		struct sched_entity *pse = &prev->se;

		while (!(cfs_rq = is_same_group(se, pse))) {
			int se_depth = se->depth;
			int pse_depth = pse->depth;

			if (se_depth <= pse_depth) {
				put_prev_entity(cfs_rq_of(pse), pse);
				pse = parent_entity(pse);
			}
			if (se_depth >= pse_depth) {
				set_next_entity(cfs_rq_of(se), se);
				se = parent_entity(se);
			}
		}

		put_prev_entity(cfs_rq, pse);
		set_next_entity(cfs_rq, se);
	}

	goto done;
simple:
#endif
	if (prev)
		put_prev_task(rq, prev);

	do {
		se = pick_next_entity(cfs_rq, NULL);
		set_next_entity(cfs_rq, se);
		cfs_rq = group_cfs_rq(se);
	} while (cfs_rq);

	p = task_of(se);

done: __maybe_unused;
#ifdef CONFIG_SMP
	/*
	 * Move the next running task to the front of
	 * the list, so our cfs_tasks list becomes MRU
	 * one.
	 */
	list_move(&p->se.group_node, &rq->cfs_tasks);
#endif

	if (hrtick_enabled(rq))
		hrtick_start_fair(rq, p);

	update_misfit_status(p, rq);

	return p;

idle:
	if (!rf)
		return NULL;

	new_tasks = newidle_balance(rq, rf);

	/*
	 * Because newidle_balance() releases (and re-acquires) rq->lock, it is
	 * possible for any higher priority task to appear. In that case we
	 * must re-start the pick_next_entity() loop.
	 */
	if (new_tasks < 0)
		return RETRY_TASK;

	if (new_tasks > 0)
		goto again;

	/*
	 * rq is about to be idle, check if we need to update the
	 * lost_idle_time of clock_pelt
	 */
	update_idle_rq_clock_pelt(rq);

	return NULL;
}
```

调度的时候是从优先级最高的调度类到优先级低的调度类,依次执行. 而对于每种调度类,有自己的实现CFS就是[fair_sched_class](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L11110).

对于同样的pick_next_task选取下一个要运行的任务这个动作,不同的调度类有自己的实现. fair_sched_class的实现是[pick_next_task_fair](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L6961),rt_sched_class的实现是pick_next_task_rt. 我们会发现这两个函数是操作不同的队列,[pick_next_task_rt](https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/rt.c#L1627)操作的是rt_rq,pick_next_task_fair操作的是cfs_rq.

这样整个运行的场景就串起来了,在每个CPU上都有一个队列rq,这个队列里面包含多个子队列,例如rt_rq和cfs_rq,不同的队列有不同的实现方式,cfs_rq就是用红黑树实现的. 

当某个CPU需要找下一个任务执行的时候,会按照优先级依次调用调度类,不同的调度类操作不同的队列: rt_sched_class先被调用,它会在rt_rq上找下一个任务,只有找不到的时候,才轮到fair_sched_class被调用,它会在cfs_rq上找下一个任务. 这样保证了实时任务的优先级永远大于普通任务. 

![sched全局](/misc/img/process/sched_task)

#### 主动调度
![主动让出调度](/misc/img/process/schedule.png)

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3499
asmlinkage __visible void __sched schedule(void)
{
	struct task_struct *tsk = current;

	sched_submit_work(tsk);
	do {
		preempt_disable();
		__schedule(false);
		sched_preempt_enable_no_resched();
	} while (need_resched());
	sched_update_worker(tsk);
}
EXPORT_SYMBOL(schedule);
```

这段代码的主要逻辑是在`__schedule`函数中实现的:
```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3499
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPTION=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu);
	prev = rq->curr;

	schedule_debug(prev, preempt);

	if (sched_feat(HRTICK))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;
	if (!preempt && prev->state) {
		if (signal_pending_state(prev->state, prev)) {
			prev->state = TASK_RUNNING;
		} else {
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf); // 获取下一个任务，task_struct *next 指向下一个任务
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		/*
		 * RCU users of rcu_dereference(rq->curr) may not see
		 * changes to task_struct made by pick_next_task().
		 */
		RCU_INIT_POINTER(rq->curr, next);
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		psi_sched_switch(prev, next, !task_on_rq_queued(prev));

		trace_sched_switch(preempt, prev, next);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf); // 当选出的继任者和前任不同，就要进行上下文切换，继任者进程正式进入运行
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
```

首先，. task_struct *prev(`prev = rq->curr`) 指向这个 CPU 的任务队列上面正在运行的那个进程 curr, 因为一旦将来它被切换下来，那它就成了前任了.

#### 进程上下文切换
上下文切换主要干两件事情,一是切换进程空间,也即虚拟内存;二是切换寄存器和CPU上下文
```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/core.c#L3402
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
	       struct task_struct *next, struct rq_flags *rf)
{
	prepare_task_switch(rq, prev, next);

	/*
	 * For paravirt, this is coupled with an exit in switch_to to
	 * combine the page table reload and the switch backend into
	 * one hypercall.
	 */
	arch_start_context_switch(prev);

	/*
	 * kernel -> kernel   lazy + transfer active
	 *   user -> kernel   lazy + mmgrab() active
	 *
	 * kernel ->   user   switch + mmdrop() active
	 *   user ->   user   switch
	 */
	if (!next->mm) {                                // to kernel
		enter_lazy_tlb(prev->active_mm, next);

		next->active_mm = prev->active_mm;
		if (prev->mm)                           // from user
			mmgrab(prev->active_mm);
		else
			prev->active_mm = NULL;
	} else {                                        // to user
		membarrier_switch_mm(rq, prev->active_mm, next->mm);
		/*
		 * sys_membarrier() requires an smp_mb() between setting
		 * rq->curr / membarrier_switch_mm() and returning to userspace.
		 *
		 * The below provides this either through switch_mm(), or in
		 * case 'prev->active_mm == next->mm' through
		 * finish_task_switch()'s mmdrop().
		 */
		switch_mm_irqs_off(prev->active_mm, next->mm, next);

		if (!prev->mm) {                        // from kernel
			/* will mmdrop() in finish_task_switch(). */
			rq->prev_mm = prev->active_mm;
			prev->active_mm = NULL;
		}
	}

	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	prepare_lock_switch(rq, next, rf);

	/* Here we just switch the register state and the stack. */
	switch_to(prev, next, prev);
	barrier(); // barrier语句是一个编译器指令,用于保证switch_to和finish_task_switch的执行顺序,不会因为编译阶段优化而改变, 忽略即可

	return finish_task_switch(prev);
}
```

首先是内存空间的切换. 接下来，就是switch_to. 它就是寄存器和栈的切换，它调用到了 __switch_to_asm. 这是一段汇编代码，主要用于栈的切换.

当进程 A 在内核里面执行 switch_to 的时候，内核态的指令指针也是指向这一行的. 但是在 switch_to 里面，将寄存器和栈都切换到成了进程 B 的，唯一没有变的就是指令指针寄存器. 当 switch_to 返回的时候，指令指针寄存器指向了下一条语句 finish_task_switch. 但这个时候的 finish_task_switch 已经不是进程 A 的 finish_task_switch 了，而是进程 B 的 finish_task_switch 了.

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/include/asm/switch_to.h#L47
#define switch_to(prev, next, last)					\
do {									\
	((last) = __switch_to_asm((prev), (next)));			\
} while (0)
```

switch_to三个参数的理解:
- prev和next很好理解: prev指向当前进程，next指向被调度的进程
- last

	用于保存由谁触发切换回prev.

	比如切换流程:`A->B->C->A`, 当切换回A时, last就是C. 此时`finish_task_switch(prev)`就是`finish_task_switch(C)`

```x86asm
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/entry/entry_64.S#L223
// 这里是64bit的, 32bit的在arch/x86/entry/entry_32.S
/*
 * %rdi: prev task
 * %rsi: next task
 */
.pushsection .text, "ax"
SYM_FUNC_START(__switch_to_asm)
	/*
	 * Save callee-saved registers
	 * This must match the order in inactive_task_frame
	 */
	pushq	%rbp
	pushq	%rbx
	pushq	%r12
	pushq	%r13
	pushq	%r14
	pushq	%r15

	/* switch stack */
	movq	%rsp, TASK_threadsp(%rdi)
	movq	TASK_threadsp(%rsi), %rsp

#ifdef CONFIG_STACKPROTECTOR
	movq	TASK_stack_canary(%rsi), %rbx
	movq	%rbx, PER_CPU_VAR(fixed_percpu_data) + stack_canary_offset
#endif

#ifdef CONFIG_RETPOLINE
	/*
	 * When switching from a shallower to a deeper call stack
	 * the RSB may either underflow or use entries populated
	 * with userspace addresses. On CPUs where those concerns
	 * exist, overwrite the RSB with entries which capture
	 * speculative execution to prevent attack.
	 */
	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
#endif

	/* restore callee-saved registers */
	popq	%r15
	popq	%r14
	popq	%r13
	popq	%r12
	popq	%rbx
	popq	%rbp

	jmp	__switch_to
SYM_FUNC_END(__switch_to_asm)
.popsection
```

最终，到了 __switch_to 函数.

```c
// https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/kernel/process_64.c#L425
// 这里是64bit的, 32bit的在arch/x86/kernel/process_64.c
/*
 *	switch_to(x,y) should switch tasks from x to y.
 *
 * This could still be optimized:
 * - fold all the options into a flag word and test it with a single test.
 * - could test fs/gs bitsliced
 *
 * Kprobes not supported here. Set the probe on schedule instead.
 * Function graph tracer not supported too.
 */
__visible __notrace_funcgraph struct task_struct *
__switch_to(struct task_struct *prev_p, struct task_struct *next_p)
{
	struct thread_struct *prev = &prev_p->thread;
	struct thread_struct *next = &next_p->thread;
	struct fpu *prev_fpu = &prev->fpu;
	struct fpu *next_fpu = &next->fpu;
	int cpu = smp_processor_id();

	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
		     this_cpu_read(irq_count) != -1);

	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
		switch_fpu_prepare(prev_fpu, cpu);

	/* We must save %fs and %gs before load_TLS() because
	 * %fs and %gs may be cleared by load_TLS().
	 *
	 * (e.g. xen_load_tls())
	 */
	save_fsgs(prev_p);

	/*
	 * Load TLS before restoring any segments so that segment loads
	 * reference the correct GDT entries.
	 */
	load_TLS(next, cpu);

	/*
	 * Leave lazy mode, flushing any hypercalls made here.  This
	 * must be done after loading TLS entries in the GDT but before
	 * loading segments that might reference them.
	 */
	arch_end_context_switch(next_p);

	/* Switch DS and ES.
	 *
	 * Reading them only returns the selectors, but writing them (if
	 * nonzero) loads the full descriptor from the GDT or LDT.  The
	 * LDT for next is loaded in switch_mm, and the GDT is loaded
	 * above.
	 *
	 * We therefore need to write new values to the segment
	 * registers on every context switch unless both the new and old
	 * values are zero.
	 *
	 * Note that we don't need to do anything for CS and SS, as
	 * those are saved and restored as part of pt_regs.
	 */
	savesegment(es, prev->es);
	if (unlikely(next->es | prev->es))
		loadsegment(es, next->es);

	savesegment(ds, prev->ds);
	if (unlikely(next->ds | prev->ds))
		loadsegment(ds, next->ds);

	x86_fsgsbase_load(prev, next);

	/*
	 * Switch the PDA and FPU contexts.
	 */
	this_cpu_write(current_task, next_p);
	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));

	switch_fpu_finish(next_fpu);

	/* Reload sp0. */
	update_task_stack(next_p);

	switch_to_extra(prev_p, next_p);

	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
		/*
		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
		 * does not update the cached descriptor.  As a result, if we
		 * do SYSRET while SS is NULL, we'll end up in user mode with
		 * SS apparently equal to __USER_DS but actually unusable.
		 *
		 * The straightforward workaround would be to fix it up just
		 * before SYSRET, but that would slow down the system call
		 * fast paths.  Instead, we ensure that SS is never NULL in
		 * system call context.  We do this by replacing NULL SS
		 * selectors at every context switch.  SYSCALL sets up a valid
		 * SS, so the only way to get NULL is to re-enter the kernel
		 * from CPL 3 through an interrupt.  Since that can't happen
		 * in the same task as a running syscall, we are guaranteed to
		 * context switch between every interrupt vector entry and a
		 * subsequent SYSRET.
		 *
		 * We read SS first because SS reads are much faster than
		 * writes.  Out of caution, we force SS to __KERNEL_DS even if
		 * it previously had a different non-NULL value.
		 */
		unsigned short ss_sel;
		savesegment(ss, ss_sel);
		if (ss_sel != __KERNEL_DS)
			loadsegment(ss, __KERNEL_DS);
	}

	/* Load the Intel cache allocation PQR MSR. */
	resctrl_sched_in();

	return prev_p;
}
```

没有了`[struct tss_struct](https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/include/asm/processor.h#L411) *tss = &per_cpu(cpu_tss_rw, cpu)`, deleted in v4.19.6~v4.19.7 on ff16701a29cba3aafa0bd1656d766813b2d0a811 for " x86/process: Consolidate and simplify switch_to_xtra() code". 从该commit msg上看, 相应代码被移入了[switch_to_bitmap()](https://elixir.bootlin.com/linux/v5.8-rc3/source/arch/x86/kernel/process.c#L339).

在 x86 体系结构中，提供了一种以硬件的方式进行进程切换的模式，对于每个进程，x86 希望在内存里面维护一个 TSS（Task State Segment，任务状态段）结构. 这里面有所有的寄存器信息. 另外，还有一个特殊的寄存器 TR（Task Register，任务寄存器），指向某个进程的 TSS. 更改 TR 的值，将会触发硬件保存 CPU 所有寄存器的值到当前进程的 TSS 中，然后从新进程的 TSS 中读出所有寄存器值，加载到 CPU 对应的寄存器中.

但是这样有个缺点, 我们做进程切换的时候，没必要每个寄存器都切换，这样每个进程一个 TSS，就需要全量保存，全量切换，动作太大了. 于是，kernel会给每一个 CPU 关联一个 TSS(by cpu_init)，然后将 TR 指向这个 TSS，然后在操作系统的运行过程中，TR 就不切换了，永远指向这个 TSS. TSS 用数据结构 tss_struct 表示.

在 Linux 中，真的参与进程切换的寄存器很少，主要的就是栈顶寄存器. 于是，在 task_struct的成员变量 thread保留了要切换进程时需要修改的寄存器.

#### 指令指针的保存与恢复
task切换:
- 用户栈

	从进程 A 切换到进程 B，在切换内存空间的时候就已经切换用户栈了. 因为每个进程的用户栈都是独立的，都在内存空间里面.

- 内核栈

	在 __switch_to 里面切换，也就是将 current_task 指向当前的 task_struct, 里面的 void *stack 指针，指向的就是当前的内核栈

- 内核栈的栈顶指针

	在 __switch_to_asm 里面已经切换了栈顶指针

- 用户栈的栈顶指针

	如果当前在内核里面的话，它当然是在内核栈顶部的 pt_regs 结构里面呀. 当从内核返回用户态运行的时候，pt_regs 里面有所有当时在用户态的时候运行的上下文信息，就可以开始运行了.


**进程的调度都最终会调用到 __schedule 函数**.

#### 完全公平调度(CFS)
cfs实现的四部分:
- 时间记账
- 进程选择
- 调度器入口
- 睡眠和唤醒

在Linux里面,实现了一个基于CFS的调度算法. CFS全称Completely Fair Scheduling,叫完全公平调度, 是linux 默认的普通进程调度器.

CFS的做法是：在所有可运行进程的总数上计算出一个进程应该运行的时间，nice值不再作为时间片分配的标准，而是用于处理计算获得的处理器使用权重. 是否抢占当前进程取决于新的可运行进程消耗了多少处理器使用比. 如果消耗的使用比比当前进程小, 则新进程立刻抢占, 否则继续等待.

> 但可运行进程数量趋向无穷大时, 每个处理器获取cpu使用比和时间片都趋向0, 导致不可接收的cpu切换消耗, cfs通过引入叫最小粒度(默认是1ms)的底线来解决. 因此进程非常多时cfs不是完全的公平调度.

CPU会提供一个时钟,过一段时间就触发一个时钟中断Tick. CFS会为每一个进程安排一个虚拟运行时间vruntime, 如果一个进程在运行,随着时间的增长,也就是一个个tick的到来,进程的vruntime将不断增大;没有得到执行的进程vruntime不变.

显然,那些vruntime少的,原来受到了不公平的对待,需要给它补上,所以会优先运行这样的进程. 

如何结合优先级? 将实际运行时间delta_exec按照一定算法转化为虚拟运行时间vruntime即可:
```txt
虚拟运行时间vruntime += 实际运行时间delta_exec * NICE_0_LOAD/权重
```

在更新进程运行的统计量的时候，就可以看出这个逻辑:
```c
// cfs算法在`kernel/sched/fair.c`

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L844
/*
 * Update the current task's runtime statistics.
 */
static void update_curr(struct cfs_rq *cfs_rq)
{
	struct sched_entity *curr = cfs_rq->curr;
	u64 now = rq_clock_task(rq_of(cfs_rq));
	u64 delta_exec;

	if (unlikely(!curr))
		return;

	delta_exec = now - curr->exec_start; // 当前的时间，以及这次的时间片开始的时间，两者相减就是这次运行的时间 delta_exec(实际运行时间)
	if (unlikely((s64)delta_exec <= 0))
		return;

	curr->exec_start = now;

	schedstat_set(curr->statistics.exec_max,
		      max(delta_exec, curr->statistics.exec_max));

	curr->sum_exec_runtime += delta_exec;
	schedstat_add(cfs_rq->exec_clock, delta_exec);

	curr->vruntime += calc_delta_fair(delta_exec, curr);
	update_min_vruntime(cfs_rq);

	if (entity_is_task(curr)) {
		struct task_struct *curtask = task_of(curr);

		trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
		cgroup_account_cputime(curtask, delta_exec);
		account_group_exec_runtime(curtask, delta_exec);
	}

	account_cfs_rq_runtime(cfs_rq, delta_exec);
}

// https://elixir.bootlin.com/linux/v5.8-rc3/source/kernel/sched/fair.c#L673
/*
 * delta /= w
 */
static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)
{
	if (unlikely(se->load.weight != NICE_0_LOAD))
		delta = __calc_delta(delta, NICE_0_LOAD, &se->load);

	return delta;
}
```

因此cfs中nice的值不再影响调度策略, 但nice的相对值才会影响cpu时间的分配比例.

同样的实际运行时间,给高权重的算少了,低权重的算多了,但是当选取下一个运行进程的时候,还是按照最小的vruntime来的,这样高权重的获得的实际运行时间自然就多了.

因此CFS需要一个数据结构来对vruntime进行排序,找出最小的那个. 这个能够排序的数据结构不但需要查询的时候,能够快速找到最小的,更新的时候也需要能够快速的调整排序,要知道vruntime可是经常在变
的,变了再插入这个数据结构,就需要重新排序. 而能够平衡查询和更新速度的是树,在这里使用的是红黑树. 

红黑树的的节点是应该包括vruntime的sched_entity,称为调度实体. 
在task_struct中有这样的成员变量:
```c
struct sched_entity se; // 完全公平算法调度实体
struct sched_rt_entity rt; // 实时调度实体
struct sched_dl_entity dl; // Deadline调度实体
```

看来不光CFS调度策略需要有这样一个数据结构进行排序,其他的调度策略也同样有自己的数据结构进行排序,因为任何一个策略做调度的时候,都是要区分谁先运行谁后运行

进程会根据自己是实时的,还是普通的类型,通过这些成员变量,将自己挂在某一个数据结构里面,和其他的进程排序,等待被调度.

对于普通进程的调度实体定义如下,这里面包含了vruntime和权重load_weight,以及对于运行时间的统计:
```c
// https://elixir.bootlin.com/linux/latest/source/include/linux/sched.h#L451
struct sched_entity {
	/* For load-balancing: */
	struct load_weight		load;
	struct rb_node			run_node;
	struct list_head		group_node;
	unsigned int			on_rq;

	u64				exec_start;
	u64				sum_exec_runtime;
	u64				vruntime;
	u64				prev_sum_exec_runtime;

	u64				nr_migrations;

	struct sched_statistics		statistics;

#ifdef CONFIG_FAIR_GROUP_SCHED
	int				depth;
	struct sched_entity		*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq			*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq			*my_q;
	/* cached value of my_q->h_nr_running */
	unsigned long			runnable_weight;
#endif

#ifdef CONFIG_SMP
	/*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
	struct sched_avg		avg;
#endif
};
```

所有可运行的进程通过不断地插入操作最终都存储在以时间为顺序的红黑树中,vruntime最小的在树的左侧,vruntime最多的在树的右侧. CFS调度策略会选择红黑树最左边的叶子节点作为下一个将获得cpu的任务

CPU也是这样的,每个CPU都有自己的 struct rq 结构,其用于描述在此CPU上所运行的所有进程,其包括一个实时进程队列rt_rq和一个CFS运行队列cfs_rq,在调度时,调度器首先会先去实时进程队列找是否有实时进程需要运行,如果没有才会去CFS运行队列找是否有进行需要运行

[rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L875), [rt_rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L601)和[cfs_rq](https://elixir.bootlin.com/linux/latest/source/kernel/sched/sched.h#L501)的定义在[`kernel/sched/sched.h](https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/sched.h)里

cfs_rq.rb_root指向的就是红黑树的根节点,这个红黑树在CPU看起来就是一个队列,不断的取下一个应该运行的进程. cfs_rq.rb_leftmost指向的是最左面的节点

![调度相关的数据结构](/misc/img/process/sched_rq.png)
![](/misc/img/process/1771657-20200314235249852-1440735803.png)