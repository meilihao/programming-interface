# 调度
参考:
- [如何更改 Linux I/O 调度器来调整性能](https://linux.cn/article-8179-1.html)
- [Linux进程和线程的调度与优先级](https://vaqeteart.github.io/categories/study/os/linux_schedule_priority.html)
- [深入 Linux 的进程优先级](https://linux.cn/article-7325-1.html)
- [linux进程调度](https://peterpan980927.cn/2017/12/18/Linux%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/)

调度策略是在两大矛盾中寻找平衡: 进程响应速度(低延迟)和最大系统利用率(高吞吐量).

调度算法:
- 优先级(最基本)

##　IO
Linux I/O 调度器Linux I/O scheduler控制内核提交读写请求给磁盘的方式. 自从 2.6 内核以来，管理员已经能够更改这个调度器，所以他们可以自定义他们的平台以完全适合他们的需要.

这些调度器是：
1. CFQ （Completely Fair Scheduler完全公平调度器）（cfq） : 它是许多 Linux 发行版的默认调度器；它为每个进程/线程单独创建一个队列来管理请求，然后各队列之间使用时间片来调度.
1. Noop 调度器（noop） ： 基于先入先出（FIFO）队列概念的 Linux 内核里最简单的 I/O 调度器. 此调度程序**最适合于 SSD**
1. 截止时间调度器（deadline） ： 尝试保证请求的开始服务时间

> 具体调度可参考*Linux性能优化大师*的`1.4.3`块层.

## 进程
进程可以分为CPU密集型和I/O密集型两种. I/O密集型的进程大部分时间都在用来提交I/O请求和等待I/O请求; CPU密集型进程大部分时间都花在执行代码上.

调度策略主要在两个矛盾之间寻找平衡：进程响应时间短和最大的系统利用率.

> 为了保证交互式应用和桌面系统的性能，一般Linux更倾向于优先调度I/O消耗型进程

### 进程优先级
1. 动态(非实时,nice值). 快速记忆: 负重运行慢.
nice值得优先级范围是从-20到+19，默认是0,**nice值越大，优先级越小**. Linux系统的nice值代表的是时间片的比例，因此越小的nice值，占有CPU的时间也就会越长. `ps -el`的`NI`列.
1. 静态(实时)优先级, 支持 posix.1b标准. 快速记忆: 权力大而快.
优先级变化范围：0~MAX_RT_PRIO-1间, 默认MAX_RT_PRIO配置为100. **值越大，优先级越高**. Linux下查看实时优先级命令： `ps -eo state,uid,pid,rtprio`的`RTPRIO`列, `-`表示非实时进程.

因为**任何实时进程(需要尽快返回结果)的优先级都高于普通进程**. 因此实时优先级和nice优先级处于两个不相交的范畴, 具体映射关系如下:

![linux进程优先级映射关系](/misc/img/process/linux_process_priority.jpg)

> 默认情况下，进程的nice是从父进程继承来的.
> `ps -el`的`NI`列包含 `-` 这个符号，该符号的含义是指：该进程是实时进程.Linux中可以通过renice命令调整进程的优先级
> `ps -eo state,uid,pid,rtprio`的`RTPRIO`列包含`-` 这个符号，该符号的含义是指：该进程不是实时进程

### 调度类型
> 可通过`chrt`命令查看及调整.

> 时间片是分配给每个可运行进程的处理器可用时间段. linux的完全公平调度(cfs)没有使用时间片.

> 非抢占模式下, 除非进程自己主动停止, 否则会一直执行.

每个Linux进程都按照以下调度类型被调度：
- SCHED_FIFO
  先进先出的**实时**进程, 不使用时间片. 高优先级的进程可以抢占低优先级的进程,而相同优先级的进程遵循先来先得,此时除非主动让出否则想运行多久便运行多久
- SCHED_RR
  时间片轮转的**实时**进程. 保证对所有相同优先级的实时进程公平地分配CPU时间即只有当它的时间片用完，内核会把它放到进程队列的末尾; 而高优先级的任务也是可以抢占低优先级的任务
- SCHED_DEADLINE
    实时进程, 按照任务的deadline进行调度的. 当产生一个调度点的时候,DL调度器总是选择其deadline距离当前时间点最近的那个任务并调度它执行
- SCHED_NORMAL : 可用nice或renice调整, 范围是`19(最低) ~ -20(最高)`, 默认为0.
  普通的分时进程
- SCHED_BATCH
    后台进程,几乎不需要和前端进行交互, 不影响交互时可以降低它的优先级
- SCHED_IDLE
    特别空闲的时候才跑的进程

SCHED_NORMAL是普通进程调度策略,其他两者都是实时进程调度策略. 它们的主要区别就是通过优先级来区分的: 所有优先级值在0-99范围内的，都是实时进程，所以这个优先级范围也可以叫做实时进程优先级，而100-139范围内的是非实时进程.

系统的整体优先级策略是：如果系统中存在需要执行的实时进程，则优先执行实时进程. 直到实时进程退出或者主动让出CPU时，才会调度执行非实时进程. 即 **任何时候，实时进程的优先级都高于普通进程，实时进程只会被更高级的实时进程抢占，同级实时进程之间是按照FIFO（一次机会做完）或者RR（多次轮转）规则调度的**.

SCHED_FIFO 与 SCHED_RR 的区别是:当进程的调度策略为前者时,当前实时进程将一直占用 CPU 直至自动退出, **除非有更紧迫的、优先级更高的实时进程**需要运行时,它才会被抢占 CPU;当进程的调度策略为后者时,它与其它实时进程以实时轮流算法去共同使用 CPU，用完时间片放到运行队列尾部.

![Linux 进程优先级与 nice 值及实时进程优先级的关系](/misc/img/process/v2-488659493625064c6227293720c117c9_hd.jpg)

> priority就是ps命令中看到的PRI值或者top命令中看到的PR值, 越小说明优先级越高.
> nice值虽然不是priority，但是它确实可以影响进程的优先级
> 实时进程, 只有静态优先级;普通进程根据动态优先级进行调度
> 动态优先级(`dynamic_prio=max(100, min(static_prio-bonus+5, 139))`)是由静态优先级（`static_prio=MAX_RT_PRIO +nice+ 20`, 静态优先级范围在100~139之间）调整而来.
> 在系统中可以使用chrt命令来查看、设置一个进程的实时优先级状态
> 进程的平均睡眠时间也即bonus

1. 完全公平调度算法，简称CFS. 它是linux 默认进程调度器, 且CFS是一个普通进程的调度器. CFS的做法是：在所有可运行进程的总数上计算出一个进程应该运行的时间，nice值不再作为时间片分配的标准，而是用于处理计算获得的处理器使用权重. 是否抢占当前进程取决于新的可运行进程消耗了多少处理器使用比. 如果消耗的使用比比当前进程小, 则新进程立刻抢占, 否则继续等待.

但可运行进程数量趋向无穷大时, 每个处理器获取cpu使用比和时间片都趋向0, 导致不可接收的cpu切换消耗, cfs通过引入叫最小粒度(默认是1ms)的底线来解决. 因此进程非常多时cfs不是完全的公平调度.

cfs中nice的值不再影响调度策略, 但nice的相对值才会影响cpu时间的分配比例.

cfs实现的四部分:
- 时间记账
- 进程选择
- 调度器入口
- 睡眠和唤醒

cfs使用的调度器实体结构是`/include/linux/sched.h`的`struct sched_entity`, 它也是`struct task_struct`的`se`成员.

### linux task_struct 调度
linux调度器是以模块的形式提供的, 该模块结构被称为调度器类(scheduler classes), 允许多种不同的可动态添加的调度算法并存, 调度属于自己范畴的进程, 每个调度器都有自己的优先级. 基础的调度器代码在`kernel/sched/core.c`, 它会按照优先级顺序遍历调度类, 拥有一个可执行进程的最高优先级的调度类胜出, 再去选择要执行哪个程序.

> cfs算法在`kernel/sched/fair.c`

在task_struct中,有一个成员变量`unsigned int policy`保存了调度策略
```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/include/uapi/linux/sched.h?utm_source=share#L17:48
/*
 * Scheduling policies
 */
#define SCHED_NORMAL		0
#define SCHED_FIFO		1
#define SCHED_RR		2
#define SCHED_BATCH		3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE		5
#define SCHED_DEADLINE		6
```

配合调度策略的还有优先级 ,也在task_struct中
```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d8f6a4109a4263176c84b899076a5f8008/-/blob/include/linux/sched.h#L649
	int				prio;
	int				static_prio;
	int				normal_prio;
	unsigned int			rt_priority;
```

优先级其实就是一个数值,对于实时进程,优先级的范围是0~99;对于普通进程,优先级的范围是100~139. 数值越小,优先级越高. 从这里可以看出,所有的实时进程都比普通进程优先级要高.

```c
const struct sched_class	*sched_class; // 调度策略的执行逻辑
```

sched_class有几种实现:
- stop_sched_class : 优先级最高的任务会使用这种策略,会中断所有其他线程,且不会被其他任务打断
- dl_sched_class : 对应上面的deadline调度策略
- rt_sched_class : 对应RR算法或者FIFO算法的调度策略,具体调度策略由进程的task_struct->policy指定;
- fair_sched_class : 普通进程的调度策略 // 对于普通进程来讲,公平是最重要的
- idle_sched_class : 空闲进程的调度策略

它们其实是放在一个链表上的,即有顺序关系.

#### 完全公平调度
在Linux里面,实现了一个基于CFS的调度算法. CFS全称Completely Fair Scheduling,叫完全公平调度

CPU会提供一个时钟,过一段时间就触发一个时钟中断Tick. CFS会为每一个进程安排一个虚拟运行时间vruntime, 如果一个进程在运行,随着时间的增长,也就是一个个tick的到来,进程的vruntime将不断增大;没有得到执行的进程vruntime不变.

显然,那些vruntime少的,原来受到了不公平的对待,需要给它补上,所以会优先运行这样的进程. 

如何结合优先级? 将实际运行时间delta_exec按照一定算法转化为虚拟运行时间vruntime即可:
```txt
虚拟运行时间vruntime += 实际运行时间delta_exec * NICE_0_LOAD/权重
```

同样的实际运行时间,给高权重的算少了,低权重的算多了,但是当选取下一个运行进程的时候,还是按照最小的vruntime来的,这样高权重的获得的实际运行时间自然就多了

因此CFS需要一个数据结构来对vruntime进行排序,找出最小的那个. 这个能够排序的数据结构不但需要查询的时候,能够快速找到最小的,更新的时候也需要能够快速的调整排序,要知道vruntime可是经常在变
的,变了再插入这个数据结构,就需要重新排序. 而能够平衡查询和更新速度的是树,在这里使用的是红黑树. 

红黑树的的节点是应该包括vruntime的,称为调度实体. 
在task_struct中有这样的成员变量:
```c
struct sched_entity se; // 完全公平算法调度实体
struct sched_rt_entity rt; // 实时调度实体
struct sched_dl_entity dl; // Deadline调度实体
```

看来不光CFS调度策略需要有这样一个数据结构进行排序,其他的调度策略也同样有自己的数据结构进行排序,因为任何一个策略做调度的时候,都是要区分谁先运行谁后运行

进程会根据自己是实时的,还是普通的类型,通过这些成员变量,将自己挂在某一个数据结构里面,和其他的进程排序,等待被调度.

对于普通进程的调度实体定义如下,这里面包含了vruntime和权重load_weight,以及对于运行时间的统计:
```c
struct sched_entity {
	/* For load-balancing: */
	struct load_weight		load;
	unsigned long			runnable_weight;
	struct rb_node			run_node;
	struct list_head		group_node;
	unsigned int			on_rq;

	u64				exec_start;
	u64				sum_exec_runtime;
	u64				vruntime;
	u64				prev_sum_exec_runtime;

	u64				nr_migrations;

	struct sched_statistics		statistics;

#ifdef CONFIG_FAIR_GROUP_SCHED
	int				depth;
	struct sched_entity		*parent;
	/* rq on which this entity is (to be) queued: */
	struct cfs_rq			*cfs_rq;
	/* rq "owned" by this entity/group: */
	struct cfs_rq			*my_q;
#endif

#ifdef CONFIG_SMP
	/*
	 * Per entity load average tracking.
	 *
	 * Put into separate cache line so it does not
	 * collide with read-mostly values above.
	 */
	struct sched_avg		avg;
#endif
};
```

所有可运行的进程通过不断地插入操作最终都存储在以时间为顺序的红黑树中,vruntime最小的在树的左侧,vruntime最多的在树的右侧. CFS调度策略会选择红黑树最左边的叶子节点作为下一个将获得cpu的任务

CPU也是这样的,每个CPU都有自己的 struct rq 结构,其用于描述在此CPU上所运行的所有进程,其包括一个实时进程队列rt_rq和一个CFS运行队列cfs_rq,在调度时,调度器首先会先去实时进程队列找是否有实时进程需要运行,如果没有才会去CFS运行队列找是否有进行需要运行

rq, rt_rq和cf_rq的定义在[`kernel/sched/sched.h](https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/sched.h?utm_source=share#L503:22)里
cfs_rq.rb_root指向的就是红黑树的根节点,这个红黑树在CPU看起来就是一个队列,不断的取下一个应该运行的进程. cfs_rq.rb_leftmost指向的是最左面的节点

![调度相关的数据结构](/misc/img/sched_rq.png)

sched_class:
```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/sched.h?utm_source=share#L807
struct sched_class {
	const struct sched_class *next; // 指向下一个调度类

	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
	void (*yield_task)   (struct rq *rq);
	bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt);

	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);

	/*
	 * It is the responsibility of the pick_next_task() method that will
	 * return the next task to call put_prev_task() on the @prev task or
	 * something equivalent.
	 *
	 * May return RETRY_TASK when it finds a higher prio class has runnable
	 * tasks.
	 */
	struct task_struct * (*pick_next_task)(struct rq *rq,
					       struct task_struct *prev,
					       struct rq_flags *rf);
	void (*put_prev_task)(struct rq *rq, struct task_struct *p);

#ifdef CONFIG_SMP
	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);

	void (*task_woken)(struct rq *this_rq, struct task_struct *task);

	void (*set_cpus_allowed)(struct task_struct *p,
				 const struct cpumask *newmask);

	void (*rq_online)(struct rq *rq);
	void (*rq_offline)(struct rq *rq);
#endif

	void (*set_curr_task)(struct rq *rq);
	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
	void (*task_fork)(struct task_struct *p);
	void (*task_dead)(struct task_struct *p);

	/*
	 * The switched_from() call is allowed to drop rq->lock, therefore we
	 * cannot assume the switched_from/switched_to pair is serliazed by
	 * rq->lock. They are however serialized by p->pi_lock.
	 */
	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
			      int oldprio);

	unsigned int (*get_rr_interval)(struct rq *rq,
					struct task_struct *task);

	void (*update_curr)(struct rq *rq);

#define TASK_SET_GROUP		0
#define TASK_MOVE_GROUP		1

#ifdef CONFIG_FAIR_GROUP_SCHED
	void (*task_change_group)(struct task_struct *p, int type);
#endif
};
```
这个结构定义了很多种方法,用于在队列上操作task.

```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/core.c?utm_source=share#L19
/*
 * Pick up the highest-prio task:
 */
static inline struct task_struct *
pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
{
	const struct sched_class *class;
	struct task_struct *p;

	/*
	 * Optimization: we know that if all tasks are in the fair class we can
	 * call that function directly, but only if the @prev task wasn't of a
	 * higher scheduling class, because otherwise those loose the
	 * opportunity to pull in more work from other CPUs.
	 */
	if (likely((prev->sched_class == &idle_sched_class ||
		    prev->sched_class == &fair_sched_class) &&
		   rq->nr_running == rq->cfs.h_nr_running)) {

		p = fair_sched_class.pick_next_task(rq, prev, rf);
		if (unlikely(p == RETRY_TASK))
			goto again;

		/* Assumes fair_sched_class->next == idle_sched_class */
		if (unlikely(!p))
			p = idle_sched_class.pick_next_task(rq, prev, rf);

		return p;
	}

again:
	for_each_class(class) { // 沿着sched_class链表的顺序,依次调用每个调度类的方法
		p = class->pick_next_task(rq, prev, rf);
		if (p) {
			if (unlikely(p == RETRY_TASK))
				goto again;
			return p;
		}
	}

	/* The idle class should always have a runnable task: */
	BUG();
}
```

调度的时候是从优先级最高的调度类到优先级低的调度类,依次执行. 而对于每种调度类,有自己的实现CFS就是[fair_sched_class](https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d8f6a4109a4263176c84b899076a5f8008/-/blob/kernel/sched/fair.c?utm_source=share#L28:24).

对于同样的pick_next_task选取下一个要运行的任务这个动作,不同的调度类有自己的实现. fair_sched_class的实现是pick_next_task_fair,rt_sched_class的实现是pick_next_task_rt. 我们会发现这两个函数是操作不同的队列,pick_next_task_rt操作的是rt_rq,pick_next_task_fair操作的是cfs_rq.

这样整个运行的场景就串起来了,在每个CPU上都有一个队列rq,这个队列里面包含多个子队列,例如rt_rq和cfs_rq,不同的队列有不同的实现方式,cfs_rq就是用红黑树实现的. 

当某个CPU需要找下一个任务执行的时候,会按照优先级依次调用调度类,不同的调度类操作不同的队列: rt_sched_class先被调用,它会在rt_rq上找下一个任务,只有找不到的时候,才轮到fair_sched_class被调用,它会在cfs_rq上找下一个任务. 这样保证了实时任务的优先级永远大于普通任务. 

看部分sched_class定义的与调度有关的函数:
- enqueue_task向就绪队列中添加一个进程,当某个进程进入可运行状态时,调用这个函数
- dequeue_task 将一个进程从就就绪队列中删除
- pick_next_task 选择接下来要运行的进程
- put_prev_task 用另一个进程代替当前运行的进程
- set_curr_task 用于修改调度策略
- task_tick 每次周期性时钟到的时候,这个函数被调用,可能触发调度

![sched全局](/misc/img/sched_task)

![主动让出调度](/misc/img/schedule.png)
```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/core.c?utm_source=share#L3499
asmlinkage __visible void __sched schedule(void)
{
	struct task_struct *tsk = current;

	sched_submit_work(tsk);
	do {
		preempt_disable();
		__schedule(false);
		sched_preempt_enable_no_resched();
	} while (need_resched());
	sched_update_worker(tsk);
}
```

```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/core.c?utm_source=share#L2810
/*
 * __schedule() is the main scheduler function.
 *
 * The main means of driving the scheduler and thus entering this function are:
 *
 *   1. Explicit blocking: mutex, semaphore, waitqueue, etc.
 *
 *   2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return
 *      paths. For example, see arch/x86/entry_64.S.
 *
 *      To drive preemption between tasks, the scheduler sets the flag in timer
 *      interrupt handler scheduler_tick().
 *
 *   3. Wakeups don't really cause entry into schedule(). They add a
 *      task to the run-queue and that's it.
 *
 *      Now, if the new task added to the run-queue preempts the current
 *      task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets
 *      called on the nearest possible occasion:
 *
 *       - If the kernel is preemptible (CONFIG_PREEMPT=y):
 *
 *         - in syscall or exception context, at the next outmost
 *           preempt_enable(). (this might be as soon as the wake_up()'s
 *           spin_unlock()!)
 *
 *         - in IRQ context, return from interrupt-handler to
 *           preemptible context
 *
 *       - If the kernel is not preemptible (CONFIG_PREEMPT is not set)
 *         then at the next:
 *
 *          - cond_resched() call
 *          - explicit schedule() call
 *          - return from syscall or exception to user-space
 *          - return from interrupt-handler to user-space
 *
 * WARNING: must be called with preemption disabled!
 */
static void __sched notrace __schedule(bool preempt)
{
	struct task_struct *prev, *next;
	unsigned long *switch_count;
	struct rq_flags rf;
	struct rq *rq;
	int cpu;

	cpu = smp_processor_id();
	rq = cpu_rq(cpu); // 在当前的CPU上,我们取出任务队列rq
	prev = rq->curr; // task_struct *prev指向这个CPU的任务队列上面正在运行的那个进程curr

	schedule_debug(prev);

	if (sched_feat(HRTICK))
		hrtick_clear(rq);

	local_irq_disable();
	rcu_note_context_switch(preempt);

	/*
	 * Make sure that signal_pending_state()->signal_pending() below
	 * can't be reordered with __set_current_state(TASK_INTERRUPTIBLE)
	 * done by the caller to avoid the race with signal_wake_up().
	 *
	 * The membarrier system call requires a full memory barrier
	 * after coming from user-space, before storing to rq->curr.
	 */
	rq_lock(rq, &rf);
	smp_mb__after_spinlock();

	/* Promote REQ to ACT */
	rq->clock_update_flags <<= 1;
	update_rq_clock(rq);

	switch_count = &prev->nivcsw;
	if (!preempt && prev->state) {
		if (signal_pending_state(prev->state, prev)) {
			prev->state = TASK_RUNNING;
		} else {
			deactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK);

			if (prev->in_iowait) {
				atomic_inc(&rq->nr_iowait);
				delayacct_blkio_start();
			}
		}
		switch_count = &prev->nvcsw;
	}

	next = pick_next_task(rq, prev, &rf); // 获取下一个任务,task_struct *next指向下一个任务
	clear_tsk_need_resched(prev);
	clear_preempt_need_resched();

	if (likely(prev != next)) {
		rq->nr_switches++;
		rq->curr = next;
		/*
		 * The membarrier system call requires each architecture
		 * to have a full memory barrier after updating
		 * rq->curr, before returning to user-space.
		 *
		 * Here are the schemes providing that barrier on the
		 * various architectures:
		 * - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC.
		 *   switch_mm() rely on membarrier_arch_switch_mm() on PowerPC.
		 * - finish_lock_switch() for weakly-ordered
		 *   architectures where spin_unlock is a full barrier,
		 * - switch_to() for arm64 (weakly-ordered, spin_unlock
		 *   is a RELEASE barrier),
		 */
		++*switch_count;

		trace_sched_switch(preempt, prev, next);

		/* Also unlocks the rq: */
		rq = context_switch(rq, prev, next, &rf); // 进行进程上下文切换
	} else {
		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
		rq_unlock_irq(rq, &rf);
	}

	balance_callback(rq);
}
```

进程上下文切换
上下文切换主要干两件事情,一是切换进程空间,也即虚拟内存;二是切换寄存器和CPU上下文
```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d/-/blob/kernel/sched/core.c?utm_source=share#L2818:2
/*
 * context_switch - switch to the new MM and the new thread's register state.
 */
static __always_inline struct rq *
context_switch(struct rq *rq, struct task_struct *prev,
	       struct task_struct *next, struct rq_flags *rf)
{
	struct mm_struct *mm, *oldmm;

	prepare_task_switch(rq, prev, next);

	mm = next->mm;
	oldmm = prev->active_mm;
	/*
	 * For paravirt, this is coupled with an exit in switch_to to
	 * combine the page table reload and the switch backend into
	 * one hypercall.
	 */
	arch_start_context_switch(prev);

	/*
	 * If mm is non-NULL, we pass through switch_mm(). If mm is
	 * NULL, we will pass through mmdrop() in finish_task_switch().
	 * Both of these contain the full memory barrier required by
	 * membarrier after storing to rq->curr, before returning to
	 * user-space.
	 */
	if (!mm) {
		next->active_mm = oldmm;
		mmgrab(oldmm);
		enter_lazy_tlb(oldmm, next);
	} else
		switch_mm_irqs_off(oldmm, mm, next);

	if (!prev->mm) {
		prev->active_mm = NULL;
		rq->prev_mm = oldmm;
	}

	rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);

	prepare_lock_switch(rq, next, rf);

	/* Here we just switch the register state and the stack. */
	switch_to(prev, next, prev); // 寄存器和栈的切换, 它调用到了__switch_to_asm. 这是一段汇编代码,主要用于栈的切换
	barrier(); // barrier语句是一个编译器指令,用于保证switch_to和finish_task_switch的执行顺序,不会因为编译阶段优化而改变, 忽略即可

	return finish_task_switch(prev);  // 这个时候的finish_task_switch已经不是进程A的finish_task_switch了,而是进程B的finish_task_switch了
}
```

```c
// https://github.com/torvalds/linux/blob/948a64995aca6820abefd17f1a4258f5835c5ad9/arch/x86/include/asm/switch_to.h
#define switch_to(prev, next, last)					\
do {									\
	prepare_switch_to(next);					\
									\
	((last) = __switch_to_asm((prev), (next)));			\
} while (0)
```

当A切换到B的时候,运行到__switch_to_asm这一行的时候,是在A的内核栈上运行的,prev是A,next是B.
但是,A执行完__switch_to_asm之后就被切换走了,当C再次切换到A的时候,运行到__switch_to_asm,是从C的内核栈运行的. 这个时候,prev是C,next是A,但是__switch_to_asm里面切换
成为了A当时的内核栈.

还记得当年的场景“prev是A,next是B”,__switch_to_asm里面return prev的时候,还没return的时候,prev这个变量里面放的还是C,因而它会把C放到返回结果中. 但是,一旦return,就会弹出A当时的内核
栈. 这个时候,prev变量就变成了A,next变量就变成了B. 这就还原了当年的场景,好在返回值里面的last还是C.

通过三个变量switch_to(prev = A, next=B, last=C),A进程就明白了,我当时被切换走的时候,是切换成B,这次切换回来,是从C回来的

```asm
// https://github.com/torvalds/linux/blob/master/arch/x86/entry/entry_64.S
/*
 * %rdi: prev task
 * %rsi: next task
 */
ENTRY(__switch_to_asm)
	UNWIND_HINT_FUNC
	/*
	 * Save callee-saved registers
	 * This must match the order in inactive_task_frame
	 */
	pushq	%rbp
	pushq	%rbx
	pushq	%r12
	pushq	%r13
	pushq	%r14
	pushq	%r15

	/* switch stack */
	movq	%rsp, TASK_threadsp(%rdi)
	movq	TASK_threadsp(%rsi), %rsp

#ifdef CONFIG_STACKPROTECTOR
	movq	TASK_stack_canary(%rsi), %rbx
	movq	%rbx, PER_CPU_VAR(fixed_percpu_data) + stack_canary_offset
#endif

#ifdef CONFIG_RETPOLINE
	/*
	 * When switching from a shallower to a deeper call stack
	 * the RSB may either underflow or use entries populated
	 * with userspace addresses. On CPUs where those concerns
	 * exist, overwrite the RSB with entries which capture
	 * speculative execution to prevent attack.
	 */
	FILL_RETURN_BUFFER %r12, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
#endif

	/* restore callee-saved registers */
	popq	%r15
	popq	%r14
	popq	%r13
	popq	%r12
	popq	%rbx
	popq	%rbp

	jmp	__switch_to
END(__switch_to_asm)
```

最终,都返回了__switch_to这个函数.

```c
// https://sourcegraph.com/github.com/torvalds/linux@d1fdb6d8f6a4109a4263176c84b899076a5f8008/-/blob/arch/x86/kernel/process_64.c?utm_source=share#L518:2
__visible __notrace_funcgraph struct task_struct *
__switch_to(struct task_struct *prev_p, struct task_struct *next_p)
{
	struct thread_struct *prev = &prev_p->thread;
	struct thread_struct *next = &next_p->thread;
	struct fpu *prev_fpu = &prev->fpu;
	struct fpu *next_fpu = &next->fpu;
	int cpu = smp_processor_id();
// 没有struct tss_struct *tss = &per_cpu(cpu_tss, cpu);???
	WARN_ON_ONCE(IS_ENABLED(CONFIG_DEBUG_ENTRY) &&
		     this_cpu_read(irq_count) != -1);

	if (!test_thread_flag(TIF_NEED_FPU_LOAD))
		switch_fpu_prepare(prev_fpu, cpu);

	/* We must save %fs and %gs before load_TLS() because
	 * %fs and %gs may be cleared by load_TLS().
	 *
	 * (e.g. xen_load_tls())
	 */
	save_fsgs(prev_p);

	/*
	 * Load TLS before restoring any segments so that segment loads
	 * reference the correct GDT entries.
	 */
	load_TLS(next, cpu);

	/*
	 * Leave lazy mode, flushing any hypercalls made here.  This
	 * must be done after loading TLS entries in the GDT but before
	 * loading segments that might reference them.
	 */
	arch_end_context_switch(next_p);

	/* Switch DS and ES.
	 *
	 * Reading them only returns the selectors, but writing them (if
	 * nonzero) loads the full descriptor from the GDT or LDT.  The
	 * LDT for next is loaded in switch_mm, and the GDT is loaded
	 * above.
	 *
	 * We therefore need to write new values to the segment
	 * registers on every context switch unless both the new and old
	 * values are zero.
	 *
	 * Note that we don't need to do anything for CS and SS, as
	 * those are saved and restored as part of pt_regs.
	 */
	savesegment(es, prev->es);
	if (unlikely(next->es | prev->es))
		loadsegment(es, next->es);

	savesegment(ds, prev->ds);
	if (unlikely(next->ds | prev->ds))
		loadsegment(ds, next->ds);

	x86_fsgsbase_load(prev, next);

	/*
	 * Switch the PDA and FPU contexts.
	 */
	this_cpu_write(current_task, next_p);
	this_cpu_write(cpu_current_top_of_stack, task_top_of_stack(next_p));

	switch_fpu_finish(next_fpu);

	/* Reload sp0. */
	update_task_stack(next_p);

	switch_to_extra(prev_p, next_p);

#ifdef CONFIG_XEN_PV
	/*
	 * On Xen PV, IOPL bits in pt_regs->flags have no effect, and
	 * current_pt_regs()->flags may not match the current task's
	 * intended IOPL.  We need to switch it manually.
	 */
	if (unlikely(static_cpu_has(X86_FEATURE_XENPV) &&
		     prev->iopl != next->iopl))
		xen_set_iopl_mask(next->iopl);
#endif

	if (static_cpu_has_bug(X86_BUG_SYSRET_SS_ATTRS)) {
		/*
		 * AMD CPUs have a misfeature: SYSRET sets the SS selector but
		 * does not update the cached descriptor.  As a result, if we
		 * do SYSRET while SS is NULL, we'll end up in user mode with
		 * SS apparently equal to __USER_DS but actually unusable.
		 *
		 * The straightforward workaround would be to fix it up just
		 * before SYSRET, but that would slow down the system call
		 * fast paths.  Instead, we ensure that SS is never NULL in
		 * system call context.  We do this by replacing NULL SS
		 * selectors at every context switch.  SYSCALL sets up a valid
		 * SS, so the only way to get NULL is to re-enter the kernel
		 * from CPL 3 through an interrupt.  Since that can't happen
		 * in the same task as a running syscall, we are guaranteed to
		 * context switch between every interrupt vector entry and a
		 * subsequent SYSRET.
		 *
		 * We read SS first because SS reads are much faster than
		 * writes.  Out of caution, we force SS to __KERNEL_DS even if
		 * it previously had a different non-NULL value.
		 */
		unsigned short ss_sel;
		savesegment(ss, ss_sel);
		if (ss_sel != __KERNEL_DS)
			loadsegment(ss, __KERNEL_DS);
	}

	/* Load the Intel cache allocation PQR MSR. */
	resctrl_sched_in();

	return prev_p;
}
```

在Linux中,真的参与进程切换的寄存器很少,主要的就是栈顶寄存器.

进程的调度都最终会调用到__schedule函数.
