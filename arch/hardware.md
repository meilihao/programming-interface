# hardware
[intel主板架构](/misc/img/z390_chipset.jpg)

平台路径控制器(Platform Controller Hub，PCH)了取代南/北桥.
目前北桥芯片的功能已被集成在cpu里, 可参考[消失的北桥 主板芯片组背后的故事](http://www.mcplive.cn/?controller=Article&id=3008), 因此主板芯片组就等价于南桥芯片.

> 北桥负责高速IO, 与CPU通信，并且连接高速设备（内存/显卡），并且与南桥通信
> 南桥负责低速IO, 与低速设备（硬盘/USB）通信，时钟/BIOS/系统管理/旧式设备控制，并且与北桥通信

前端总线已淘汰, 目前通用的是pcie总线.

## cpu
参考:
- [支撑处理器的技术 永无止境地追求速度的世界.pdf]()

cpu包括三个部分,运算单元、数据单元和控制单元, 其实它还包括时钟.

运算单元只管算,可以执行算术和逻辑运算, 例如做加法、做位移等等.

数据单元包括 CPU 内部的缓存和寄存器组,空间很小,但是速度飞快,可以暂时存放数据/指令和运算结果. 寄存器组可分为专用寄存器和通用寄存器. 专用寄存器的作用是固定的，分别寄存相应的数据；而通用寄存器用途广泛并可由程序员规定其用途.

控制单元是整个CPU的指挥控制中心，由指令寄存器IR（Instruction Register）、指令译码器ID（Instruction Decoder）和 操作控制器OC（Operation Controller） 等组成，协调整个cpu有序工作.  它根据用户预先编好的程序，依次从存储器中取出各条指令，放在指令寄存器IR中，通过指令译码（分析）确定应该进行什么操作，然后通过操作控制器OC，按确定的时序，向相应的部件发出微操作控制信号. 操作控制器OC中主要包括：节拍脉冲发生器、控制矩阵、时钟脉冲发生器、复位电路和启停电路等控制逻辑.

现代cpu使用了多级高速cache, 超标量处理器, 流水线, 分支预测, 乱序执行, 微代码转换, 协处理器等优化技术.

> 多核cpu的每个cpu都有自己的独立cache、寄存器、运算单元, 但也允许共享某些高速cache, 比如L3 Cache.

**对于CPU, 我们只需要了解寄存器即可, 其他部分不需要太过关注:　程序是把寄存器作为操作对象来描述的**．因此对程序员来说CPU就是具有各种功能的寄存器的集合体.

> CPU 字长是CPU在单位时间内(同一时间)能一次处理的二进制数的位数叫字长.
> 外频(来自主板时钟, 其目前都相当低只有100MHz)指的是CPU与外部组件进行数据传输时的速度，倍频则是CPU内部用来加速工作效能的一个倍数，两者相乘才是CPU的频率速度. 因为CPU的倍频通常在出厂时已经被锁定而无法修改，因此较常被超频的是外频
> CPU通常在内部设计有一个锁相环频率发生器，对于输入的时钟信号(即外频)进行分频处理，按照一定比例提高输入的外频频率，从而得到CPU的实际工作频率，这个比例就称之为倍频系数
> 时钟周期作为CPU操作的最小时间单位，内部的所有操作都是以这个时钟周期作为基准

CPU架构:
- SMP(symmetric Multi-Processing, 对称多处理结构), 主要特征是共享: CPU核心独享L1 Cache, 但共享L2 Cache和L3 Cache.

    在一个计算机上汇集了一组处理器(多CPU), 但各CPU之间共享内存子系统以及总线结构.
- NUMA(Non-Uniform Memory Access,非一致性存储访问), 每个NUMA节点就是一个SMP, 多见于服务器.

> intel cpu间使用快速通道互连(quick path interconnect, QPI)通信.

### 工作原理
1. 取指令
CPU 的控制单元里面,有一个指令指针寄存器,执行的是下一条指令在内存中的地址. 控制单元会不停地将代码段的指令拿进来,先放入指令寄存器.

指令=操作码+操作数地址.
操作码：汇编语言里的 mov，add，jmp 等符号码；
操作数地址：该指令需要的操作数所在的地方，是在内存里还是在CPU的内部寄存器里

[常见流水线中指令的生命周期(即流水线级数)](https://my.oschina.net/u/3536632/blog/2934131), 把一个指令的执行过程拆分成：Fetch -> Decode -> Execute，那么这就是三级的流水线:
1. 取指令(Instruction Fetch，IF)

    从内存中取到指令寄存器IR(InstructionRegister)的过程.

1. 指令译码(Instruction Decode，ID)

    指令寄存器中的指令经过译码，决定该指令应进行何种操作（就是指令里的操作码）、操作数在哪里（操作数的地址）

    程序计数器PC(ProgramCounter)中的数值，用来指示当前指令在主存中的位置. 当一条指令被译码后，PC中的数值将根据指令字长度而自动递增

    在众多运用微码控制技术的新型CPU中，微码有时是可重写的，可以通过修改成品CPU来改变CPU的译码方式
1. 执行指令(Execute，EX)

    执行指令分为两个阶段： 取操作数 和 进行运算:
    - 取操作数：CPU 通过寻址操作，从内存（数据段）中读取操作数到通用寄存器中，暂存起来
    - 进行运算by ALU(arithmetic ligical unit, 算术逻辑单元)：运算单元通过指令中的操作码，对寄存器中的操作数进行 mov，add，jmp 操作

    如果进一步把Execute拆分成：ALU计算（指令执行）-> 内存访问 -> 数据写回，那么就会变成一个五级的流水线:
    1. 访存(Memory，MEM)

        根据指令需要，有可能要访问主存，读取操作数
    1. 结果写回(Writeback，WB)

        把执行指令阶段的运行结果数据“写回”到某种存储形式

### 高速缓存
基于局部性原理(principle of locality): CPU访问存储器时，短期内无论是存取指令还是存取数据，所访问的存储单元都趋于聚集在一个较小的连续区域中.

cache中有多个存储槽位(slots/lines), 每个槽保存一段内存中数据的副本.

采用最近最少使用(LRU, Least-Recently-Used)算法, 由硬件完成.

一致性问题由硬件而非os来解决.

### 寄存器
所有cpu都有一个或一组程序状态字(Program Status Word, PSW)的寄存器, 它包含有状态信息, 在x86上叫eflags.

x86 cpsr: 当前程序状态寄存器, 其0~bit保存了处理器运行的模式即ring, 该模式定义了可访问的寄存器范围.

### 流水线
![](/misc/img/os/c35eec09e6194d528fddb8803dca82e4.png)

流水线是指将计算机指令处理过程拆分为多个步骤，并通过多个硬件处理单元并行执行来加快指令执行速度. 从本质上讲，流水线技术是一种时间并行技术, 即多条指令重叠进行操作.

指令形成流水线以后，就需要一种高效的调控来保证硬件层面并发的效果：最佳情况是每条流水线里的十几个指令都是正确的，这样完全不浪费时钟周期. 而分支预测就是干这个的.

分支预测器猜测条件表达式两路分支中哪一路最可能发生，然后推测执行这一路的指令，来避免流水线停顿造成的时间浪费. 但是，如果后来发现分支预测错误，那么流水线中推测执行的那些中间结果全部放弃，重新获取正确的分支路线上的指令开始执行，这就带来了十几个时钟周期的延迟，这个时候，这个 CPU 核心就是完全在浪费时间. 幸运的是，当下的主流 CPU 在现代编译器的配合下，把这项工作做得越来越好了.

> N条流水线的本质是N组寄存器

在同主频下，增加流水线深度，其实是降低了CPU的性能:
- 因为一个Pipeline Stage(每一个独立的步骤，称之为流水线阶段或者流水线级)，就需要一个时钟周期
- 每个Stage都需要对应的Pipeline寄存器的开销，更深的流水线性能可能还会更差

过深的流水线，不仅不能提升计算机指令的吞吐率，还会加大计算的功耗和散热问题, 且超长的流水线的执行效率变得很低.

流水线深度取舍:
1. 流水线深度(级数, Pipeline Stage)越深, 即流水过程被切得越细, 因此每级间硬件逻辑越少, 能够达到更高的主频, 因此流水线吞吐率越高即性能越高, 功耗也越高
1. 更多的流水线即寄存器越多, 面积越大, 功耗越高
1. 每一级流水线都需要进行握手, 最后一级的反压信号可能会一直串扰到最前一级, 造成严重的时序问题, 可通过前向旁路缓存(forword bypass buffer)等手段来解决???
1. 流水线冲刷(pipeline flush): 深度越深, 分支预测越难, 丢弃错误的执行结果重新执行的概率越高, 即浪费和损失越严重

目前intel和arm cortex-a的流水线深度一般在十几级的范围.

流水线中的数据冲突:
1. WAR(write-after-read) : 后写和前读使用相同寄存器
1. WAW(write-after-write) : 后写和前写使用相同寄存器
1. RAW(read-after-write) : 后读与前写使用相同寄存器

war和waw可通过寄存器重命名解决.
RAW可通过动态调度解决.

### cpu所处状态的三种状态
- 运行于用户空间, 执行用户进程
- 运行于内核空间, 处于进程上下文, 代表某个特定的进程在执行
- 运行于内核空间, 处于中断上下文, 与任何进程无关, 处理某个特定的中断


### 超标量
![](/misc/img/os/Superscalarpipeline.png)

指在一颗处理器内核中实行了指令级并行的一类并行运算, 即同时进行多条流水线. 

未实现超标量体系结构时，CPU在每个时钟周期仅执行单条指令，因此仅有一个执行单元在工作，其它执行单元空闲. 超标量体系结构的CPU在一个时钟周期可以同时分派（dispatching）多条指令在不同的执行单元中被执行，这就实现了指令级的并行.

### 指令集
参考:
- [《手把手教你设计CPU——RISC-V处理器篇》]

CPU的灵魂是指令集架构（Instruction Set Architecture，ISA）, 它定义了计算机遵循的机器语言指令系统, 是硬件与软件的分界线.

指令集，顾名思义是一组指令的集合，而指令是指处理器进行操作的最小单元（譬如加减乘除操作或者读/写存储器数据）.

![指令集架构示意图](/misc/img/arch/isa.png)

指令集架构主要分为复杂指令集（Complex Instruction Set Computer，CISC）和精简指令集（Reduced Instruction Set Computer，RISC），两者的主要区别如下:
- CISC不仅包含了处理器常用的指令，还包含了许多不常用的特殊指令. 其指令数目比较多，所以称为复杂指令集.
- RISC只包含处理器常用的指令，而对于不常用的操作，则通过执行多条常用指令的方式来达到同样的效果, 由于其指令数目比较精简，所以称为精简指令集.

指令集使用也遵循8/2原则: CISC指令集定义的指令，只有20%被经常使用到，而有80%则很少被用到, 那些很少被用到的特殊指令尤其让CPU设计变得极为复杂，大大增加了硬件设计的时间成本与面积开销. 因此自从RISC诞生之后，**基本上所有现代指令集架构都选择使用RISC架构**.

通俗来讲，处理器架构的位数是指通用寄存器的宽度，其决定了寻址范围的大小、数据运算能力的强弱. 除嵌入式主流进入32bit时代外, 目前主流的mobile、pc和server，均使用64位架构.

> 处理器指令集架构的宽度和指令的编码长度无任何关系. 并不是说64位架构的指令长度为64位（这是一个常见的误区）. **从理论上来讲，指令本身的编码长度越短越好，因为可以节省代码的存储空间**. 因此即便在64位的架构中，也大量存在16位编码的指令，且基本上很少出现过64位长的指令编码.

主流指令集架构:
- x86

    pc和server中的主流, 是CISC. Intel公司通过内部`微码化`的方法克服掉了CISC架构的部分缺点.

    微码化: 是指将复杂的CISC指令先用硬件解码器翻译成对应的内部简单指令（微码）序列，然后送给处理器流水线执行的方法，使得x86的处理器核也变成了一种RISC的形式，从而能够借鉴RISC架构的优点. 不过，额外的硬件解码器同样也会带来额外的复杂度与面积开销.
- arm

    嵌入式和mobile的主流.

    arm cpu分类:
    - Cortex-A：面向性能密集型系统的应用处理器核
    - Cortex-R：面向实时应用的高性能核
    - Cortex-M：面向各类嵌入式应用的微控制器核

    Cortex-A、Cortex-M和Cortex-R架构的最大区别是前者包含了**存储器管理单元（Memory Management Unit，MMU），因此可以支持操作系统的运行**.
- risc-v

    开源的模块化的isa, 没有历史包袱, 后发优势, 获得大学教育和业界的大力支持.

### RISC-V
模块化的 RISC-V 架构能够使得用户灵活地选择不同的模块进行组合,以满足不同的应用场景:
- 针对小面积、低功耗的嵌入式场景 : RV32IC 组合的指令集,仅使用机器模式( Machine Mode) 
- 针对高性能应用操作系统场景 : RV32IMFDC 的指令集,使用机器模式( Machine Mode )与用户模式( User Mode )两种模式

RISC-V 最基本也是唯一强制要求实现的指令集部分是由`I`字母表示的基本整数指令子集. 使用该整数指令子集,便能够实现完整的软件编译器.

> 一个特定组合`IMAFD`,也被称为`通用`组合,用英文字母 G 表示. 因此 RV32G 表示 RV32IMAFD, 同理 RV64G 表示 RV64IMAFD.

为了提高代码密度, RISC -V 架构也提供可选的`压缩`指令子集,用英文字母 C 表示, 压缩指令的指令编码长度为 16 bit, 而普通的非压缩指令的长度为 32 bit.

为了进一步减少面积, RISC-V 架构还提供一种`嵌入式`架构,用英文字母 E 表示, 该架构主要用于追求极低面积与功耗的深嵌入式场景, 仅需要支持 16 个通用整数寄存器,而**非嵌入式的普通架构则需要支持 32 个通用整数寄存器**.

RISC-V 架构的整数通用寄存器组,包含 32个 (I 架构)或者 16个 (E 架构),其中整数寄存器 0 被预留为常数0, 其他的 31个 (I 架构)或者 15个 (E 架构)为普通的通用整数寄存器.

如果使用浮点模块F或者D,则需要另外一个独立的浮点寄存器组,包含 32 个通用浮点寄存器. 如果仅使用 F 模块的浮点指令子集,则每个通用浮点寄存器的宽度为 32 bit; 如果使用了 D 模块的浮点指令子集,则每个通用浮点寄存器的宽度为 64 bit.

在流水线中能够尽快地读取通用寄存器组,往往是处理器流水线设计的期望之一,这样
可以提高处理器性能和优化时序. 大部分risc因为历史和指令累积原因很难实现, 而因为后发优势和业界经验总结, RISC-V 的指令集编码非常规整,指令所需的通用寄存器的索引 Clndex)都被放在固定的位置, 便于指令译码器( Instruction Decoder )译码出寄存器索引, 然后读取通用寄存器.

> 由于现在的主流应用是小端格式, RISC-V 架构仅支持小端格式.

RISC-V 通过公用的程序库(专门用于保存和恢复现场)来处理子程序调用, 而不是使用`一次读多个寄存器指令`和`一次写多个寄存器指令`, 因此可以大幅简化 CPU 的硬件设计.

RISC-V 的一个特殊之处是对任何的运算指令错误(包括整数与浮点指令〉均不产生异常,而是产生某个特殊的默认值,同时设置某些状态寄存器的状态位.

RISC-V 架构定义了 3 种工作模式,又称为特权模式( Privileged Mode ):
- Machine Mode :机器模式,简称 M Mode.
- Supervisor Mode :监督模式,简称 S Mode.
- User Mode :用户模式,简称 U Mode.

RISC-V 架构定义 M Mode 为必选模式,另外两种为可选模式,通过不同的模式组合可以实现不同的系统

RISC-V 架构也支持几种不同的存储器地址管理机制,包括对于物理地址和虚拟地址的管理机制,使得 RISC-V 架构能够支持从简单的嵌入式系统(直接操作物理地址〉到复杂的操作系统(直接操作虚拟地址)的各种系统.

RISC-V 架构定义了一些控制和状态寄存器( Control and Status Regist町, CSR ),用于配置或记录一些运行的状态. CSR 寄存器是处理器核内部的寄存器,使用自己的地址编码空间和存储器寻址的地址区间完全无关系.

RISC-V 架构目前虽然还没有定型的矢量( Vector )指令子集,但是从目前的草案中看, RISC-V 架构将使用可变长度的矢量, 而不是矢量定长的 SIMD 指 令集( 例如ARM 的 NEON 和 Intel 的 M孔1X ),从而能够灵活地支持不同的实现.

RISC -V 架构支持第三方的扩展. 用户可以扩展自己的指令子集, RISC-V 预留了大量的指令编码空间用于用户的自定义扩展,同时还定义了 4条 Custom 指令可供用户直接使用.